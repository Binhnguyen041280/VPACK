# üì¶ T·ªïng h·ª£p m√£ ngu·ªìn Vtrack Backend

**T·ªïng c·ªông:** 61 file `.py`, 0 file `.js`

---

## üìÑ File: `database.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/database.py`

```python
import sqlite3
import os
import json
from modules.db_utils import find_project_root
import time
from datetime import datetime, timedelta
import logging
from typing import List, Dict, Any, Optional

# üîí SECURITY: Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# X√°c ƒë·ªãnh th∆∞ m·ª•c g·ªëc c·ªßa d·ª± √°n
BASE_DIR = find_project_root(os.path.abspath(__file__))

# ƒê∆∞·ªùng d·∫´n c∆° s·ªü d·ªØ li·ªáu
DB_DIR = os.path.join(BASE_DIR, "backend/database")
DB_PATH = os.path.join(DB_DIR, "events.db")

# ƒê∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh
INPUT_VIDEO_DIR = os.path.join(BASE_DIR, "resources/Inputvideo")
OUTPUT_CLIPS_DIR = os.path.join(BASE_DIR, "resources/output_clips")

def get_db_connection():
    """
    Get DB connection with retry logic for locked database and enhanced WAL config
    """
    for attempt in range(5):  # Retry t·ªëi ƒëa 5 l·∫ßn
        try:
            conn = sqlite3.connect(DB_PATH, timeout=60.0)  # TƒÉng timeout l√™n 60 gi√¢y
            conn.execute("PRAGMA busy_timeout = 60000")   # Busy timeout 60s
            conn.execute("PRAGMA journal_mode = WAL")     # WAL mode cho concurrent reads/writes
            conn.execute("PRAGMA synchronous = NORMAL")   # Balanced sync (nhanh h∆°n FULL)
            conn.execute("PRAGMA temp_store = MEMORY")    # Temp data in memory (tƒÉng speed)
            conn.execute("PRAGMA foreign_keys = ON")      # Enforce FK n·∫øu c·∫ßn
            print(f"‚úÖ DB connection success (attempt {attempt+1})")  # Debug log
            return conn
        except sqlite3.OperationalError as e:
            if "database is locked" in str(e) and attempt < 4:
                print(f"‚ö†Ô∏è DB locked, retrying in 2s... (attempt {attempt+1}/5)")
                time.sleep(2)  # Wait 2 gi√¢y tr∆∞·ªõc retry (tƒÉng t·ª´ 1s ƒë·ªÉ an to√†n)
                continue
            raise e  # Raise error n·∫øu h·∫øt retry
    raise sqlite3.OperationalError("Database locked after max retries")

def update_database():
    try:
        conn = get_db_connection()
        cursor = conn.cursor()

        # T·∫°o b·∫£ng file_list
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS file_list (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                program_type TEXT NOT NULL,
                days INTEGER,
                custom_path TEXT,
                file_path TEXT NOT NULL,
                ctime DATETIME,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                is_processed INTEGER DEFAULT 0,
                priority INTEGER DEFAULT 0,
                status TEXT DEFAULT 'ch∆∞a b·∫Øt ƒë·∫ßu',
                log_file_path TEXT,
                camera_name TEXT
            )
        """)

        # T·∫°o b·∫£ng program_status
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS program_status (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                key TEXT NOT NULL UNIQUE,
                value TEXT NOT NULL
            )
        """)
        cursor.execute("SELECT COUNT(*) FROM program_status WHERE key = 'first_run_completed'")
        if cursor.fetchone()[0] == 0:
            cursor.execute("INSERT INTO program_status (key, value) VALUES ('first_run_completed', 'false')")

        # T·∫°o b·∫£ng processing_config
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS processing_config (
                id INTEGER PRIMARY KEY,
                input_path TEXT,
                output_path TEXT,
                storage_duration INTEGER,
                min_packing_time INTEGER,
                max_packing_time INTEGER,
                frame_rate INTEGER,
                frame_interval INTEGER,
                video_buffer INTEGER,
                default_frame_mode TEXT,
                selected_cameras TEXT,
                db_path TEXT NOT NULL,
                run_default_on_start INTEGER DEFAULT 0,
                motion_threshold FLOAT DEFAULT 0.1,
                stable_duration_sec FLOAT DEFAULT 1
            )
        """)
        cursor.execute("UPDATE processing_config SET db_path = ?, run_default_on_start = 0 WHERE db_path IS NULL OR run_default_on_start IS NULL", (DB_PATH,))

        # Add camera_paths column to processing_config
        try:
            cursor.execute("ALTER TABLE processing_config ADD COLUMN camera_paths TEXT DEFAULT '{}'")
            print("‚úÖ Added camera_paths column to processing_config")
        except sqlite3.OperationalError:
            pass  # Column already exists

        # Th√™m c·ªôt multiple_sources_enabled n·∫øu ch∆∞a c√≥
        try:
            cursor.execute("ALTER TABLE processing_config ADD COLUMN multiple_sources_enabled INTEGER DEFAULT 0")
        except sqlite3.OperationalError:
            pass  # C·ªôt ƒë√£ t·ªìn t·∫°i

        # Ch√®n d·ªØ li·ªáu m·∫∑c ƒë·ªãnh n·∫øu b·∫£ng processing_config r·ªóng
        cursor.execute("SELECT COUNT(*) FROM processing_config")
        if cursor.fetchone()[0] == 0:
            cursor.execute("""
                INSERT INTO processing_config (
                    id, input_path, output_path, storage_duration, min_packing_time, 
                    max_packing_time, frame_rate, frame_interval, video_buffer, default_frame_mode, 
                    selected_cameras, db_path, run_default_on_start, multiple_sources_enabled, camera_paths
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (1, INPUT_VIDEO_DIR, OUTPUT_CLIPS_DIR, 30, 10, 120, 30, 5, 2, "default", "[]", DB_PATH, 0, 0, "{}"))

        # üîí SECURITY: Add user_sessions table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS user_sessions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_token TEXT NOT NULL UNIQUE,
                user_email TEXT NOT NULL,
                provider TEXT NOT NULL,
                encrypted_credentials TEXT,
                expires_at TIMESTAMP NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                is_active BOOLEAN DEFAULT 1,
                user_agent TEXT,
                ip_address TEXT
            )
        """)

        # üîí SECURITY: Add authentication audit trail
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS auth_audit (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_token TEXT,
                user_email TEXT,
                event_type TEXT NOT NULL,
                provider TEXT,
                success BOOLEAN NOT NULL,
                ip_address TEXT,
                user_agent TEXT,
                error_message TEXT,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # Create sync_status table for auto-sync management (Cloud sources)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS sync_status (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_id INTEGER NOT NULL,
                sync_enabled INTEGER DEFAULT 1,
                last_sync_timestamp TEXT,
                next_sync_timestamp TEXT,
                sync_interval_minutes INTEGER DEFAULT 10,
                last_sync_status TEXT DEFAULT 'pending',
                last_sync_message TEXT,
                files_downloaded_count INTEGER DEFAULT 0,
                total_download_size_mb REAL DEFAULT 0.0,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (source_id) REFERENCES video_sources (id) ON DELETE CASCADE,
                UNIQUE(source_id)
            )
        """)
        print("‚úÖ Created sync_status table")

        # Create downloaded_files table for tracking downloaded content
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS downloaded_files (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_id INTEGER NOT NULL,
                camera_name TEXT NOT NULL,
                original_filename TEXT,
                local_file_path TEXT NOT NULL,
                file_size_bytes INTEGER DEFAULT 0,
                download_timestamp TEXT DEFAULT CURRENT_TIMESTAMP,
                recording_start_time TEXT,
                recording_end_time TEXT,
                file_format TEXT,
                checksum TEXT,
                sync_batch_id TEXT,
                is_processed INTEGER DEFAULT 0,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (source_id) REFERENCES video_sources (id) ON DELETE CASCADE
            )
        """)
        print("‚úÖ Created downloaded_files table")

        # Create last_downloaded_file table for efficient tracking
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS last_downloaded_file (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_id INTEGER NOT NULL,
                camera_name TEXT NOT NULL,
                last_filename TEXT,
                last_file_timestamp TEXT,
                last_download_time TEXT DEFAULT CURRENT_TIMESTAMP,
                total_files_count INTEGER DEFAULT 0,
                total_size_mb REAL DEFAULT 0.0,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (source_id) REFERENCES video_sources (id) ON DELETE CASCADE,
                UNIQUE(source_id, camera_name)
            )
        """)
        print("‚úÖ Created last_downloaded_file table")

        # Create indexes for performance
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_sync_status_source_id ON sync_status(source_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_sync_status_next_sync ON sync_status(next_sync_timestamp)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_downloaded_files_source_camera ON downloaded_files(source_id, camera_name)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_downloaded_files_timestamp ON downloaded_files(download_timestamp)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_downloaded_files_processed ON downloaded_files(is_processed)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_last_downloaded_source_camera ON last_downloaded_file(source_id, camera_name)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_last_downloaded_timestamp ON last_downloaded_file(last_file_timestamp)")
        
        # üîí SECURITY: Indexes for session management
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_sessions_token ON user_sessions(session_token)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_sessions_email ON user_sessions(user_email)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_sessions_expires ON user_sessions(expires_at)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_audit_email ON auth_audit(user_email)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_audit_created ON auth_audit(created_at)')
   
        # T·∫°o b·∫£ng frame_settings
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS frame_settings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                mode TEXT NOT NULL,
                frame_rate INTEGER,
                frame_interval INTEGER,
                description TEXT
            )
        """)
        cursor.execute("SELECT COUNT(*) FROM frame_settings")
        if cursor.fetchone()[0] == 0:
            cursor.execute("""
                INSERT INTO frame_settings (mode, frame_rate, frame_interval, description)
                VALUES (?, ?, ?, ?)
            """, ("default", 30, 5, "Ch·∫ø ƒë·ªô m·∫∑c ƒë·ªãnh t·ª´ giao di·ªán"))

        # T·∫°o b·∫£ng general_info v·ªõi working_days d·∫°ng JSON ti·∫øng Anh
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS general_info (
                id INTEGER PRIMARY KEY,
                country TEXT,
                timezone TEXT,
                brand_name TEXT,
                working_days TEXT,
                from_time TEXT,
                to_time TEXT
            )
        """)
        cursor.execute("SELECT COUNT(*) FROM general_info")
        if cursor.fetchone()[0] == 0:
            working_days = json.dumps(["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"])
            cursor.execute("""
                INSERT INTO general_info (
                    id, country, timezone, brand_name, working_days, from_time, to_time
                ) VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (1, "Vi·ªát Nam", "UTC+7", "MyBrand", working_days, "07:00", "23:00"))

        # T·∫°o b·∫£ng events
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS events (
                event_id INTEGER PRIMARY KEY AUTOINCREMENT,
                ts INTEGER,
                te INTEGER,
                duration INTEGER,
                tracking_codes TEXT,
                video_file TEXT NOT NULL,
                buffer INTEGER NOT NULL,
                camera_name TEXT,
                packing_time_start INTEGER,
                packing_time_end INTEGER,
                is_processed INTEGER DEFAULT 0,
                processed_timestamp INTEGER,
                output_video_path TEXT,
                session_id TEXT,
                output_file TEXT
            )
        """)
        # T·∫°o ch·ªâ m·ª•c tr√™n te v√† event_id
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_events_te_event_id ON events(te, event_id)")

        # T·∫°o b·∫£ng processed_logs
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS processed_logs (
                log_file TEXT PRIMARY KEY,
                processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                is_processed INTEGER DEFAULT 0
            )
        """)

        # T·∫°o b·∫£ng packing_profiles
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS packing_profiles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                profile_name TEXT NOT NULL,
                qr_trigger_area TEXT,
                qr_motion_area TEXT,
                qr_mvd_area TEXT,
                packing_area TEXT,
                min_packing_time INTEGER,
                jump_time_ratio REAL,
                mvd_jump_ratio REAL,
                scan_mode TEXT,
                fixed_threshold INTEGER,
                margin INTEGER,
                additional_params TEXT
            )
        """)

        # T·∫°o b·∫£ng video_sources (support local + cloud only)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS video_sources (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_type TEXT NOT NULL CHECK(source_type IN ('local', 'cloud')),
                name TEXT NOT NULL,
                path TEXT NOT NULL,
                config TEXT,
                active INTEGER DEFAULT 1,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                folder_depth INTEGER DEFAULT 0,
                parent_folder_id TEXT
            )
        """)
        
        # Add folder depth tracking columns
        try:
            cursor.execute("ALTER TABLE video_sources ADD COLUMN folder_depth INTEGER DEFAULT 0")
            print("‚úÖ Added folder_depth column to video_sources")
        except sqlite3.OperationalError:
            pass  # Column already exists

        try:
            cursor.execute("ALTER TABLE video_sources ADD COLUMN parent_folder_id TEXT")
            print("‚úÖ Added parent_folder_id column to video_sources")
        except sqlite3.OperationalError:
            pass  # Column already exists
            
        # Create indexes for lazy folder tree
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_video_sources_folder_depth ON video_sources(folder_depth)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_video_sources_parent_folder ON video_sources(parent_folder_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_video_sources_source_type_active ON video_sources(source_type, active)")
        print("‚úÖ Created indexes for lazy folder tree performance")

        # Update index cho video_sources
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_video_sources_active ON video_sources(active)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_video_sources_source_type ON video_sources(source_type)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_video_sources_created_at ON video_sources(created_at)")

        # T·∫°o table camera_configurations
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS camera_configurations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_id INTEGER NOT NULL,
                camera_name TEXT NOT NULL,
                camera_config TEXT, -- JSON config specific to this camera
                is_selected INTEGER DEFAULT 1,
                folder_path TEXT, -- Local folder path for this camera
                stream_url TEXT, -- RTSP/stream URL if applicable
                resolution TEXT,
                codec TEXT,
                capabilities TEXT, -- JSON array of capabilities
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (source_id) REFERENCES video_sources (id) ON DELETE CASCADE,
                UNIQUE(source_id, camera_name)
            )
        """)
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_camera_configurations_source_id ON camera_configurations(source_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_camera_configurations_selected ON camera_configurations(is_selected)")

        # Create view active_cameras
        cursor.execute("""
            CREATE VIEW IF NOT EXISTS active_cameras AS
            SELECT 
                vs.id as source_id,
                vs.name as source_name,
                vs.source_type,
                vs.path as source_path,
                cc.camera_name,
                cc.folder_path,
                cc.stream_url,
                cc.resolution,
                cc.codec,
                cc.capabilities,
                cc.is_selected
            FROM video_sources vs
            LEFT JOIN camera_configurations cc ON vs.id = cc.source_id
            WHERE vs.active = 1 AND cc.is_selected = 1
        """)

        # Create trigger update_camera_configurations_timestamp
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS update_camera_configurations_timestamp
            AFTER UPDATE ON camera_configurations
            FOR EACH ROW
            BEGIN
                UPDATE camera_configurations SET updated_at = CURRENT_TIMESTAMP WHERE id = NEW.id;
            END
        """)

        # Create trigger to update sync_status timestamp
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS update_sync_status_timestamp
            AFTER UPDATE ON sync_status
            FOR EACH ROW
            BEGIN
                UPDATE sync_status SET updated_at = CURRENT_TIMESTAMP WHERE id = NEW.id;
            END
        """)

        # Create trigger to update last_downloaded_file timestamp
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS update_last_downloaded_file_timestamp
            AFTER UPDATE ON last_downloaded_file
            FOR EACH ROW
            BEGIN
                UPDATE last_downloaded_file SET updated_at = CURRENT_TIMESTAMP WHERE id = NEW.id;
            END
        """)

        # Create view for sync dashboard (Cloud sources only)
        cursor.execute("""
            CREATE VIEW IF NOT EXISTS sync_dashboard AS
            SELECT 
                vs.id as source_id,
                vs.name as source_name,
                vs.source_type,
                vs.path as source_path,
                ss.sync_enabled,
                ss.last_sync_timestamp,
                ss.next_sync_timestamp,
                ss.sync_interval_minutes,
                ss.last_sync_status,
                ss.last_sync_message,
                ss.files_downloaded_count,
                ss.total_download_size_mb,
                COUNT(df.id) as total_downloaded_files,
                SUM(df.file_size_bytes) / (1024*1024) as total_size_mb_calculated
            FROM video_sources vs
            LEFT JOIN sync_status ss ON vs.id = ss.source_id
            LEFT JOIN downloaded_files df ON vs.id = df.source_id
            WHERE vs.active = 1 AND vs.source_type = 'cloud'
            GROUP BY vs.id, vs.name, vs.source_type, vs.path, ss.sync_enabled, 
                     ss.last_sync_timestamp, ss.next_sync_timestamp, ss.sync_interval_minutes,
                     ss.last_sync_status, ss.last_sync_message, ss.files_downloaded_count, ss.total_download_size_mb
        """)

        # Create view for efficient camera tracking (Cloud sources only)
        cursor.execute("""
            CREATE VIEW IF NOT EXISTS camera_sync_status AS
            SELECT 
                vs.id as source_id,
                vs.name as source_name,
                vs.source_type,
                ldf.camera_name,
                ldf.last_filename,
                ldf.last_file_timestamp,
                ldf.last_download_time,
                ldf.total_files_count,
                ldf.total_size_mb,
                ss.sync_enabled,
                ss.sync_interval_minutes,
                ss.last_sync_status
            FROM video_sources vs
            LEFT JOIN last_downloaded_file ldf ON vs.id = ldf.source_id
            LEFT JOIN sync_status ss ON vs.id = ss.source_id
            WHERE vs.active = 1 AND vs.source_type = 'cloud'
            ORDER BY vs.name, ldf.camera_name
        """)

        conn.commit()
        conn.close()
        print(f"üéâ Database updated successfully at {DB_PATH}")
        print("‚úÖ Added camera_paths column to processing_config")
        print("‚úÖ Created sync_status table for auto-sync management")
        print("‚úÖ Created downloaded_files table for file tracking")
        print("‚úÖ Created last_downloaded_file table for efficient tracking")
        print("‚úÖ Created indexes and views for performance")
        print("‚úÖ Added folder_depth and parent_folder_id columns to video_sources")
        print("‚úÖ Created indexes for lazy folder tree performance")
        print("‚úÖ Added helper functions for folder depth management")
        print("üîí Added user_sessions and auth_audit tables for security")
        print("‚úÖ Updated source_type constraint to support local and cloud only")
        
    except Exception as e:
        print(f"Error updating database: {e}")
        raise

# üîí SECURITY: Session Management Methods
def create_session(session_token: str, user_email: str, provider: str, 
                  encrypted_credentials: str, expires_at: datetime, 
                  user_agent: str = None, ip_address: str = None) -> bool:
    """Create new user session"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO user_sessions 
            (session_token, user_email, provider, encrypted_credentials, 
             expires_at, user_agent, ip_address)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (session_token, user_email, provider, encrypted_credentials, 
              expires_at, user_agent, ip_address))
        conn.commit()
        conn.close()
        logger.info(f"Session created for {user_email}")
        return True
    except Exception as e:
        logger.error(f"Failed to create session: {e}")
        return False

def get_session(session_token: str) -> Optional[Dict]:
    """Get session by token"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('''
            SELECT * FROM user_sessions 
            WHERE session_token = ? AND is_active = 1 AND expires_at > CURRENT_TIMESTAMP
        ''', (session_token,))
        row = cursor.fetchone()
        if row:
            # Update last accessed
            cursor.execute('''
                UPDATE user_sessions 
                SET last_accessed = CURRENT_TIMESTAMP 
                WHERE session_token = ?
            ''', (session_token,))
            conn.commit()
            # Convert row to dict
            columns = [description[0] for description in cursor.description]
            result = dict(zip(columns, row))
            conn.close()
            return result
        conn.close()
        return None
    except Exception as e:
        logger.error(f"Failed to get session: {e}")
        return None

def update_session_credentials(session_token: str, encrypted_credentials: str) -> bool:
    """Update session credentials (for token refresh)"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('''
            UPDATE user_sessions 
            SET encrypted_credentials = ?, last_accessed = CURRENT_TIMESTAMP
            WHERE session_token = ? AND is_active = 1
        ''', (encrypted_credentials, session_token))
        conn.commit()
        result = cursor.rowcount > 0
        conn.close()
        return result
    except Exception as e:
        logger.error(f"Failed to update session credentials: {e}")
        return False

def invalidate_session(session_token: str) -> bool:
    """Invalidate a session"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('''
            UPDATE user_sessions 
            SET is_active = 0 
            WHERE session_token = ?
        ''', (session_token,))
        conn.commit()
        result = cursor.rowcount > 0
        conn.close()
        return result
    except Exception as e:
        logger.error(f"Failed to invalidate session: {e}")
        return False

def cleanup_expired_sessions() -> int:
    """Clean up expired sessions"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('''
            DELETE FROM user_sessions 
            WHERE expires_at < CURRENT_TIMESTAMP OR is_active = 0
        ''')
        conn.commit()
        deleted_count = cursor.rowcount
        conn.close()
        logger.info(f"Cleaned up {deleted_count} expired sessions")
        return deleted_count
    except Exception as e:
        logger.error(f"Failed to cleanup sessions: {e}")
        return 0

def get_user_sessions(user_email: str) -> List[Dict]:
    """Get all active sessions for a user"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('''
            SELECT session_token, provider, created_at, last_accessed, expires_at
            FROM user_sessions 
            WHERE user_email = ? AND is_active = 1 AND expires_at > CURRENT_TIMESTAMP
            ORDER BY last_accessed DESC
        ''', (user_email,))
        columns = [description[0] for description in cursor.description]
        results = [dict(zip(columns, row)) for row in cursor.fetchall()]
        conn.close()
        return results
    except Exception as e:
        logger.error(f"Failed to get user sessions: {e}")
        return []

# üîí SECURITY: Authentication Audit Methods
def log_auth_event(event_type: str, success: bool, user_email: str = None, 
                  session_token: str = None, provider: str = None, 
                  ip_address: str = None, user_agent: str = None, 
                  error_message: str = None, metadata: Dict = None) -> bool:
    """Log authentication event"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO auth_audit 
            (session_token, user_email, event_type, provider, success, 
             ip_address, user_agent, error_message, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (session_token, user_email, event_type, provider, success,
              ip_address, user_agent, error_message, 
              json.dumps(metadata) if metadata else None))
        conn.commit()
        conn.close()
        return True
    except Exception as e:
        logger.error(f"Failed to log auth event: {e}")
        return False

def get_auth_audit(user_email: str = None, limit: int = 100) -> List[Dict]:
    """Get authentication audit trail"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        if user_email:
            cursor.execute('''
                SELECT * FROM auth_audit 
                WHERE user_email = ?
                ORDER BY created_at DESC 
                LIMIT ?
            ''', (user_email, limit))
        else:
            cursor.execute('''
                SELECT * FROM auth_audit 
                ORDER BY created_at DESC 
                LIMIT ?
            ''', (limit,))
        columns = [description[0] for description in cursor.description]
        results = [dict(zip(columns, row)) for row in cursor.fetchall()]
        conn.close()
        return results
    except Exception as e:
        logger.error(f"Failed to get auth audit: {e}")
        return []

# Helper functions for database operations

def update_camera_paths(source_id: int, camera_paths: dict):
    """Update camera_paths in processing_config for a source"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Update processing_config with camera paths
        camera_paths_json = json.dumps(camera_paths)
        cursor.execute("""
            UPDATE processing_config 
            SET camera_paths = ? 
            WHERE id = 1
        """, (camera_paths_json,))
        
        conn.commit()
        conn.close()
        return True
    except Exception as e:
        print(f"Error updating camera paths: {e}")
        return False

def initialize_sync_status(source_id: int, sync_enabled: bool = True, interval_minutes: int = 10):
    """Initialize sync status for a new source (Cloud sources only)"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        from datetime import datetime, timedelta
        
        now = datetime.now()
        next_sync = now + timedelta(minutes=interval_minutes)
        
        cursor.execute("""
            INSERT OR REPLACE INTO sync_status (
                source_id, sync_enabled, last_sync_timestamp, next_sync_timestamp,
                sync_interval_minutes, last_sync_status, last_sync_message
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            source_id, 
            1 if sync_enabled else 0,
            now.isoformat(),
            next_sync.isoformat(),
            interval_minutes,
            'initialized',
            'Auto-sync initialized'
        ))
        
        conn.commit()
        conn.close()
        return True
    except Exception as e:
        print(f"Error initializing sync status: {e}")
        return False

def get_sync_status(source_id: int):
    """Get sync status for a source"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT * FROM sync_status WHERE source_id = ?
        """, (source_id,))
        
        result = cursor.fetchone()
        conn.close()
        
        if result:
            columns = [description[0] for description in cursor.description]
            return dict(zip(columns, result))
        return None
    except Exception as e:
        print(f"Error getting sync status: {e}")
        return None

# Helper functions for efficient file tracking

def update_last_downloaded_file(source_id: int, camera_name: str, latest_file_info: dict, total_count: int, total_size_mb: float):
    """Update last downloaded file info for a camera"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        from datetime import datetime
        
        cursor.execute("""
            INSERT OR REPLACE INTO last_downloaded_file (
                source_id, camera_name, last_filename, last_file_timestamp,
                last_download_time, total_files_count, total_size_mb
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            source_id,
            camera_name,
            latest_file_info['filename'],
            latest_file_info['timestamp'].isoformat(),
            datetime.now().isoformat(),
            total_count,
            total_size_mb
        ))
        
        conn.commit()
        conn.close()
        return True
    except Exception as e:
        print(f"Error updating last downloaded file: {e}")
        return False

def get_last_downloaded_timestamp(source_id: int, camera_name: str):
    """Get last downloaded file timestamp for a camera"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT last_file_timestamp FROM last_downloaded_file 
            WHERE source_id = ? AND camera_name = ?
        """, (source_id, camera_name))
        
        result = cursor.fetchone()
        conn.close()
        
        return result[0] if result else "1970-01-01T00:00:00"
    except Exception as e:
        print(f"Error getting last downloaded timestamp: {e}")
        return "1970-01-01T00:00:00"

def get_camera_download_stats(source_id: int):
    """Get download statistics for all cameras of a source"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT camera_name, last_filename, last_file_timestamp,
                   total_files_count, total_size_mb, last_download_time
            FROM last_downloaded_file 
            WHERE source_id = ?
            ORDER BY camera_name
        """, (source_id,))
        
        results = cursor.fetchall()
        conn.close()
        
        camera_stats = {}
        total_files = 0
        total_size = 0
        
        for row in results:
            camera_name, last_filename, last_timestamp, files_count, size_mb, last_download = row
            camera_stats[camera_name] = {
                'last_filename': last_filename,
                'last_timestamp': last_timestamp,
                'files_count': files_count or 0,
                'size_mb': size_mb or 0.0,
                'last_download': last_download
            }
            total_files += files_count or 0
            total_size += size_mb or 0.0
        
        return {
            'camera_stats': camera_stats,
            'total_files': total_files,
            'total_size_mb': total_size,
            'cameras_count': len(camera_stats)
        }
    except Exception as e:
        print(f"Error getting camera download stats: {e}")
        return {
            'camera_stats': {},
            'total_files': 0,
            'total_size_mb': 0.0,
            'cameras_count': 0
        }

# Helper functions for lazy folder tree

def create_source_with_folder_info(source_data, selected_folders=None):
    """Create video source with lazy folder tree information"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Prepare source data
        source_type = source_data.get('source_type')
        name = source_data.get('name')
        path = source_data.get('path')
        config = json.dumps(source_data.get('config', {}))
        
        # For cloud sources with lazy folder selection
        if source_type == 'cloud' and selected_folders:
            # Store selected folders in config
            config_dict = source_data.get('config', {})
            config_dict['selected_folders'] = selected_folders
            config_dict['lazy_loading_enabled'] = True
            config = json.dumps(config_dict)
            
            # Use depth from first selected folder (they should all be depth 4)
            folder_depth = selected_folders[0].get('depth', 4) if selected_folders else 4
            parent_folder_id = selected_folders[0].get('parent_id') if selected_folders else None
        else:
            folder_depth = 0
            parent_folder_id = None
        
        # Insert source
        cursor.execute("""
            INSERT INTO video_sources (
                source_type, name, path, config, active, 
                folder_depth, parent_folder_id
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (source_type, name, path, config, 1, folder_depth, parent_folder_id))
        
        source_id = cursor.lastrowid
        
        conn.commit()
        conn.close()
        
        print(f"‚úÖ Created source with lazy folder info: {name} (ID: {source_id})")
        return source_id
        
    except Exception as e:
        print(f"‚ùå Error creating source with folder info: {e}")
        return None

def get_sources_with_folder_info():
    """Get all sources with folder depth information"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT id, source_type, name, path, config, active, 
                   folder_depth, parent_folder_id, created_at
            FROM video_sources 
            WHERE active = 1
            ORDER BY created_at DESC
        """)
        
        sources = []
        for row in cursor.fetchall():
            source = {
                'id': row[0],
                'source_type': row[1],
                'name': row[2],
                'path': row[3],
                'config': json.loads(row[4]) if row[4] else {},
                'active': row[5],
                'folder_depth': row[6],
                'parent_folder_id': row[7],
                'created_at': row[8]
            }
            sources.append(source)
        
        conn.close()
        return sources
        
    except Exception as e:
        print(f"‚ùå Error getting sources with folder info: {e}")
        return []

def update_source_folder_depth(source_id, folder_depth, parent_folder_id=None):
    """Update folder depth for existing source"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            UPDATE video_sources 
            SET folder_depth = ?, parent_folder_id = ?
            WHERE id = ?
        """, (folder_depth, parent_folder_id, source_id))
        
        conn.commit()
        conn.close()
        
        print(f"‚úÖ Updated folder depth for source {source_id}: depth={folder_depth}")
        return True
        
    except Exception as e:
        print(f"‚ùå Error updating folder depth: {e}")
        return False

if __name__ == "__main__":
    os.makedirs(DB_DIR, exist_ok=True)
    update_database()
```
## üìÑ File: `app.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/app.py`

```python
import os
import sys
from flask_cors import CORS

# ==================== T·∫ÆT T·∫§T C·∫¢ LOGS TR∆Ø·ªöC KHI IMPORT ====================
# TensorFlow logs
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TF_LOGGING'] = 'ERROR'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

# Google/Abseil logs (MediaPipe)
os.environ['GLOG_minloglevel'] = '3'
os.environ['GLOG_logtostderr'] = '0'
os.environ['GLOG_stderrthreshold'] = '3'
os.environ['GLOG_v'] = '0'

# MediaPipe logs
os.environ['MEDIAPIPE_DISABLE_GPU'] = '1'
os.environ['MEDIAPIPE_LOG_LEVEL'] = '3'

# OpenCV logs
os.environ['OPENCV_LOG_LEVEL'] = 'ERROR'

# T·∫Øt C++ warnings
os.environ['PYTHONWARNINGS'] = 'ignore'

# Redirect stderr ƒë·ªÉ t·∫Øt ho√†n to√†n C++ logs
import warnings
warnings.filterwarnings('ignore')

# T·∫Øt absl logging
try:
    import absl.logging
    absl.logging.set_verbosity(absl.logging.ERROR)
    absl.logging.set_stderrthreshold(absl.logging.ERROR)
except ImportError:
    pass

# ==================== IMPORT MODULES ====================
from modules.config.logging_config import setup_logging, get_logger
from datetime import datetime
import logging
import signal
import threading
import socket
import atexit
import sqlite3
from datetime import datetime, timedelta, timezone

# Thi·∫øt l·∫≠p logging t·ª´ logging_config
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
setup_logging(BASE_DIR, app_name="app", log_level=logging.DEBUG)
logger = logging.getLogger("app")

# Import c√°c modules kh√°c
from modules.config.config import config_bp, init_app_and_config
from modules.scheduler.program import program_bp
from modules.query.query import query_bp
from blueprints.cutter_bp import cutter_bp
from blueprints.hand_detection_bp import hand_detection_bp
from blueprints.qr_detection_bp import qr_detection_bp
from blueprints.roi_bp import roi_bp
from modules.scheduler.program import scheduler  # Import BatchScheduler

# üÜï NEW: Import cloud endpoints blueprint
from modules.sources.cloud_endpoints import cloud_bp
# üÜï NEW: Import sync endpoints blueprint
from modules.sources.sync_endpoints import sync_bp

# Kh·ªüi t·∫°o Flask app v√† DB path t·ª´ config
app, DB_PATH, logger = init_app_and_config()
from flask_session import Session

# üîß FIXED: OAuth-Compatible Session Configuration
import secrets
app.config.update(
    # OAuth Session Fix - CRITICAL for Google OAuth
    SECRET_KEY=os.environ.get('SECRET_KEY', secrets.token_urlsafe(32)),
    SESSION_COOKIE_NAME='vtrack_session',
    SESSION_COOKIE_HTTPONLY=True,
    SESSION_COOKIE_SECURE=False,  # Set to True in production with HTTPS
    SESSION_COOKIE_SAMESITE='Lax',  # CRITICAL for OAuth redirects
    PERMANENT_SESSION_LIFETIME=timedelta(hours=24),  # Longer for OAuth
    
    # Session storage
    SESSION_TYPE='filesystem',
    SESSION_FILE_DIR=os.path.join(BASE_DIR, 'flask_session'),
    
    # OAuth specific
    OAUTH_INSECURE_TRANSPORT=True,  # Only for development
)

# Initialize session
Session(app)

# ‚úÖ SINGLE CORS Configuration - No duplicates
CORS(app, 
     resources={
         r"/*": {
             "origins": [
                 "http://localhost:3000", 
                 "http://127.0.0.1:3000"
             ],
             "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
             "allow_headers": [
                 "Content-Type", 
                 "Authorization",
                 "X-Requested-With",
                 "Accept",
                 "Origin",
                 "Cache-Control",      # ‚úÖ INCLUDE: Cache-Control header
                 "Pragma",             # ‚úÖ INCLUDE: For cache control
                 "Expires"             # ‚úÖ INCLUDE: For cache control
             ],
             "supports_credentials": True,
             "expose_headers": [
                 "Content-Type", 
                 "Authorization",
                 "Cache-Control",      # ‚úÖ EXPOSE: Cache-Control header
                 "Pragma",
                 "Expires"
             ]
         }
     })

# ‚ùå REMOVED: @app.after_request to avoid duplicate headers

# üîß CRITICAL: Make sessions permanent for OAuth
@app.before_request
def make_session_permanent():
    from flask import session
    session.permanent = True

logger.info("üîë OAuth-compatible session configuration applied")
logger.info("‚úÖ Single CORS configuration with Cache-Control support applied")

# ƒêƒÉng k√Ω c√°c Blueprint
app.register_blueprint(program_bp)
app.register_blueprint(config_bp, url_prefix='/api/config')
app.register_blueprint(query_bp)
app.register_blueprint(cutter_bp)
app.register_blueprint(hand_detection_bp)
app.register_blueprint(qr_detection_bp)
app.register_blueprint(roi_bp)

# üÜï NEW: Register cloud endpoints blueprint with error handling
try:
    app.register_blueprint(cloud_bp, name='cloud_endpoints')
    logger.info("‚úÖ Cloud endpoints registered: /api/cloud/*")
except ValueError as e:
    logger.warning(f"‚ö†Ô∏è Cloud blueprint already registered: {e}")
    # If already registered, skip (could be from config.py)
    pass

# üÜï NEW: Register sync endpoints blueprint with error handling
try:
    app.register_blueprint(sync_bp, url_prefix='/api/sync')
    logger.info("‚úÖ Sync endpoints registered: /api/sync/*")
except ValueError as e:
    logger.warning(f"‚ö†Ô∏è Sync blueprint already registered: {e}")
    # If already registered, skip
    pass
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Sync endpoints not available: {e}")
    pass

# H√†m ghi last_stop_time khi ·ª©ng d·ª•ng d·ª´ng
def exit_handler():
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        last_stop_time = datetime.now(tz=timezone(timedelta(hours=7))).strftime('%Y-%m-%d %H:%M:%S')
        cursor.execute("""
            INSERT OR REPLACE INTO program_status (key, value)
            VALUES ('last_stop_time', ?)
        """, (last_stop_time,))
        conn.commit()
        conn.close()
        logger.info("Application stopped gracefully")
    except Exception as e:
        logger.error(f"Error saving last_stop_time: {e}")

# ƒêƒÉng k√Ω exit_handler
atexit.register(exit_handler)

def is_port_in_use(port):
    """Ki·ªÉm tra xem c·ªïng c√≥ ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng hay kh√¥ng."""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        try:
            s.bind(("localhost", port))
            return False
        except OSError:
            return True

# Global flag ƒë·ªÉ tr√°nh multiple shutdown
_shutdown_in_progress = False

def signal_handler(sig, frame):
    """X·ª≠ l√Ω t√≠n hi·ªáu d·ª´ng ·ª©ng d·ª•ng m·ªôt c√°ch graceful"""
    global _shutdown_in_progress
    
    # Tr√°nh multiple shutdown
    if _shutdown_in_progress:
        print("\nForced shutdown...")
        os._exit(1)  # Force exit n·∫øu ƒë√£ shutdown r·ªìi
    
    _shutdown_in_progress = True
    print("\nShutting down gracefully... (Press Ctrl+C again to force)")
    
    try:
        logger.info("Received shutdown signal, stopping application...")
        
        # D·ª´ng scheduler
        if 'scheduler' in globals():
            scheduler.stop()
            logger.info("Scheduler stopped")
        
        # ƒê·ª£i c√°c thread k·∫øt th√∫c v·ªõi timeout ng·∫Øn h∆°n
        main_thread = threading.current_thread()
        for t in threading.enumerate():
            if t != main_thread and t.is_alive():
                try:
                    t.join(timeout=2)  # Gi·∫£m timeout xu·ªëng 2 gi√¢y
                    if t.is_alive():
                        logger.warning(f"Thread {t.name} did not stop gracefully")
                except:
                    pass  # Ignore errors during shutdown
        
        logger.info("Application shutdown complete")
        
    except Exception as e:
        logger.error(f"Error during shutdown: {e}")
    finally:
        os._exit(0)  # Force exit

# ƒêƒÉng k√Ω signal handler
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# üîß Debug: List all registered endpoints on startup
def log_registered_routes():
    """Log all registered Flask routes for debugging"""
    logger.info("üìã Registered Flask Routes:")
    for rule in app.url_map.iter_rules():
        methods = ', '.join(rule.methods - {'HEAD', 'OPTIONS'})
        logger.info(f"   {methods:15} {rule.rule}")

# Kh·ªüi ch·∫°y ·ª©ng d·ª•ng
if __name__ == "__main__":
    port = 8080
    
    # Ki·ªÉm tra port tr∆∞·ªõc khi kh·ªüi ch·∫°y
    if is_port_in_use(port):
        logger.error(f"Port {port} is already in use!")
        sys.exit(1)
    
    logger.info(f"Starting VTrack application on port {port}")
    logger.info(f"üîë OAuth session security: ‚úÖ Enabled")
    
    # üîß Debug: Log registered routes
    log_registered_routes()
    
    try:
        app.run(
            host='0.0.0.0',
            port=port,
            debug=False,
            use_reloader=False,
            threaded=True
        )
    except KeyboardInterrupt:
        logger.info("Application interrupted by user")
    except Exception as e:
        logger.error(f"Application error: {e}")
    finally:
        logger.info("Application terminated")
```
## üìÑ File: `debug_auth.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/debug_auth.py`

```python
#!/usr/bin/env python3
"""
Standalone authentication debugging script for V_track
Usage: python backend/debug_auth.py
"""

import sys
import os
import json
import hashlib

# Add modules to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'modules'))
sys.path.append(os.path.dirname(__file__))

def debug_authentication(source_id=82):
    """Debug authentication step by step"""
    print(f"üîç Debugging authentication for source {source_id}")
    print("=" * 50)
    
    try:
        # Step 1: Import dependencies
        print("Step 1: Importing dependencies...")
        from modules.db_utils import get_db_connection
        from google.oauth2.credentials import Credentials
        from google.auth.transport.requests import Request
        from pydrive2.auth import GoogleAuth
        from pydrive2.drive import GoogleDrive
        print("‚úÖ All imports successful")
        
        # Step 2: Get source config
        print("\nStep 2: Loading source config...")
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            SELECT config FROM video_sources 
            WHERE id = ? AND source_type = 'cloud' AND active = 1
        """, (source_id,))
        result = cursor.fetchone()
        conn.close()
        
        if not result or not result[0]:
            print(f"‚ùå No source config found for source {source_id}")
            return False
        
        print("‚úÖ Source config found")
        
        # Step 3: Parse config
        print("\nStep 3: Parsing config...")
        config_data = json.loads(result[0])
        user_email = config_data.get('user_email')
        print(f"üìß User email: {user_email}")
        
        if not user_email:
            print("‚ùå No user_email in source config")
            return False
        
        # Step 4: Calculate file path
        print("\nStep 4: Calculating credentials file path...")
        tokens_dir = os.path.join(os.path.dirname(__file__), 'modules', 'sources', 'tokens')
        email_hash = hashlib.sha256(user_email.encode()).hexdigest()[:16]
        token_filename = f"google_drive_{email_hash}.json"
        token_filepath = os.path.join(tokens_dir, token_filename)
        
        print(f"üìÅ Tokens directory: {tokens_dir}")
        print(f"üìÑ Token filename: {token_filename}")
        print(f"üîó Full path: {token_filepath}")
        print(f"üìã File exists: {os.path.exists(token_filepath)}")
        
        if not os.path.exists(token_filepath):
            print(f"‚ùå Credentials file not found: {token_filepath}")
            # List available files
            if os.path.exists(tokens_dir):
                files = os.listdir(tokens_dir)
                print(f"Available files in tokens dir: {files}")
            return False
        
        # Step 5: Load encrypted storage
        print("\nStep 5: Loading encrypted storage...")
        with open(token_filepath, 'r') as f:
            encrypted_storage = json.load(f)
        
        print(f"üîë Storage keys: {list(encrypted_storage.keys())}")
        print(f"üì¶ Has encrypted_data: {'encrypted_data' in encrypted_storage}")
        
        # Step 6: Test decryption
        print("\nStep 6: Testing decryption...")
        try:
            from modules.sources.cloud_endpoints import decrypt_credentials
            credential_data = decrypt_credentials(encrypted_storage['encrypted_data'])
            
            if not credential_data:
                print("‚ùå Decryption returned None")
                return False
            
            print("‚úÖ Decryption successful")
            print(f"üîê Credential keys: {list(credential_data.keys())}")
            
        except Exception as e:
            print(f"‚ùå Decryption failed: {e}")
            return False
        
        # Step 7: Test credentials object creation
        print("\nStep 7: Creating credentials object...")
        try:
            credentials = Credentials(
                token=credential_data['token'],
                refresh_token=credential_data['refresh_token'],
                token_uri=credential_data['token_uri'],
                client_id=credential_data['client_id'],
                client_secret=credential_data['client_secret'],
                scopes=credential_data['scopes']
            )
            print("‚úÖ Credentials object created")
            print(f"‚è∞ Credentials expired: {credentials.expired}")
            print(f"üîÑ Has refresh token: {credentials.refresh_token is not None}")
            
        except Exception as e:
            print(f"‚ùå Credentials object creation failed: {e}")
            return False
        
        # Step 8: Test credential refresh if needed
        if credentials.expired and credentials.refresh_token:
            print("\nStep 8: Refreshing expired credentials...")
            try:
                credentials.refresh(Request())
                print("‚úÖ Credentials refreshed successfully")
            except Exception as e:
                print(f"‚ùå Credential refresh failed: {e}")
                return False
        else:
            print("\nStep 8: Credentials are valid, no refresh needed")
        
        # Step 9: Test PyDrive authentication
        print("\nStep 9: Testing PyDrive authentication...")
        try:
            gauth = GoogleAuth()
            gauth.credentials = credentials
            drive = GoogleDrive(gauth)
            
            # Test connection
            about = drive.GetAbout()
            print("‚úÖ PyDrive authentication successful")
            print(f"üë§ User name: {about.get('name', 'Unknown')}")
            print(f"üìß User email: {about.get('user', {}).get('emailAddress', 'Unknown')}")
            
        except Exception as e:
            print(f"‚ùå PyDrive authentication failed: {e}")
            print(f"Full error details: {type(e).__name__}: {e}")
            import traceback
            print("Traceback:")
            traceback.print_exc()
            return False
        
        print("\n" + "=" * 50)
        print("üéâ ALL AUTHENTICATION STEPS SUCCESSFUL!")
        return True
        
    except Exception as e:
        print(f"‚ùå Unexpected error: {e}")
        import traceback
        print("Full traceback:")
        traceback.print_exc()
        return False

if __name__ == "__main__":
    debug_authentication()

```
## üìÑ File: `debug_auth_enhanced.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/debug_auth_enhanced.py`

```python
#!/usr/bin/env python3
"""
Enhanced authentication debugging script for V_track
Usage: python backend/debug_auth_enhanced.py
"""

import sys
import os
import json
import hashlib

# Add modules to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'modules'))
sys.path.append(os.path.dirname(__file__))

def test_encryption_keys():
    """Test encryption key consistency"""
    print("üîê Testing encryption keys...")
    
    # Test 1: Check environment variable
    env_key = os.getenv('ENCRYPTION_KEY')
    print(f"üîë ENCRYPTION_KEY env var: {'SET' if env_key else 'NOT SET'}")
    
    if env_key:
        print(f"   Key length: {len(env_key)} chars")
        print(f"   Key type: {type(env_key)}")
    else:
        print("‚ö†Ô∏è  WARNING: No ENCRYPTION_KEY environment variable!")
        print("   This means a new key is generated each time!")
    
    # Test 2: Import and check actual key used
    try:
        from modules.sources.cloud_endpoints import ENCRYPTION_KEY
        print(f"üîë Actual ENCRYPTION_KEY: {len(ENCRYPTION_KEY)} bytes")
        print(f"   Key (first 16 chars): {ENCRYPTION_KEY[:16] if isinstance(ENCRYPTION_KEY, bytes) else 'Not bytes'}")
        return ENCRYPTION_KEY
    except Exception as e:
        print(f"‚ùå Error importing ENCRYPTION_KEY: {e}")
        return None

def test_manual_decryption(encryption_key=None):
    """Test manual decryption with different approaches"""
    print("\nüîì Testing manual decryption...")
    
    try:
        # Load the encrypted file
        tokens_dir = os.path.join(os.path.dirname(__file__), 'modules', 'sources', 'tokens')
        token_filepath = os.path.join(tokens_dir, 'google_drive_2146a5516664cebd.json')
        
        with open(token_filepath, 'r') as f:
            encrypted_storage = json.load(f)
        
        encrypted_data = encrypted_storage['encrypted_data']
        print(f"üì¶ Loaded encrypted data: {len(encrypted_data)} characters")
        
        # Test current encryption key
        if encryption_key:
            try:
                from cryptography.fernet import Fernet
                import base64
                
                fernet = Fernet(encryption_key)
                encrypted_bytes = base64.b64decode(encrypted_data.encode())
                decrypted_data = fernet.decrypt(encrypted_bytes)
                credential_data = json.loads(decrypted_data.decode())
                
                print("‚úÖ Decryption with current key: SUCCESS")
                print(f"üîê Credential keys: {list(credential_data.keys())}")
                return True
                
            except Exception as e:
                print(f"‚ùå Decryption with current key: FAILED")
                print(f"   Error: {e}")
                return False
        else:
            print("‚ùå No encryption key available for testing")
            return False
            
    except Exception as e:
        print(f"‚ùå Manual decryption test failed: {e}")
        return False

def fix_encryption_key_issue():
    """Generate and set a persistent encryption key"""
    print("\nüîß Generating persistent encryption key...")
    
    try:
        from cryptography.fernet import Fernet
        
        # Generate new key
        new_key = Fernet.generate_key()
        print(f"üîë Generated new key: {new_key}")
        
        # Create .env file or print instruction
        env_file_path = os.path.join(os.path.dirname(__file__), '.env')
        
        env_content = f"ENCRYPTION_KEY={new_key.decode()}\n"
        
        # Check if .env exists
        if os.path.exists(env_file_path):
            print(f"üìÑ .env file exists at: {env_file_path}")
            with open(env_file_path, 'r') as f:
                existing_content = f.read()
            
            if 'ENCRYPTION_KEY' in existing_content:
                print("‚ö†Ô∏è  ENCRYPTION_KEY already exists in .env file")
                print("   Manual action required:")
                print(f"   Update ENCRYPTION_KEY={new_key.decode()}")
            else:
                # Append to existing .env
                with open(env_file_path, 'a') as f:
                    f.write(f"\n{env_content}")
                print(f"‚úÖ Added ENCRYPTION_KEY to existing .env file")
        else:
            # Create new .env file
            with open(env_file_path, 'w') as f:
                f.write(env_content)
            print(f"‚úÖ Created new .env file with ENCRYPTION_KEY")
        
        print("\nüìã Next steps:")
        print("1. Restart the backend server")
        print("2. Re-authenticate from frontend to generate new credentials")
        print("3. Test sync again")
        
        return new_key
        
    except Exception as e:
        print(f"‚ùå Error generating encryption key: {e}")
        return None

def main():
    print("üîç Enhanced Authentication Debugging")
    print("=" * 50)
    
    # Test encryption keys
    encryption_key = test_encryption_keys()
    
    # Test manual decryption
    decryption_success = test_manual_decryption(encryption_key)
    
    if not decryption_success:
        print("\n‚ùå DECRYPTION FAILED!")
        print("This confirms the encryption key mismatch issue.")
        
        # Offer to fix
        response = input("\nüîß Would you like to generate a persistent encryption key? (y/n): ")
        if response.lower() in ['y', 'yes']:
            fix_encryption_key_issue()
        else:
            print("Manual fix required:")
            print("1. Set ENCRYPTION_KEY environment variable")
            print("2. Or restart backend and re-authenticate")
    else:
        print("\n‚úÖ DECRYPTION SUCCESSFUL!")
        print("The encryption key is working correctly.")
        print("The issue might be elsewhere in the authentication flow.")

if __name__ == "__main__":
    main()

```
## üìÑ File: `cutter_bp.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/blueprints/cutter_bp.py`

```python
import sqlite3
import os
from datetime import datetime
import subprocess
from flask import Blueprint, request, jsonify
from modules.db_utils import get_db_connection
import ast
from modules.technician.cutter.cutter_complete import cut_complete_event
from modules.technician.cutter.cutter_incomplete import cut_incomplete_event, merge_incomplete_events
from modules.technician.cutter.cutter_utils import generate_output_filename, update_event_in_db
from modules.scheduler.db_sync import db_rwlock

cutter_bp = Blueprint('cutter', __name__)

with db_rwlock.gen_rlock():
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT output_path FROM processing_config LIMIT 1")
    result = cursor.fetchone()
    output_dir = result[0] if result else os.path.join(os.path.dirname(os.path.abspath(__file__)), "../../resources/output_clips")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    conn.close()

def get_video_duration(video_file):
    try:
        cmd = ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", video_file]
        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return float(result.stdout.strip())
    except Exception:
        return None

def cut_and_update_events(selected_events, tracking_codes_filter, brand_name="Alan"):
    with db_rwlock.gen_wlock():
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT video_buffer, max_packing_time FROM processing_config LIMIT 1")
        result = cursor.fetchone()
        video_buffer = result[0] if result else 5
        max_packing_time = result[1] if result else 120
        cut_files = []

        for event in selected_events:
            event_id = event.get("event_id")
            ts = event.get("ts")
            te = event.get("te")
            video_file = event.get("video_file")
            cursor.execute("SELECT is_processed FROM events WHERE event_id = ?", (event_id,))
            is_processed = cursor.fetchone()[0]
            if is_processed:
                print(f"B·ªè qua: S·ª± ki·ªán {event_id} ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥")
                continue

            has_ts = ts is not None
            has_te = te is not None
            is_incomplete = (has_ts and not has_te) or (not has_ts and has_te)

            if is_incomplete:
                next_event = None
                if has_ts and not has_te:
                    cursor.execute("SELECT event_id, ts, te, video_file, packing_time_start, packing_time_end, tracking_codes, is_processed FROM events WHERE event_id = ? AND is_processed = 0", (event_id + 1,))
                    next_event_row = cursor.fetchone()
                    if next_event_row:
                        next_event = {
                            "event_id": next_event_row[0],
                            "ts": next_event_row[1],
                            "te": next_event_row[2],
                            "video_file": next_event_row[3],
                            "packing_time_start": next_event_row[4],
                            "packing_time_end": next_event_row[5],
                            "tracking_codes": next_event_row[6],
                            "is_processed": next_event_row[7]
                        }
                elif not has_ts and has_te:
                    cursor.execute("SELECT event_id, ts, te, video_file, packing_time_start, packing_time_end, tracking_codes, is_processed FROM events WHERE event_id = ? AND is_processed = 0", (event_id - 1,))
                    prev_event_row = cursor.fetchone()
                    if prev_event_row:
                        next_event = {
                            "event_id": prev_event_row[0],
                            "ts": prev_event_row[1],
                            "te": prev_event_row[2],
                            "video_file": prev_event_row[3],
                            "packing_time_start": prev_event_row[4],
                            "packing_time_end": prev_event_row[5],
                            "tracking_codes": prev_event_row[6],
                            "is_processed": prev_event_row[7]
                        }

                if next_event:
                    next_event_id = next_event.get("event_id")
                    next_ts = next_event.get("ts")
                    next_te = next_event.get("te")
                    next_video_file = next_event.get("video_file")
                    next_has_ts = next_ts is not None
                    next_has_te = next_te is not None
                    can_merge = (has_ts and not has_te and not next_has_ts and next_has_te) or (not has_ts and has_te and next_has_ts and not next_has_te)

                    if can_merge:
                        video_length_a = get_video_duration(video_file)
                        video_length_b = get_video_duration(next_video_file)
                        if video_length_a is None or video_length_b is None:
                            print(f"L·ªói: Kh√¥ng th·ªÉ l·∫•y ƒë·ªô d√†i video {video_file} ho·∫∑c {next_video_file}")
                            continue

                        output_file_a = generate_output_filename(event, tracking_codes_filter, output_dir, brand_name)
                        print(f"ƒêang x·ª≠ l√Ω s·ª± ki·ªán {event_id}: output_file={output_file_a}, packing_time_start={event.get('packing_time_start')}, packing_time_end={event.get('packing_time_end')}")
                        if cut_incomplete_event(event, video_buffer, video_length_a, output_file_a):
                            update_event_in_db(cursor, event_id, output_file_a)

                        output_file_b = generate_output_filename(next_event, tracking_codes_filter, output_dir, brand_name)
                        print(f"ƒêang x·ª≠ l√Ω s·ª± ki·ªán {next_event_id}: output_file={output_file_b}, packing_time_start={next_event.get('packing_time_start')}, packing_time_end={next_event.get('packing_time_end')}")
                        if cut_incomplete_event(next_event, video_buffer, video_length_b, output_file_b):
                            update_event_in_db(cursor, next_event_id, output_file_b)

                        if has_ts and not has_te:
                            event_a = event
                            event_b = next_event
                            video_length_event_a = video_length_a
                            video_length_event_b = video_length_b
                        else:
                            event_a = next_event
                            event_b = event
                            video_length_event_a = video_length_b
                            video_length_event_b = video_length_a

                        print(f"G·ªçi merge_incomplete_events: event_a (ID: {event_a.get('event_id')}, ts: {event_a.get('ts')}, te: {event_a.get('te')}), event_b (ID: {event_b.get('event_id')}, ts: {event_b.get('ts')}, te: {event_b.get('te')})")

                        merged_file = merge_incomplete_events(event_a, event_b, video_buffer, video_length_event_a, video_length_event_b, output_dir, max_packing_time, brand_name)
                        if merged_file:
                            update_event_in_db(cursor, event_id, merged_file)
                            update_event_in_db(cursor, next_event_id, merged_file)
                            cut_files.append(merged_file)
                        continue

            video_length = get_video_duration(video_file)
            if video_length is None:
                print(f"L·ªói: Kh√¥ng th·ªÉ l·∫•y ƒë·ªô d√†i video {video_file}")
                continue

            output_file = generate_output_filename(event, tracking_codes_filter, output_dir, brand_name)
            print(f"ƒêang x·ª≠ l√Ω s·ª± ki·ªán {event_id}: output_file={output_file}, packing_time_start={event.get('packing_time_start')}, packing_time_end={event.get('packing_time_end')}")

            if has_ts and has_te:
                if cut_complete_event(event, video_buffer, video_length, output_file):
                    update_event_in_db(cursor, event_id, output_file)
                    cut_files.append(output_file)
            elif is_incomplete:
                if cut_incomplete_event(event, video_buffer, video_length, output_file):
                    update_event_in_db(cursor, event_id, output_file)
                    cut_files.append(output_file)
            else:
                print(f"B·ªè qua: S·ª± ki·ªán {event_id} kh√¥ng c√≥ ts ho·∫∑c te")
                continue

        conn.commit()
        conn.close()
    return cut_files

@cutter_bp.route('/cut-videos', methods=['POST'])
def cut_videos():
    data = request.get_json()
    selected_events = data.get('selected_events', [])
    tracking_codes_filter = data.get('tracking_codes_filter', [])

    if not selected_events:
        return jsonify({"error": "No selected events provided"}), 400

    try:
        cut_files = cut_and_update_events(selected_events, tracking_codes_filter)
        return jsonify({"message": "Videos cut successfully", "cut_files": cut_files}), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500
```
## üìÑ File: `__init__.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/blueprints/__init__.py`

```python

```
## üìÑ File: `hand_detection_bp.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/blueprints/hand_detection_bp.py`

```python
from flask import Blueprint, request, jsonify
from modules.technician.hand_detection import select_roi
import subprocess
import os
import json
import logging
import threading
import gc
import psutil
import sys

hand_detection_bp = Blueprint('hand_detection', __name__)

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ‚úÖ NEW: Model Management & Threading Protection
class ModelManager:
    """Singleton pattern for managing AI models to prevent memory leaks"""
    _instance = None
    _lock = threading.Lock()
    _models_loaded = False
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(ModelManager, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not hasattr(self, 'initialized'):
            self.model_lock = threading.Lock()
            self.initialized = True
            logger.info("ModelManager initialized")
    
    def get_memory_usage(self):
        """Get current memory usage for monitoring"""
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        return {
            'rss_mb': memory_info.rss / 1024 / 1024,
            'vms_mb': memory_info.vms / 1024 / 1024
        }
    
    def cleanup_memory(self):
        """Force garbage collection to free up memory"""
        gc.collect()
        memory_after = self.get_memory_usage()
        logger.info(f"Memory cleanup completed. Current usage: {memory_after['rss_mb']:.1f} MB")

# Global model manager instance
model_manager = ModelManager()

@hand_detection_bp.route('/select-roi', methods=['POST'])
def select_roi_endpoint():
    """
    ‚úÖ MAIN ENDPOINT: Direct function call for ROI selection with hand detection
    This is the recommended endpoint to use (no subprocess overhead)
    """
    try:
        data = request.get_json()
        video_path = data.get('videoPath')
        camera_id = data.get('cameraId')
        step = data.get('step', 'packing')
        
        logger.debug(f"[DIRECT] ROI selection - step: {step}, video: {video_path}, camera: {camera_id}")
        
        # Input validation
        if not video_path or not camera_id:
            logger.error("[DIRECT] Missing videoPath or cameraId")
            return jsonify({"success": False, "error": "Thi·∫øu videoPath ho·∫∑c cameraId."}), 400
        
        if not os.path.exists(video_path):
            logger.error(f"[DIRECT] Video path does not exist: {video_path}")
            return jsonify({"success": False, "error": "ƒê∆∞·ªùng d·∫´n video kh√¥ng t·ªìn t·∫°i."}), 404
        
        # Clean up previous results
        if os.path.exists("/tmp/roi.json"):
            os.remove("/tmp/roi.json")
            logger.info("[DIRECT] Cleaned up previous ROI results")
        
        # ‚úÖ Memory monitoring before processing
        memory_before = model_manager.get_memory_usage()
        logger.info(f"[DIRECT] Memory before processing: {memory_before['rss_mb']:.1f} MB")
        
        # ‚úÖ Thread-safe model usage
        with model_manager.model_lock:
            logger.debug("[DIRECT] Acquired model lock, starting ROI selection")
            result = select_roi(video_path, camera_id, step)
            logger.debug(f"[DIRECT] ROI selection completed with result: {result}")
        
        # ‚úÖ Memory monitoring after processing
        memory_after = model_manager.get_memory_usage()
        memory_diff = memory_after['rss_mb'] - memory_before['rss_mb']
        logger.info(f"[DIRECT] Memory after processing: {memory_after['rss_mb']:.1f} MB (diff: {memory_diff:+.1f} MB)")
        
        # ‚úÖ Cleanup if memory usage increased significantly
        if memory_diff > 100:  # More than 100MB increase
            logger.warning(f"[DIRECT] High memory increase detected ({memory_diff:.1f} MB), cleaning up")
            model_manager.cleanup_memory()
        
        # Return result
        status_code = 200 if result["success"] else 400
        return jsonify(result), status_code
    
    except Exception as e:
        logger.error(f"[DIRECT] Exception in select_roi_endpoint: {str(e)}", exc_info=True)
        
        # ‚úÖ Emergency memory cleanup on exception
        try:
            model_manager.cleanup_memory()
        except:
            pass
            
        return jsonify({
            "success": False, 
            "error": f"L·ªói h·ªá th·ªëng: {str(e)}"
        }), 500

@hand_detection_bp.route('/run-select-roi', methods=['POST'])
def run_select_roi_endpoint():
    """
    ‚úÖ SUBPROCESS ENDPOINT: Subprocess-based ROI selection (s·ª≠ d·ª•ng approach c≈© - ƒë√£ working)
    """
    logger.warning("[SUBPROCESS] Using subprocess endpoint for GUI stability")
    
    try:
        data = request.get_json()
        video_path = data.get('videoPath')
        camera_id = data.get('cameraId', 'default_camera')
        step = data.get('step', 'packing')
        
        # Input validation
        if not video_path:
            logger.error("[SUBPROCESS] Missing videoPath parameter")
            return jsonify({"success": False, "error": "Thi·∫øu videoPath."}), 400
        
        # Check video file exists
        if not os.path.exists(video_path):
            logger.error(f"[SUBPROCESS] Video file not found: {video_path}")
            return jsonify({"success": False, "error": "ƒê∆∞·ªùng d·∫´n video kh√¥ng t·ªìn t·∫°i."}), 404
        
        logger.info(f"[SUBPROCESS] Running hand_detection.py with video_path: {video_path}, camera_id: {camera_id}, step: {step}")
        
        # Clean up previous results
        if os.path.exists("/tmp/roi.json"):
            os.remove("/tmp/roi.json")
            logger.info("[SUBPROCESS] Cleaned up previous ROI results")
        
        # ‚úÖ SIMPLE APPROACH: Use simple subprocess call like the old working version
        # Working directory s·∫Ω l√† backend/ ƒë·ªÉ c√≥ th·ªÉ access modules/
        BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        BACKEND_DIR = os.path.join(BASE_DIR, "backend")
        
        logger.debug(f"[SUBPROCESS] BASE_DIR: {BASE_DIR}")
        logger.debug(f"[SUBPROCESS] BACKEND_DIR (working dir): {BACKEND_DIR}")
        
        # Check script exists
        script_path = os.path.join(BACKEND_DIR, "modules", "technician", "hand_detection.py")
        if not os.path.exists(script_path):
            logger.error(f"[SUBPROCESS] Script not found: {script_path}")
            return jsonify({"success": False, "error": f"Script {script_path} kh√¥ng t·ªìn t·∫°i."}), 404
        
        try:
            # ‚úÖ SIMPLE: Use same approach as old working qr_detection_bp.py
            result = subprocess.run(
                ["python3", "modules/technician/hand_detection.py", video_path, camera_id, step],
                cwd=BACKEND_DIR,  # Working directory = backend/
                capture_output=True,
                text=True,
                timeout=300
            )
            
            logger.debug(f"[SUBPROCESS] Script exit code: {result.returncode}")
            logger.debug(f"[SUBPROCESS] Script stdout: {result.stdout}")
            logger.debug(f"[SUBPROCESS] Script stderr: {result.stderr}")
            
        except subprocess.TimeoutExpired:
            logger.error("[SUBPROCESS] Script execution timeout")
            return jsonify({"success": False, "error": "H·∫øt th·ªùi gian ch·ªù khi ch·∫°y script (300s)."}), 500
        except FileNotFoundError as e:
            logger.error(f"[SUBPROCESS] Python executable not found: {e}")
            return jsonify({"success": False, "error": "Kh√¥ng t√¨m th·∫•y Python executable."}), 500
        
        # Check execution result
        if result.returncode != 0:
            error_msg = f"Script failed with code {result.returncode}"
            if result.stderr:
                error_msg += f": {result.stderr}"
            if result.stdout:
                error_msg += f"\nOutput: {result.stdout}"
            
            logger.error(f"[SUBPROCESS] {error_msg}")
            return jsonify({
                "success": False, 
                "error": f"L·ªói khi ch·∫°y script (code {result.returncode})",
                "details": result.stderr,
                "stdout": result.stdout
            }), 500
        
        # Check result file exists
        if not os.path.exists("/tmp/roi.json"):
            logger.error("[SUBPROCESS] ROI result file not created")
            logger.debug(f"[SUBPROCESS] Script stdout: {result.stdout}")
            return jsonify({
                "success": False, 
                "error": "Script ch·∫°y th√†nh c√¥ng nh∆∞ng kh√¥ng t·∫°o file k·∫øt qu·∫£.",
                "stdout": result.stdout
            }), 500
        
        # Read and return result
        try:
            with open("/tmp/roi.json", "r", encoding='utf-8') as f:
                roi_result = json.load(f)
            
            logger.info(f"[SUBPROCESS] ROI result loaded successfully: {roi_result.get('success', False)}")
            status_code = 200 if roi_result.get("success", False) else 400
            return jsonify(roi_result), status_code
            
        except json.JSONDecodeError as e:
            logger.error(f"[SUBPROCESS] Invalid JSON in result file: {e}")
            return jsonify({"success": False, "error": "File k·∫øt qu·∫£ c√≥ ƒë·ªãnh d·∫°ng JSON kh√¥ng h·ª£p l·ªá."}), 500
        except Exception as e:
            logger.error(f"[SUBPROCESS] Error reading result file: {e}")
            return jsonify({"success": False, "error": f"L·ªói ƒë·ªçc file k·∫øt qu·∫£: {str(e)}"}), 500
    
    except Exception as e:
        logger.error(f"[SUBPROCESS] Unexpected error in run-select-roi: {str(e)}", exc_info=True)
        return jsonify({"success": False, "error": f"L·ªói h·ªá th·ªëng: {str(e)}"}), 500

@hand_detection_bp.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint with memory monitoring"""
    try:
        memory_usage = model_manager.get_memory_usage()
        return jsonify({
            "status": "healthy",
            "memory_usage_mb": memory_usage['rss_mb'],
            "model_manager_initialized": hasattr(model_manager, 'initialized')
        }), 200
    except Exception as e:
        return jsonify({
            "status": "unhealthy",
            "error": str(e)
        }), 500

@hand_detection_bp.route('/cleanup', methods=['POST'])
def manual_cleanup():
    """Manual memory cleanup endpoint for debugging"""
    try:
        memory_before = model_manager.get_memory_usage()
        model_manager.cleanup_memory()
        memory_after = model_manager.get_memory_usage()
        
        return jsonify({
            "success": True,
            "memory_before_mb": memory_before['rss_mb'],
            "memory_after_mb": memory_after['rss_mb'],
            "memory_freed_mb": memory_before['rss_mb'] - memory_after['rss_mb']
        }), 200
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500
```
## üìÑ File: `roi_bp.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/blueprints/roi_bp.py`

```python
from flask import Blueprint, request, jsonify, send_file, make_response
from modules.technician.hand_detection import finalize_roi
import os
import glob
import json
import sqlite3
from datetime import datetime
import logging

roi_bp = Blueprint('roi', __name__)

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# ‚úÖ CORRECTED: Path calculation to project root
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))  # Go up to project root V_Track/
DB_PATH = os.path.join(BASE_DIR, "backend", "database", "events.db")
CAMERA_ROI_DIR = os.path.join(BASE_DIR, "resources", "output_clips", "CameraROI")

# ‚úÖ NEW: Global OPTIONS handler for all routes in this blueprint
@roi_bp.before_request
def handle_preflight():
    if request.method == "OPTIONS":
        response = make_response()
        response.headers.add("Access-Control-Allow-Origin", "http://localhost:3000")
        response.headers.add("Access-Control-Allow-Headers", "Content-Type, Authorization, Cache-Control, Pragma, Expires")
        response.headers.add("Access-Control-Allow-Methods", "GET, POST, PUT, DELETE, OPTIONS")
        response.headers.add("Access-Control-Allow-Credentials", "true")
        return response

@roi_bp.route('/finalize-roi', methods=['POST', 'OPTIONS'])
def finalize_roi_endpoint():
    try:
        # ‚úÖ FIXED: Only parse JSON for POST requests (OPTIONS already handled by before_request)
        if request.method == 'POST':
            data = request.get_json()
            video_path = data.get('videoPath')
            camera_id = data.get('cameraId')
            rois = data.get('rois')

            if not video_path or not camera_id or not rois:
                return jsonify({"success": False, "error": "Thi·∫øu videoPath, cameraId ho·∫∑c rois."}), 400

            if not os.path.exists(video_path):
                return jsonify({"success": False, "error": "ƒê∆∞·ªùng d·∫´n video kh√¥ng t·ªìn t·∫°i."}), 404

            packing_roi = [0, 0, 0, 0]
            if os.path.exists("/tmp/roi.json"):
                with open("/tmp/roi.json", "r") as f:
                    roi_data = json.load(f)
                    if roi_data.get("success") and "roi" in roi_data:
                        packing_roi = [roi_data["roi"]["x"], roi_data["roi"]["y"], roi_data["roi"]["w"], roi_data["roi"]["h"]]
            
            qr_mvd_area = [0, 0, 0, 0]
            qr_trigger_area = [0, 0, 0, 0]
            table_type = None
            if os.path.exists("/tmp/qr_roi.json"):
                with open("/tmp/qr_roi.json", "r") as f:
                    qr_roi_data = json.load(f)
                    table_type = qr_roi_data.get("table_type")
                    for roi in rois:
                        if roi["type"] == "mvd":
                            qr_mvd_area = [roi["x"], roi["y"], roi["w"], roi["h"]]
                        elif roi["type"] == "trigger" and table_type == "standard":
                            qr_trigger_area = [roi["x"], roi["y"], roi["w"], roi["h"]]

            profile_name = camera_id
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            cursor.execute("SELECT 1 FROM packing_profiles WHERE profile_name = ?", (profile_name,))
            exists = cursor.fetchone()
            
            if exists:
                cursor.execute('''
                    UPDATE packing_profiles
                    SET qr_trigger_area = ?, qr_motion_area = ?, qr_mvd_area = ?, packing_area = ?,
                        min_packing_time = ?, jump_time_ratio = ?, scan_mode = ?, fixed_threshold = ?, margin = ?, additional_params = ?, mvd_jump_ratio = ?
                    WHERE profile_name = ?
                ''', (
                    json.dumps(qr_trigger_area),
                    None,  # qr_motion_area
                    json.dumps(qr_mvd_area),
                    json.dumps(packing_roi),
                    10, 0.5, "full", 20, 60, json.dumps({}),
                    None,  # mvd_jump_ratio
                    profile_name
                ))
            else:
                cursor.execute('''
                    INSERT INTO packing_profiles (
                        profile_name, qr_trigger_area, qr_motion_area, qr_mvd_area, packing_area,
                        min_packing_time, jump_time_ratio, scan_mode, fixed_threshold, margin, additional_params, mvd_jump_ratio
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    profile_name,
                    json.dumps(qr_trigger_area),
                    None,  # qr_motion_area
                    json.dumps(qr_mvd_area),
                    json.dumps(packing_roi),
                    10, 0.5, "full", 20, 60, json.dumps({}),
                    None   # mvd_jump_ratio
                ))
            conn.commit()
            conn.close()
            logging.info(f"L∆∞u ROI v√†o packing_profiles v·ªõi profile_name: {profile_name}")

            result = finalize_roi(video_path, camera_id, rois)
            return jsonify(result), 200 if result["success"] else 400
    
    except Exception as e:
        logging.error(f"Exception in finalize_roi_endpoint: {str(e)}")
        return jsonify({"success": False, "error": f"L·ªói h·ªá th·ªëng: {str(e)}"}), 500

@roi_bp.route('/get-roi-frame', methods=['GET', 'OPTIONS'])
def get_roi_frame():
    # ‚úÖ OPTIONS already handled by before_request
    if request.method == 'GET':
        camera_id = request.args.get('camera_id')
        file = request.args.get('file')
        
        logging.info(f"[GET-ROI-FRAME] camera_id: {camera_id}, file: {file}")
        logging.info(f"[GET-ROI-FRAME] CAMERA_ROI_DIR: {CAMERA_ROI_DIR}")
        
        if not camera_id or not file:
            return jsonify({"success": False, "error": "Thi·∫øu camera_id ho·∫∑c file."}), 400

        # ‚úÖ IMPROVED: Handle different file naming patterns
        if file == "roi_packing.jpg":
            file_name = f"camera_{camera_id}_roi_packing.jpg"
        elif file == "roi_MVD.jpg":
            file_name = f"camera_{camera_id}_roi_MVD.jpg"
        elif file == "roi_trigger.jpg":
            file_name = f"camera_{camera_id}_roi_trigger.jpg"
        elif file == "original.jpg":
            file_name = f"camera_{camera_id}_original.jpg"
        else:
            # Fallback: use the file parameter as-is with camera prefix
            file_name = f"camera_{camera_id}_{file}"
        
        file_path = os.path.join(CAMERA_ROI_DIR, file_name)
        logging.info(f"[GET-ROI-FRAME] Attempting to fetch file: {file_path}")
        logging.info(f"[GET-ROI-FRAME] File exists: {os.path.exists(file_path)}")

        if not os.path.exists(file_path):
            logging.error(f"[GET-ROI-FRAME] File kh√¥ng t·ªìn t·∫°i: {file_path}")
            # ‚úÖ List available files for debugging
            try:
                if os.path.exists(CAMERA_ROI_DIR):
                    available_files = os.listdir(CAMERA_ROI_DIR)
                    camera_files = [f for f in available_files if f.startswith(f"camera_{camera_id}")]
                    logging.info(f"[GET-ROI-FRAME] Available files for camera {camera_id}: {camera_files}")
                    logging.info(f"[GET-ROI-FRAME] All files in CAMERA_ROI_DIR: {available_files}")
                else:
                    logging.error(f"[GET-ROI-FRAME] CAMERA_ROI_DIR does not exist: {CAMERA_ROI_DIR}")
            except Exception as e:
                logging.error(f"[GET-ROI-FRAME] Error listing directory: {e}")
            
            return jsonify({
                "success": False, 
                "error": "Kh√¥ng t√¨m th·∫•y ·∫£nh.", 
                "file_path": file_path,
                "camera_roi_dir": CAMERA_ROI_DIR,
                "dir_exists": os.path.exists(CAMERA_ROI_DIR)
            }), 404

        try:
            # ‚úÖ Serve file with proper cache control headers
            response = send_file(file_path, mimetype='image/jpeg', as_attachment=False)
            response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
            response.headers['Pragma'] = 'no-cache'
            response.headers['Expires'] = '0'
            return response
        except Exception as e:
            logging.error(f"[GET-ROI-FRAME] Error serving file: {e}")
            return jsonify({"success": False, "error": f"L·ªói khi serve file: {str(e)}"}), 500

@roi_bp.route('/get-final-roi-frame', methods=['GET', 'OPTIONS'])
def get_final_roi_frame():
    # ‚úÖ OPTIONS already handled by before_request
    if request.method == 'GET':
        camera_id = request.args.get('camera_id')
        logging.info(f"[GET-FINAL-ROI] camera_id: {camera_id}")
        logging.info(f"[GET-FINAL-ROI] CAMERA_ROI_DIR: {CAMERA_ROI_DIR}")
        
        if not camera_id:
            return jsonify({"success": False, "error": "Thi·∫øu camera_id."}), 400
        
        final_pattern = os.path.join(CAMERA_ROI_DIR, f"camera_{camera_id}_roi_final_*.jpg")
        final_files = glob.glob(final_pattern)
        logging.info(f"[GET-FINAL-ROI] Pattern: {final_pattern}")
        logging.info(f"[GET-FINAL-ROI] Files found for camera_id={camera_id}: {final_files}")

        if not final_files:
            logging.error(f"[GET-FINAL-ROI] Kh√¥ng t√¨m th·∫•y file t·ªïng h·ª£p n√†o trong {CAMERA_ROI_DIR} v·ªõi pattern {final_pattern}")
            # ‚úÖ List all files for debugging
            try:
                if os.path.exists(CAMERA_ROI_DIR):
                    all_files = os.listdir(CAMERA_ROI_DIR)
                    camera_files = [f for f in all_files if f.startswith(f"camera_{camera_id}")]
                    logging.info(f"[GET-FINAL-ROI] All files for camera {camera_id}: {camera_files}")
                    logging.info(f"[GET-FINAL-ROI] All files in directory: {all_files}")
                else:
                    logging.error(f"[GET-FINAL-ROI] CAMERA_ROI_DIR does not exist: {CAMERA_ROI_DIR}")
            except Exception as e:
                logging.error(f"[GET-FINAL-ROI] Error listing directory: {e}")
            
            return jsonify({
                "success": False, 
                "error": "Kh√¥ng t√¨m th·∫•y ·∫£nh t·ªïng h·ª£p.",
                "pattern": final_pattern,
                "camera_roi_dir": CAMERA_ROI_DIR,
                "dir_exists": os.path.exists(CAMERA_ROI_DIR)
            }), 404

        latest_file = max(final_files, key=os.path.getmtime)
        logging.info(f"[GET-FINAL-ROI] Latest file selected: {latest_file}")

        if not os.path.exists(latest_file):
            logging.error(f"[GET-FINAL-ROI] File m·ªõi nh·∫•t {latest_file} kh√¥ng t·ªìn t·∫°i")
            return jsonify({"success": False, "error": "Kh√¥ng t√¨m th·∫•y ·∫£nh t·ªïng h·ª£p."}), 404

        try:
            # ‚úÖ Serve file with proper cache control headers
            response = send_file(latest_file, mimetype='image/jpeg', as_attachment=False)
            response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
            response.headers['Pragma'] = 'no-cache'
            response.headers['Expires'] = '0'
            return response
        except Exception as e:
            logging.error(f"[GET-FINAL-ROI] Error serving file: {e}")
            return jsonify({"success": False, "error": f"L·ªói khi serve file: {str(e)}"}), 500

# ‚úÖ DEBUG ENDPOINT: To check paths and files
@roi_bp.route('/debug-roi-paths', methods=['GET', 'OPTIONS'])
def debug_roi_paths():
    """Debug endpoint to check file paths and availability"""
    # ‚úÖ OPTIONS already handled by before_request
    if request.method == 'GET':
        try:
            camera_id = request.args.get('camera_id', 'Hik_recorde')
            
            debug_info = {
                "current_file": __file__,
                "base_dir": BASE_DIR,
                "camera_roi_dir": CAMERA_ROI_DIR,
                "db_path": DB_PATH,
                "camera_roi_dir_exists": os.path.exists(CAMERA_ROI_DIR),
                "expected_file_path": f"/Users/annhu/vtrack_app/V_Track/resources/output_clips/CameraROI/camera_{camera_id}_roi_packing.jpg",
                "test_file_exists": os.path.exists(f"/Users/annhu/vtrack_app/V_Track/resources/output_clips/CameraROI/camera_{camera_id}_roi_packing.jpg")
            }
            
            # List files in directory
            if os.path.exists(CAMERA_ROI_DIR):
                all_files = os.listdir(CAMERA_ROI_DIR)
                camera_files = [f for f in all_files if f.startswith(f"camera_{camera_id}")]
                debug_info.update({
                    "all_files_count": len(all_files),
                    "camera_files": camera_files,
                    "all_files": all_files[:10]  # Show first 10 files
                })
            else:
                debug_info.update({
                    "error": "CAMERA_ROI_DIR does not exist",
                    "all_files": []
                })
            
            return jsonify({
                "success": True,
                "debug_info": debug_info
            })
            
        except Exception as e:
            return jsonify({
                "success": False,
                "error": str(e),
                "debug_info": {
                    "current_file": __file__ if '__file__' in globals() else "Unknown"
                }
            })
```
## üìÑ File: `qr_detection_bp.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/blueprints/qr_detection_bp.py`

```python
from flask import Blueprint, request, jsonify
from modules.technician.qr_detector import select_qr_roi
import subprocess
import os
import json
import logging
import sys

qr_detection_bp = Blueprint('qr_detection', __name__)

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ‚úÖ FIXED: Use same path calculation as hand_detection_bp.py
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
BACKEND_DIR = os.path.join(BASE_DIR, "backend")
CAMERA_ROI_DIR = os.path.join(BASE_DIR, "resources", "output_clips", "CameraROI")

@qr_detection_bp.route('/select-qr-roi', methods=['POST'])
def select_qr_roi_endpoint():
    """
    ‚úÖ MAIN ENDPOINT: Direct function call for QR ROI selection
    This is the recommended endpoint to use (no subprocess overhead)
    """
    try:
        data = request.get_json()
        video_path = data.get('videoPath')
        camera_id = data.get('cameraId')
        roi_frame_path = data.get('roiFramePath')
        step = data.get('step', 'mvd')
        
        logger.debug(f"[MVD-DIRECT] Received data: {data}")
        logger.debug(f"[MVD-DIRECT] Parameters - video_path: {video_path}, camera_id: {camera_id}, roi_frame_path: {roi_frame_path}, step: {step}")
        
        # Input validation
        if not video_path or not camera_id or not roi_frame_path:
            logger.error("[MVD-DIRECT] Missing required parameters")
            return jsonify({"success": False, "error": "Thi·∫øu videoPath, cameraId ho·∫∑c roiFramePath."}), 400
        
        logger.debug(f"[MVD-DIRECT] Checking video path exists: {video_path}")
        if not os.path.exists(video_path):
            logger.error(f"[MVD-DIRECT] Video path does not exist: {video_path}")
            return jsonify({"success": False, "error": "ƒê∆∞·ªùng d·∫´n video kh√¥ng t·ªìn t·∫°i."}), 404
        
        logger.debug(f"[MVD-DIRECT] Checking ROI frame path exists: {roi_frame_path}")
        if not os.path.exists(roi_frame_path):
            logger.error(f"[MVD-DIRECT] ROI frame path does not exist: {roi_frame_path}")
            return jsonify({"success": False, "error": "ƒê∆∞·ªùng d·∫´n ·∫£nh t·∫°m kh√¥ng t·ªìn t·∫°i."}), 404
        
        # Clean up previous results
        if os.path.exists("/tmp/qr_roi.json"):
            logger.debug("[MVD-DIRECT] Removing existing /tmp/qr_roi.json")
            os.remove("/tmp/qr_roi.json")
            logger.info("[MVD-DIRECT] ƒê√£ x√≥a file /tmp/qr_roi.json ƒë·ªÉ b·∫Øt ƒë·∫ßu quy tr√¨nh m·ªõi")
        
        logger.debug(f"[MVD-DIRECT] Calling select_qr_roi with video_path: {video_path}, camera_id: {camera_id}, roi_frame_path: {roi_frame_path}, step: {step}")
        result = select_qr_roi(video_path, camera_id, roi_frame_path, step)
        logger.debug(f"[MVD-DIRECT] select_qr_roi result: {result}")
        logger.info(f"[MVD-DIRECT] Completed MVD step for camera_id: {camera_id}")
        
        status_code = 200 if result["success"] else 400
        return jsonify(result), status_code
    
    except Exception as e:
        logger.error(f"[MVD-DIRECT] Exception in select_qr_roi_endpoint: {str(e)}", exc_info=True)
        return jsonify({"success": False, "error": f"L·ªói h·ªá th·ªëng: {str(e)}"}), 500

@qr_detection_bp.route('/run-qr-detector', methods=['POST'])
def run_qr_detector_endpoint():
    """
    ‚úÖ SUBPROCESS ENDPOINT: Subprocess-based QR detection (use same approach as hand_detection_bp.py)
    """
    logger.warning("[MVD-SUBPROCESS] Using subprocess endpoint for GUI stability")
    
    try:
        data = request.get_json()
        video_path = data.get('videoPath')
        camera_id = data.get('cameraId', 'default_camera')
        
        # Input validation
        if not video_path:
            logger.error("[MVD-SUBPROCESS] Missing videoPath parameter")
            return jsonify({"success": False, "error": "Thi·∫øu videoPath."}), 400
        
        # Check video file exists
        if not os.path.exists(video_path):
            logger.error(f"[MVD-SUBPROCESS] Video file not found: {video_path}")
            return jsonify({"success": False, "error": "ƒê∆∞·ªùng d·∫´n video kh√¥ng t·ªìn t·∫°i."}), 404
        
        # Construct ROI frame path - use same pattern as roi_bp.py
        absolute_roi_frame_path = os.path.join(CAMERA_ROI_DIR, f"camera_{camera_id}_roi_packing.jpg")
        
        if not os.path.exists(absolute_roi_frame_path):
            logger.error(f"[MVD-SUBPROCESS] ROI frame not found: {absolute_roi_frame_path}")
            return jsonify({"success": False, "error": "ƒê∆∞·ªùng d·∫´n ·∫£nh packing kh√¥ng t·ªìn t·∫°i."}), 404
        
        logger.info(f"[MVD-SUBPROCESS] Running qr_detector.py with video_path: {video_path}, camera_id: {camera_id}, roi_frame_path: {absolute_roi_frame_path}")
        
        # Clean up previous results
        if os.path.exists("/tmp/qr_roi.json"):
            os.remove("/tmp/qr_roi.json")
            logger.info("[MVD-SUBPROCESS] Cleaned up previous QR ROI results")
        
        logger.debug(f"[MVD-SUBPROCESS] BASE_DIR: {BASE_DIR}")
        logger.debug(f"[MVD-SUBPROCESS] BACKEND_DIR (working dir): {BACKEND_DIR}")
        
        # Check script exists
        script_path = os.path.join(BACKEND_DIR, "modules", "technician", "qr_detector.py")
        if not os.path.exists(script_path):
            logger.error(f"[MVD-SUBPROCESS] Script not found: {script_path}")
            return jsonify({"success": False, "error": f"Script {script_path} kh√¥ng t·ªìn t·∫°i."}), 404
        
        try:
            # ‚úÖ SIMPLE: Use same approach as hand_detection_bp.py (working)
            result = subprocess.run(
                ["python3", "modules/technician/qr_detector.py", video_path, camera_id, absolute_roi_frame_path],
                cwd=BACKEND_DIR,  # Working directory = backend/
                capture_output=True,
                text=True,
                timeout=300
            )
            
            logger.debug(f"[MVD-SUBPROCESS] Script exit code: {result.returncode}")
            logger.debug(f"[MVD-SUBPROCESS] Script stdout: {result.stdout}")
            logger.debug(f"[MVD-SUBPROCESS] Script stderr: {result.stderr}")
            
        except subprocess.TimeoutExpired:
            logger.error("[MVD-SUBPROCESS] Script execution timeout")
            return jsonify({"success": False, "error": "H·∫øt th·ªùi gian ch·ªù khi ch·∫°y script (300s)."}), 500
        except FileNotFoundError as e:
            logger.error(f"[MVD-SUBPROCESS] Python executable not found: {e}")
            return jsonify({"success": False, "error": "Kh√¥ng t√¨m th·∫•y Python executable."}), 500
        
        # Check execution result
        if result.returncode != 0:
            error_msg = f"Script failed with code {result.returncode}"
            if result.stderr:
                error_msg += f": {result.stderr}"
            if result.stdout:
                error_msg += f"\nOutput: {result.stdout}"
            
            logger.error(f"[MVD-SUBPROCESS] {error_msg}")
            return jsonify({
                "success": False, 
                "error": f"L·ªói khi ch·∫°y script (code {result.returncode})",
                "details": result.stderr,
                "stdout": result.stdout
            }), 500
        
        # Check result file exists
        if not os.path.exists("/tmp/qr_roi.json"):
            logger.error("[MVD-SUBPROCESS] QR ROI result file not created")
            logger.debug(f"[MVD-SUBPROCESS] Script stdout: {result.stdout}")
            return jsonify({
                "success": False, 
                "error": "Script ch·∫°y th√†nh c√¥ng nh∆∞ng kh√¥ng t·∫°o file k·∫øt qu·∫£.",
                "stdout": result.stdout
            }), 500
        
        # Read and return result
        try:
            with open("/tmp/qr_roi.json", "r", encoding='utf-8') as f:
                qr_roi_result = json.load(f)
            
            logger.info(f"[MVD-SUBPROCESS] QR ROI result loaded successfully: {qr_roi_result.get('success', False)}")
            status_code = 200 if qr_roi_result.get("success", False) else 400
            return jsonify(qr_roi_result), status_code
            
        except json.JSONDecodeError as e:
            logger.error(f"[MVD-SUBPROCESS] Invalid JSON in result file: {e}")
            return jsonify({"success": False, "error": "File k·∫øt qu·∫£ c√≥ ƒë·ªãnh d·∫°ng JSON kh√¥ng h·ª£p l·ªá."}), 500
        except Exception as e:
            logger.error(f"[MVD-SUBPROCESS] Error reading result file: {e}")
            return jsonify({"success": False, "error": f"L·ªói ƒë·ªçc file k·∫øt qu·∫£: {str(e)}"}), 500
    
    except Exception as e:
        logger.error(f"[MVD-SUBPROCESS] Unexpected error in run-qr-detector: {str(e)}", exc_info=True)
        return jsonify({"success": False, "error": f"L·ªói h·ªá th·ªëng: {str(e)}"}), 500

@qr_detection_bp.route('/health', methods=['GET'])
def qr_health_check():
    """Health check endpoint for QR detection"""
    try:
        return jsonify({
            "status": "healthy",
            "endpoints": ["/select-qr-roi", "/run-qr-detector"],
            "camera_roi_dir": CAMERA_ROI_DIR,
            "camera_roi_dir_exists": os.path.exists(CAMERA_ROI_DIR),
            "base_dir": BASE_DIR,
            "backend_dir": BACKEND_DIR,
            "script_path": os.path.join(BACKEND_DIR, "modules", "technician", "qr_detector.py"),
            "script_exists": os.path.exists(os.path.join(BACKEND_DIR, "modules", "technician", "qr_detector.py"))
        }), 200
    except Exception as e:
        return jsonify({
            "status": "unhealthy",
            "error": str(e)
        }), 500
```
## üìÑ File: `config_loader.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/config_loader.py`

```python
import sqlite3
import os
from modules.path_utils import get_paths


def get_processing_config():
    """
    Tr·∫£ v·ªÅ INPUT_VIDEO_DIR v√† LOG_DIR t·ª´ b·∫£ng processing_config.
    N·∫øu thi·∫øu, fallback v·ªÅ path m·∫∑c ƒë·ªãnh.
    """
    paths = get_paths()
    db_path = paths["DB_PATH"]

    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT input_path, output_path FROM processing_config LIMIT 1")
        row = cursor.fetchone()
        conn.close()

        if row:
            return {
                "INPUT_VIDEO_DIR": row[0],
                "LOG_DIR": row[1],
            }
    except Exception as e:
        print(f"[!] L·ªói ƒë·ªçc c·∫•u h√¨nh DB: {e}")

    # fallback n·∫øu l·ªói ho·∫∑c DB tr·ªëng
    return {
        "INPUT_VIDEO_DIR": os.path.join(paths["BASE_DIR"], "resources/Inputvideo"),
        "LOG_DIR": os.path.join(paths["BASE_DIR"], "resources/output_clips/LOG"),
    }


def get_log_path(file_name: str) -> str:
    """Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß ƒë·∫øn file trong LOG_DIR."""
    log_dir = get_processing_config()["LOG_DIR"]
    return os.path.join(log_dir, file_name)


def get_input_path(file_name: str) -> str:
    """Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß ƒë·∫øn file trong INPUT_VIDEO_DIR."""
    input_dir = get_processing_config()["INPUT_VIDEO_DIR"]
    return os.path.join(input_dir, file_name)
```
## üìÑ File: `__init__.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/__init__.py`

```python

```
## üìÑ File: `path_utils.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/path_utils.py`

```python
import os


def find_project_root(file_path):
    """T√¨m th∆∞ m·ª•c g·ªëc c·ªßa d·ª± √°n b·∫Øt ƒë·∫ßu t·ª´ file_path."""
    current_path = os.path.dirname(os.path.abspath(file_path))
    while current_path != os.path.dirname(current_path):
        if "backend" in os.listdir(current_path):
            return current_path
        current_path = os.path.dirname(current_path)
    raise RuntimeError("Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c g·ªëc d·ª± √°n.")


def get_paths():
    base_dir = find_project_root(__file__)
    return {
        "BASE_DIR": base_dir,
        "DB_PATH": os.path.join(base_dir, "backend/database/events.db"),
        "BACKEND_DIR": os.path.join(base_dir, "backend"),
        "FRONTEND_DIR": os.path.join(base_dir, "frontend"),
        "CAMERA_ROI_DIR": os.path.join(base_dir, "resources", "output_clips", "CameraROI")
    }

```
## üìÑ File: `config.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/config/config.py`

```python
from datetime import datetime, timedelta
from flask import Blueprint, request, jsonify, Flask, g
from flask_cors import CORS
import os
import json
import sqlite3
import logging
import time
from modules.db_utils import find_project_root, get_db_connection
from modules.scheduler.db_sync import db_rwlock
from modules.sources.path_manager import PathManager
from database import update_database, DB_PATH, initialize_sync_status
# üÜï NEW: Cloud endpoints module
from modules.sources.cloud_endpoints import cloud_bp
# üîí SECURITY: Import security modules
import jwt
from cryptography.fernet import Fernet
import base64
import hashlib
from functools import wraps
from typing import Dict, List, Any, Optional, Tuple
from google.oauth2.credentials import Credentials
import google.auth.exceptions
from modules.sources.sync_endpoints import sync_bp

"""
üéØ V_TRACK BACKGROUND SERVICE AUTHENTICATION STRATEGY

V_track is designed as a "set it and forget it" background monitoring service,
similar to traditional DVR/NVR systems. Users expect:

1. Setup once ‚Üí Run forever
2. No daily authentication prompts  
3. Continuous auto-sync without interruption
4. Zero maintenance authentication

SESSION DURATION STRATEGY:
- JWT Session: 90 days (3 months)
- Refresh Token: 365 days (1 year)  
- Auto-refresh: When 7 days remaining
- Grace Period: 7 days if refresh fails

This ensures maximum user convenience while maintaining reasonable security.
User only needs to re-authenticate every 3 months, or ideally never if
auto-refresh works correctly.

BACKGROUND SERVICE PHILOSOPHY:
- Silent operation (no popups/interruptions)
- Aggressive auto-refresh to prevent expiry
- Graceful degradation if authentication fails
- Prioritize reliability over strict security
"""

config_bp = Blueprint('config', __name__)

# ‚úÖ CLEAN: No more blueprint-level CORS handlers
# All CORS will be handled by Flask-CORS global config only

# X√°c ƒë·ªãnh th∆∞ m·ª•c g·ªëc c·ªßa d·ª± √°n
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
CONFIG_FILE = os.path.join(BASE_DIR, "config.json")

# üîí SECURITY: Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SecurityConfig:
    """Security configuration and utilities"""
    
    def __init__(self):
        # üîí SECURITY: Initialize encryption keys
        self.jwt_secret = os.getenv('JWT_SECRET_KEY', self._generate_jwt_secret())
        self.encryption_key = os.getenv('ENCRYPTION_KEY', self._generate_encryption_key())
        self.cipher_suite = Fernet(self.encryption_key.encode() if isinstance(self.encryption_key, str) else self.encryption_key)
        
        # üéØ V_TRACK BACKGROUND SERVICE: Long-running authentication for "set it and forget it" model
        # User behavior: Setup once ‚Üí Run forever like DVR/NVR, no daily interaction
        
        # Primary session duration (JWT token) - Background service mode
        self.session_duration = timedelta(days=90)  # 3 months for background service
        
        # Refresh token configuration - Long-term authentication
        self.refresh_token_duration = timedelta(days=365)  # 1 year refresh token
        
        # Aggressive auto-refresh to prevent interruption
        self.refresh_threshold = timedelta(days=7)  # Refresh when 7 days remaining
        self.refresh_retry_interval = timedelta(hours=1)  # Retry every hour if fail
        self.max_refresh_retries = 168  # Retry for 1 week (168 hours)
        
        # Background service flags
        self.background_service_mode = True  # Enable background service features
        self.silent_operation = True  # No user popups/interruptions
        self.emergency_grace_period = timedelta(days=7)  # Grace period if refresh fails
        
        logger.info("üîí V_track Background Service authentication initialized")
        logger.info(f"üìÖ JWT Session Duration: {self.session_duration}")
        logger.info(f"üîÑ Refresh Token Duration: {self.refresh_token_duration}")
        logger.info(f"‚ö° Auto-refresh Threshold: {self.refresh_threshold}")    
    def _generate_jwt_secret(self) -> str:
        """Generate JWT secret key if not provided"""
        import secrets
        secret = secrets.token_urlsafe(32)
        logger.warning("üîë JWT_SECRET_KEY not found in environment, generated temporary key")
        return secret
    
    def _generate_encryption_key(self) -> bytes:
        """Generate encryption key if not provided"""
        key = Fernet.generate_key()
        logger.warning("üîë ENCRYPTION_KEY not found in environment, generated temporary key")
        return key
    
    def encrypt_credentials(self, credentials_dict: Dict) -> str:
        """Encrypt OAuth credentials"""
        try:
            credentials_json = json.dumps(credentials_dict)
            encrypted = self.cipher_suite.encrypt(credentials_json.encode())
            return base64.b64encode(encrypted).decode()
        except Exception as e:
            logger.error(f"Failed to encrypt credentials: {e}")
            raise
    
    def decrypt_credentials(self, encrypted_credentials: str) -> Dict:
        """Decrypt OAuth credentials"""
        try:
            encrypted_bytes = base64.b64decode(encrypted_credentials.encode())
            decrypted = self.cipher_suite.decrypt(encrypted_bytes)
            return json.loads(decrypted.decode())
        except Exception as e:
            logger.error(f"Failed to decrypt credentials: {e}")
            raise
    
    def generate_session_token(self, user_email: str, provider: str = 'google_drive') -> str:
        """Generate JWT session token"""
        payload = {
            'user_email': user_email,
            'provider': provider,
            'iat': datetime.utcnow(),
            'exp': datetime.utcnow() + self.session_duration,
            'iss': 'vtrack-background-service',  # Match cloud_endpoints.py issuer
            'type': 'session'  # Add type field for consistency
        }
        return jwt.encode(payload, self.jwt_secret, algorithm='HS256')

    def generate_token_pair(self, user_email: str, provider: str = 'google_drive') -> dict:
        """Generate access token + refresh token pair for background service"""
        now = datetime.utcnow()
        
        # Access token (for API calls)
        access_payload = {
            'user_email': user_email,
            'provider': provider,
            'type': 'access',
            'iat': now,
            'exp': now + self.session_duration,
            'background_service': True
        }
        
        # Refresh token (for renewing access token)
        refresh_payload = {
            'user_email': user_email,
            'provider': provider,
            'type': 'refresh', 
            'iat': now,
            'exp': now + self.refresh_token_duration,
            'background_service': True
        }
        
        return {
            'access_token': jwt.encode(access_payload, self.jwt_secret, algorithm='HS256'),
            'refresh_token': jwt.encode(refresh_payload, self.jwt_secret, algorithm='HS256'),
            'expires_in': int(self.session_duration.total_seconds()),
            'refresh_expires_in': int(self.refresh_token_duration.total_seconds()),
            'token_type': 'Bearer'
        }
    
    def should_refresh_token(self, token: str) -> bool:
        """Check if token should be refreshed (when 7 days remaining)"""
        try:
            payload = jwt.decode(token, self.jwt_secret, algorithms=['HS256'])
            exp_time = datetime.utcfromtimestamp(payload['exp'])
            time_left = exp_time - datetime.utcnow()
            
            # Refresh aggressively when 7 days left (not waiting until expiry)
            return time_left < self.refresh_threshold
        except jwt.ExpiredSignatureError:
            return True  # Expired token, needs refresh
        except jwt.InvalidTokenError:
            return True  # Invalid token, needs refresh
    
    def refresh_access_token(self, refresh_token: str) -> Optional[dict]:
        """Refresh access token using refresh token"""
        try:
            payload = jwt.decode(refresh_token, self.jwt_secret, algorithms=['HS256'])
            
            if payload.get('type') != 'refresh':
                logger.warning("Invalid token type for refresh")
                return None
                
            user_email = payload['user_email']
            provider = payload.get('provider', 'google_drive')
            
            # Generate new token pair
            new_tokens = self.generate_token_pair(user_email, provider)
            logger.info(f"‚úÖ Refreshed tokens for user: {user_email}")
            return new_tokens
            
        except jwt.ExpiredSignatureError:
            logger.warning("üö® Refresh token expired - require re-authentication")
            return None
        except jwt.InvalidTokenError:
            logger.warning("‚ùå Invalid refresh token")
            return None
    
    def get_token_time_remaining(self, token: str) -> Optional[timedelta]:
        """Get time remaining until token expiry"""
        try:
            payload = jwt.decode(token, self.jwt_secret, algorithms=['HS256'])
            exp_time = datetime.utcfromtimestamp(payload['exp'])
            time_left = exp_time - datetime.utcnow()
            return time_left if time_left.total_seconds() > 0 else timedelta(0)
        except:
            return None
    
    def validate_session_token(self, token: str) -> Optional[Dict]:
        """Validate JWT session token"""
        try:
            payload = jwt.decode(token, self.jwt_secret, algorithms=['HS256'])
            return payload
        except jwt.ExpiredSignatureError:
            logger.warning("Session token expired")
            return None
        except jwt.InvalidTokenError as e:
            logger.warning(f"Invalid session token: {e}")
            return None

class ConfigManager:
    """Enhanced configuration manager with security features"""
    
    def __init__(self, db_manager=None):
        self.db = db_manager
        self.security = SecurityConfig()
        
        # üîí SECURITY: Cloud credential proxy cache
        self._credential_cache = {}
        self._cache_expiry = {}
        
        logger.info("üîß Configuration manager initialized with security")
    
    # üîí SECURITY: Session verification middleware
    def require_session(self, f):
        """Decorator to require valid session"""
        @wraps(f)
        def decorated_function(*args, **kwargs):
            auth_header = request.headers.get('Authorization')
            if not auth_header or not auth_header.startswith('Bearer '):
                return jsonify({'error': 'No valid session token provided'}), 401
            
            token = auth_header.split(' ')[1]
            session_data = self.security.validate_session_token(token)
            
            if not session_data:
                return jsonify({'error': 'Invalid or expired session'}), 401
            
            # Check session in database
            if self.db:
                from database import get_session
                db_session = get_session(token)
                if not db_session:
                    return jsonify({'error': 'Session not found'}), 401
                
                g.session_token = token
                g.user_email = session_data['user_email']
                g.provider = session_data.get('provider', 'google_drive')
                g.db_session = db_session
            
            return f(*args, **kwargs)
        return decorated_function
    
    # üîí SECURITY: Session-based source validation
    def validate_cloud_source(self, source_config: Dict, session_token: str) -> Tuple[bool, str]:
        """Validate cloud source configuration with session"""
        try:
            if not session_token:
                return False, "No session token provided"
            
            # Validate session
            session_data = self.security.validate_session_token(session_token)
            if not session_data:
                return False, "Invalid or expired session"
            
            # Get session from database
            if self.db:
                from database import get_session
                db_session = get_session(session_token)
                if not db_session:
                    return False, "Session not found in database"
                
                # Validate provider matches
                if source_config.get('provider') != db_session.get('provider'):
                    return False, "Provider mismatch with session"
                
                # Validate user matches
                if session_data['user_email'] != db_session['user_email']:
                    return False, "User mismatch with session"
            
            # Validate required fields
            required_fields = ['provider', 'selected_folders']
            for field in required_fields:
                if field not in source_config:
                    return False, f"Missing required field: {field}"
            
            if not source_config['selected_folders']:
                return False, "No folders selected"
            
            return True, "Cloud source configuration valid"
            
        except Exception as e:
            logger.error(f"Error validating cloud source: {e}")
            return False, f"Validation error: {str(e)}"
    
    # üîí SECURITY: Backend credential proxy for cloud operations
    def get_cloud_credentials(self, session_token: str) -> Optional[Credentials]:
        """Get decrypted credentials for cloud operations"""
        try:
            # Check cache first
            if session_token in self._credential_cache:
                cached_time = self._cache_expiry.get(session_token)
                if cached_time and datetime.utcnow() < cached_time:
                    return self._credential_cache[session_token]
                else:
                    # Clean expired cache
                    self._credential_cache.pop(session_token, None)
                    self._cache_expiry.pop(session_token, None)
            
            # Get session from database
            if not self.db:
                logger.error("Database manager not available")
                return None
            
            from database import get_session
            db_session = get_session(session_token)
            if not db_session:
                logger.error("Session not found")
                return None
            
            # Decrypt credentials
            encrypted_creds = db_session.get('encrypted_credentials')
            if not encrypted_creds:
                logger.error("No encrypted credentials found")
                return None
            
            creds_dict = self.security.decrypt_credentials(encrypted_creds)
            
            # Create Google credentials object
            credentials = Credentials(
                token=creds_dict.get('token'),
                refresh_token=creds_dict.get('refresh_token'),
                token_uri=creds_dict.get('token_uri'),
                client_id=creds_dict.get('client_id'),
                client_secret=creds_dict.get('client_secret'),
                scopes=creds_dict.get('scopes')
            )
            
            # Cache for 5 minutes
            self._credential_cache[session_token] = credentials
            self._cache_expiry[session_token] = datetime.utcnow() + timedelta(minutes=5)
            
            return credentials
            
        except Exception as e:
            logger.error(f"Failed to get cloud credentials: {e}")
            return None
    
    def refresh_cloud_credentials(self, session_token: str) -> bool:
        """Refresh cloud credentials and update session"""
        try:
            credentials = self.get_cloud_credentials(session_token)
            if not credentials:
                return False
            
            # Try to refresh
            credentials.refresh(google.auth.transport.requests.Request())
            
            # Update encrypted credentials in database
            creds_dict = {
                'token': credentials.token,
                'refresh_token': credentials.refresh_token,
                'token_uri': credentials.token_uri,
                'client_id': credentials.client_id,
                'client_secret': credentials.client_secret,
                'scopes': credentials.scopes
            }
            
            encrypted_creds = self.security.encrypt_credentials(creds_dict)
            
            if self.db:
                from database import update_session_credentials
                success = update_session_credentials(session_token, encrypted_creds)
                if success:
                    # Update cache
                    self._credential_cache[session_token] = credentials
                    self._cache_expiry[session_token] = datetime.utcnow() + timedelta(minutes=5)
                    logger.info("Cloud credentials refreshed successfully")
                    return True
            
            return False
            
        except google.auth.exceptions.RefreshError as e:
            logger.error(f"Failed to refresh credentials: {e}")
            return False
        except Exception as e:
            logger.error(f"Error refreshing credentials: {e}")
            return False
    
    def proxy_cloud_operation(self, session_token: str, operation: str, **kwargs) -> Dict:
        """Proxy cloud operations with backend credentials"""
        try:
            credentials = self.get_cloud_credentials(session_token)
            if not credentials:
                return {'success': False, 'error': 'Unable to get credentials'}
            
            # Import Google Drive service here to avoid circular imports
            from googleapiclient.discovery import build
            
            service = build('drive', 'v3', credentials=credentials)
            
            if operation == 'list_folders':
                # List folders operation
                query = "mimeType='application/vnd.google-apps.folder'"
                if 'parent_id' in kwargs:
                    query += f" and '{kwargs['parent_id']}' in parents"
                
                results = service.files().list(
                    q=query,
                    fields="files(id, name, parents)",
                    pageSize=kwargs.get('page_size', 100)
                ).execute()
                
                folders = results.get('files', [])
                return {
                    'success': True,
                    'folders': folders,
                    'count': len(folders)
                }
            
            elif operation == 'download_file':
                # Download file operation
                file_id = kwargs.get('file_id')
                if not file_id:
                    return {'success': False, 'error': 'File ID required'}
                
                # Get file metadata first
                file_metadata = service.files().get(fileId=file_id).execute()
                
                # Download file content
                content = service.files().get_media(fileId=file_id).execute()
                
                return {
                    'success': True,
                    'metadata': file_metadata,
                    'content': content
                }
            
            else:
                return {'success': False, 'error': f'Unsupported operation: {operation}'}
                
        except Exception as e:
            logger.error(f"Cloud operation failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def cleanup_expired_sessions(self):
        """Cleanup expired sessions and cache"""
        if self.db:
            from database import cleanup_expired_sessions
            deleted_count = cleanup_expired_sessions()
            logger.info(f"Cleaned up {deleted_count} expired sessions")
        
        # Clean credential cache
        current_time = datetime.utcnow()
        expired_tokens = [
            token for token, expiry in self._cache_expiry.items()
            if expiry < current_time
        ]
        
        for token in expired_tokens:
            self._credential_cache.pop(token, None)
            self._cache_expiry.pop(token, None)
        
        if expired_tokens:
            logger.info(f"Cleaned up {len(expired_tokens)} expired credential cache entries")

# Global configuration instance
config_manager = None

def get_config_manager(db_manager=None):
    """Get or create global configuration manager"""
    global config_manager
    if config_manager is None:
        config_manager = ConfigManager(db_manager)
    return config_manager

def init_config(db_manager):
    """Initialize configuration with database manager"""
    global config_manager
    config_manager = ConfigManager(db_manager)
    logger.info("üîß Configuration initialized")
    return config_manager

def init_app_and_config():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger(__name__)

    app = Flask(__name__)
    
    # ‚úÖ FIXED: Single CORS configuration - no duplicates
    CORS(app, 
         resources={
             r"/*": {
                 "origins": ["http://localhost:3000"],
                 "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
                 "allow_headers": ["Content-Type", "Authorization"],
                 "supports_credentials": True
             }
         })

    # üéØ V_TRACK BACKGROUND SERVICE: Long-term session configuration
    app.config.update(
        # Background service session - Very long duration for "set it and forget it"
        PERMANENT_SESSION_LIFETIME=timedelta(days=90),  # 3 months session
        
        # Session persistence
        SESSION_REFRESH_EACH_REQUEST=True,  # Extend session on activity
        SESSION_COOKIE_HTTPONLY=True,
        SESSION_COOKIE_SECURE=False,  # Set to True in production with HTTPS
        SESSION_COOKIE_SAMESITE='Lax',
        
        # OAuth configuration for background service
        OAUTH_SESSION_LIFETIME=timedelta(days=90),  # Match JWT session duration
        
        # Background service flags
        BACKGROUND_SERVICE_MODE=True,
        SILENT_OPERATION=True
    )
    
    logger.info("üéØ V_track Background Service session configured")
    logger.info("üìÖ Session lifetime: 90 days (background service mode)")

    DB_PATH = os.path.join(BASE_DIR, "database", "events.db")
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)

    # G·ªçi update_database ƒë·ªÉ t·∫°o b·∫£ng tr∆∞·ªõc khi truy v·∫•n
    update_database()

    def get_db_path():
        default_db_path = DB_PATH
        try:
            with db_rwlock.gen_rlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("SELECT db_path FROM processing_config WHERE id = 1")
                result = cursor.fetchone()
                conn.close()
            return result[0] if result else default_db_path
        except Exception as e:
            logger.error(f"Error getting DB_PATH from database: {e}")
            return default_db_path

    final_db_path = get_db_path()
    logger.info(f"Using DB_PATH: {final_db_path}")

    # Truy v·∫•n c·∫•u h√¨nh t·ª´ processing_config
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT input_path, output_path, frame_rate, frame_interval FROM processing_config WHERE id = 1")
        result = cursor.fetchone()
        conn.close()
        if result:
            INPUT_PATH, OUTPUT_PATH, FRAME_RATE, FRAME_INTERVAL = result
        else:
            INPUT_PATH = os.path.join(BASE_DIR, "Inputvideo")
            OUTPUT_PATH = os.path.join(BASE_DIR, "output_clips")
            FRAME_RATE = 30
            FRAME_INTERVAL = 6
    except Exception as e:
        logger.error(f"Error querying processing_config: {e}")
        INPUT_PATH = os.path.join(BASE_DIR, "Inputvideo")
        OUTPUT_PATH = os.path.join(BASE_DIR, "output_clips")
        FRAME_RATE = 30
        FRAME_INTERVAL = 6

    return app, final_db_path, logger

# H√†m ƒë·ªçc c·∫•u h√¨nh t·ª´ file ho·∫∑c bi·∫øn m√¥i tr∆∞·ªùng
def load_config():
    default_config = {
        "db_path": os.path.join(BASE_DIR, "database", "events.db"),
        "input_path": os.path.join(BASE_DIR, "Inputvideo"),
        "output_path": os.path.join(BASE_DIR, "output_clips")
    }
    
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"Error loading config file: {e}")
            return default_config
    
    return {
        "db_path": os.getenv("DB_PATH", default_config["db_path"]),
        "input_path": os.getenv("INPUT_PATH", default_config["input_path"]),
        "output_path": os.getenv("OUTPUT_PATH", default_config["output_path"])
    }

# Load c·∫•u h√¨nh t·ª´ processing_config thay v√¨ config.json
config = load_config()

def detect_camera_folders(path):
    """Detect camera folders in the given path"""
    cameras = []
    
    if not os.path.exists(path):
        return cameras
    
    try:
        for item in os.listdir(path):
            item_path = os.path.join(path, item)
            if os.path.isdir(item_path):
                # Check if this looks like a camera folder
                # Common camera folder patterns: Cam*, Camera*, Channel*, etc.
                item_lower = item.lower()
                if any(pattern in item_lower for pattern in ['cam', 'camera', 'channel', 'ch']):
                    cameras.append(item)
                # Also include any directory that contains video files
                elif has_video_files(item_path):
                    cameras.append(item)
        
        cameras.sort()  # Sort alphabetically
        return cameras
    except Exception as e:
        print(f"Error detecting cameras in {path}: {e}")
        return cameras

def has_video_files(path, max_depth=2):
    """Check if directory contains video files (recursive up to max_depth)"""
    video_extensions = ('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.m4v', '.mpg', '.mpeg')
    
    def check_directory(dir_path, depth):
        if depth > max_depth:
            return False
        
        try:
            for item in os.listdir(dir_path):
                item_path = os.path.join(dir_path, item)
                if os.path.isfile(item_path) and item.lower().endswith(video_extensions):
                    return True
                elif os.path.isdir(item_path) and depth < max_depth:
                    if check_directory(item_path, depth + 1):
                        return True
        except (PermissionError, OSError):
            pass
        
        return False
    
    return check_directory(path, 0)

# ‚úÖ FIX: Helper function ƒë·ªÉ map path cho c√°c source type kh√°c nhau (NO NVR)
def get_working_path_for_source(source_type, source_name, source_path):
    """Map source connection info to actual working directory"""
    if source_type == 'local':
        # Local: source_path is actual file system path
        working_path = source_path
        print(f"üìÅ Local Path Mapping: {source_path} ‚Üí {working_path}")
        return working_path
        
    elif source_type == 'cloud':
        # Cloud: source_path is cloud URL, working path is sync directory
        working_path = os.path.join(BASE_DIR, "cloud_sync", source_name)
        print(f"‚òÅÔ∏è Cloud Path Mapping: {source_path} ‚Üí {working_path}")
        
        # Create directory if it doesn't exist
        try:
            os.makedirs(working_path, exist_ok=True)
            print(f"üìÅ Created/verified Cloud directory: {working_path}")
        except Exception as e:
            print(f"‚ùå Failed to create Cloud directory {working_path}: {e}")
            
        return working_path
        
    else:
        # Unknown source type: use as-is
        print(f"‚ùì Unknown source type '{source_type}', using path as-is: {source_path}")
        return source_path
    
# üîß ADD HELPER FUNCTION HERE - NGAY SAU function tr√™n k·∫øt th√∫c:
def extract_cameras_from_cloud_folders(folders):
    """Extract camera names from cloud folders"""
    cameras = []
    
    for folder in folders:
        if isinstance(folder, dict):
            # Folder object v·ªõi metadata
            camera_name = folder.get('name', 'Unknown_Folder')
            
            # If it's a deep folder, use parent names for context
            if folder.get('depth', 1) > 2:
                path_parts = folder.get('path', '').split('/')
                # Use second-to-last part as camera name (skip filename)
                if len(path_parts) >= 2:
                    camera_name = path_parts[-2] or path_parts[-1]
        else:
            # Simple string name
            camera_name = str(folder)
        
        # Clean v√† standardize camera name
        camera_name = (camera_name
                      .replace(' ', '_')
                      .replace('/', '_')
                      .replace('\\', '_')
                      .strip('_'))
        
        if camera_name and camera_name not in cameras:
            cameras.append(camera_name)
    
    return cameras

# üîß FIX 3: Add debug endpoint to check camera sync
@config_bp.route('/debug-cameras', methods=['GET'])
def debug_cameras():
    """Debug endpoint to check camera sync status"""
    try:
        # Get processing_config cameras
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT selected_cameras, input_path FROM processing_config WHERE id = 1")
        result = cursor.fetchone()
        conn.close()
        
        if result:
            selected_cameras, input_path = result
            try:
                cameras = json.loads(selected_cameras) if selected_cameras else []
            except:
                cameras = []
                
            # Get active source info
            path_manager = PathManager()
            active_sources = path_manager.get_all_active_sources()
            
            return jsonify({
                "processing_config": {
                    "selected_cameras": cameras,
                    "camera_count": len(cameras),
                    "input_path": input_path
                },
                "active_sources": active_sources,
                "debug_time": datetime.now().isoformat()
            }), 200
        else:
            return jsonify({"error": "No processing config found"}), 404
            
    except Exception as e:
        return jsonify({"error": f"Debug failed: {str(e)}"}), 500

@config_bp.route('/detect-cameras', methods=['POST'])
def detect_cameras():
    """Detect camera folders in the specified path"""
    try:
        data = request.json
        path = data.get('path')
        
        if not path:
            return jsonify({"error": "Path is required"}), 400
        
        if not os.path.exists(path):
            return jsonify({"error": f"Path does not exist: {path}"}), 400
        
        # Detect camera folders
        cameras = detect_camera_folders(path)
        
        # Get current selected cameras from processing_config
        selected_cameras = []
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT selected_cameras FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            if result and result[0]:
                selected_cameras = json.loads(result[0])
            conn.close()
        except Exception as e:
            print(f"Error getting selected cameras: {e}")
        
        return jsonify({
            "cameras": cameras,
            "selected_cameras": selected_cameras,
            "path": path,
            "count": len(cameras)
        }), 200
        
    except Exception as e:
        return jsonify({"error": f"Failed to detect cameras: {str(e)}"}), 500

@config_bp.route('/update-source-cameras', methods=['POST'])
def update_source_cameras():
    """üîß Update selected cameras for a source in processing_config (Simple Update)"""
    try:
        data = request.json
        source_id = data.get('source_id')
        selected_cameras = data.get('selected_cameras', [])
        
        # Update processing_config with selected cameras
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            UPDATE processing_config 
            SET selected_cameras = ? 
            WHERE id = 1
        """, (json.dumps(selected_cameras),))
        
        conn.commit()
        conn.close()
        
        return jsonify({
            "message": "Camera selection updated successfully",
            "selected_cameras": selected_cameras,
            "count": len(selected_cameras)
        }), 200
        
    except Exception as e:
        return jsonify({"error": f"Failed to update camera selection: {str(e)}"}), 500

@config_bp.route('/get-cameras', methods=['GET'])
def get_cameras():
    try:
        path_manager = PathManager()
        sources = path_manager.get_all_active_sources()
        
        cameras = []
        
        if sources:
            # Use active source
            active_source = sources[0]  # Single active source
            source_type = active_source['source_type']
            
            if source_type == 'local':
                # Local directory scanning
                video_root = active_source['path']
                if not os.path.exists(video_root):
                    return jsonify({"error": f"Directory {video_root} does not exist. Ensure the path is correct or create the directory."}), 400
                
                # Detect camera folders
                detected_cameras = detect_camera_folders(video_root)
                for camera in detected_cameras:
                    cameras.append({"name": camera, "path": os.path.join(video_root, camera)})
            
            elif source_type in ['cloud', 'camera']:
                # For other source types, use source name as camera
                cameras.append({"name": active_source['name'], "path": active_source['path']})
        else:
            # Fallback to legacy behavior
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT input_path FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            conn.close()

            if not result:
                return jsonify({"error": "video_root not found in configuration. Please update via /save-config endpoint."}), 400

            video_root = result[0]
            if not os.path.exists(video_root):
                return jsonify({"error": f"Directory {video_root} does not exist. Ensure the path is correct or create the directory."}), 400

            detected_cameras = detect_camera_folders(video_root)
            for camera in detected_cameras:
                cameras.append({"name": camera, "path": os.path.join(video_root, camera)})

        return jsonify({"cameras": cameras}), 200
    except Exception as e:
        return jsonify({"error": f"Failed to retrieve cameras: {str(e)}"}), 500

@config_bp.route('/save-config', methods=['POST'])
def save_config():
    """Fixed save_config without frame_settings table dependency"""
    try:
        data = request.json
        if not data:
            return jsonify({"error": "No data provided"}), 400
            
        video_root = data.get('video_root')
        output_path = data.get('output_path', config.get("output_path", "/default/output"))
        default_days = data.get('default_days', 30)
        min_packing_time = data.get('min_packing_time', 10)
        max_packing_time = data.get('max_packing_time', 120)
        frame_rate = data.get('frame_rate', 30)
        frame_interval = data.get('frame_interval', 5)
        video_buffer = data.get('video_buffer', 2)
        selected_cameras = data.get('selected_cameras', [])
        run_default_on_start = data.get('run_default_on_start', 1)

        print(f"=== SAVE CONFIG REQUEST ===")
        print(f"Raw video_root from frontend: {video_root}")
        print(f"Selected cameras: {selected_cameras}")

        # ‚úÖ FIX: Enhanced path mapping with better error handling
        try:
            # Try to get active source for path mapping
            try:
                from modules.sources.path_manager import PathManager
                path_manager = PathManager()
                active_sources = path_manager.get_all_active_sources()
                
                if active_sources:
                    active_source = active_sources[0]  # Single active source
                    source_type = active_source['source_type']
                    source_name = active_source['name']
                    source_path = active_source['path']
                    
                    print(f"Found active source: {source_name} ({source_type})")
                    
                    # Apply proper path mapping (NO NVR)
                    correct_working_path = get_working_path_for_source(source_type, source_name, source_path)
                    
                    if video_root != correct_working_path:
                        print(f"üîÑ Correcting video_root: {video_root} ‚Üí {correct_working_path}")
                        video_root = correct_working_path
                    else:
                        print(f"‚úÖ video_root already correct: {video_root}")
                else:
                    print("‚ö†Ô∏è No active video source found, using video_root as-is")
                    
            except ImportError:
                print("‚ö†Ô∏è PathManager not available, using video_root as-is")
            except Exception as pm_error:
                print(f"‚ö†Ô∏è PathManager error: {pm_error}, using video_root as-is")
                
        except Exception as path_error:
            print(f"‚ùå Error in path mapping: {path_error}")

        # ‚úÖ Final validation
        if not video_root or video_root.strip() == "":
            error_msg = "‚ùå video_root cannot be empty"
            print(error_msg)
            return jsonify({"error": error_msg}), 400

        # Basic URL validation
        if '://' in video_root or 'localhost:' in video_root:
            error_msg = f"‚ùå video_root cannot be a URL: {video_root}"
            print(error_msg)
            return jsonify({"error": error_msg}), 400

        print(f"üìù Final video_root for database: {video_root}")

        # ‚úÖ FIXED: Database operations - only processing_config table
        try:
            conn = get_db_connection()
            if not conn:
                return jsonify({"error": "Database connection failed"}), 500
                
            cursor = conn.cursor()
            
            # Check if processing_config table exists
            cursor.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name='processing_config'
            """)
            if not cursor.fetchone():
                return jsonify({"error": "processing_config table not found"}), 500
            
            # Add column if not exists (safe operation)
            try:
                cursor.execute("ALTER TABLE processing_config ADD COLUMN run_default_on_start INTEGER DEFAULT 1")
                print("‚úÖ Added run_default_on_start column")
            except sqlite3.OperationalError:
                pass  # Column already exists
            
            # ‚úÖ MAIN FIX: Only insert into processing_config (no frame_settings)
            cursor.execute("""
                INSERT OR REPLACE INTO processing_config (
                    id, input_path, output_path, storage_duration, min_packing_time, 
                    max_packing_time, frame_rate, frame_interval, video_buffer, default_frame_mode, 
                    selected_cameras, db_path, run_default_on_start
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (1, video_root, output_path, default_days, min_packing_time, max_packing_time, 
                  frame_rate, frame_interval, video_buffer, "default", json.dumps(selected_cameras), 
                  DB_PATH, run_default_on_start))

            print("‚úÖ processing_config updated successfully")

            # ‚úÖ REMOVED: No more frame_settings table insert
            # Frame data is now stored in processing_config table only

            conn.commit()
            
            # ‚úÖ Verify what was saved
            cursor.execute("SELECT input_path, selected_cameras, frame_rate, frame_interval FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            if result:
                saved_path, saved_cameras, saved_fr, saved_fi = result
                print(f"‚úÖ Verified saved input_path: {saved_path}")
                print(f"‚úÖ Verified saved cameras: {saved_cameras}")
                print(f"‚úÖ Verified saved frame_rate: {saved_fr}, frame_interval: {saved_fi}")
            
            conn.close()
            
            print("‚úÖ Config saved successfully")
            return jsonify({
                "message": "Configuration saved successfully",
                "saved_path": video_root,
                "saved_cameras": selected_cameras,
                "frame_settings": {
                    "frame_rate": frame_rate,
                    "frame_interval": frame_interval
                }
            }), 200
            
        except sqlite3.PermissionError as e:
            error_msg = f"Database permission denied: {str(e)}"
            print(f"‚ùå {error_msg}")
            return jsonify({"error": error_msg}), 403
        except sqlite3.Error as e:
            error_msg = f"Database error: {str(e)}"
            print(f"‚ùå {error_msg}")
            return jsonify({"error": error_msg}), 500
        except Exception as e:
            error_msg = f"Database operation failed: {str(e)}"
            print(f"‚ùå {error_msg}")
            return jsonify({"error": error_msg}), 500

    except Exception as e:
        # ‚úÖ Catch-all error handler to ensure JSON response
        error_msg = f"Unexpected error: {str(e)}"
        print(f"‚ùå CRITICAL ERROR in save_config: {error_msg}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": error_msg}), 500

@config_bp.route('/save-general-info', methods=['POST'])
def save_general_info():
    data = request.json
    country = data.get('country')
    timezone = data.get('timezone')
    brand_name = data.get('brand_name')
    working_days = data.get('working_days', ["Th·ª© Hai", "Th·ª© Ba", "Th·ª© T∆∞", "Th·ª© NƒÉm", "Th·ª© S√°u", "Th·ª© B·∫£y", "Ch·ªß Nh·∫≠t"])
    
    # B·∫£n ƒë·ªì ng√†y ti·∫øng Vi·ªát sang ti·∫øng Anh
    day_map = {
        'Th·ª© Hai': 'Monday', 'Th·ª© Ba': 'Tuesday', 'Th·ª© T∆∞': 'Wednesday',
        'Th·ª© NƒÉm': 'Thursday', 'Th·ª© S√°u': 'Friday', 'Th·ª© B·∫£y': 'Saturday',
        'Ch·ªß Nh·∫≠t': 'Sunday'
    }
    
    # Chuy·ªÉn ƒë·ªïi working_days sang ti·∫øng Anh
    working_days_en = [day_map.get(day, day) for day in working_days]
    
    from_time = data.get('from_time', "07:00")
    to_time = data.get('to_time', "23:00")

    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            INSERT OR REPLACE INTO general_info (
                id, country, timezone, brand_name, working_days, from_time, to_time
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (1, country, timezone, brand_name, json.dumps(working_days_en), from_time, to_time))

        conn.commit()
        conn.close()
    except PermissionError as e:
        return jsonify({"error": f"Permission denied: {str(e)}. Check database file permissions."}), 403
    except Exception as e:
        return jsonify({"error": f"Database error: {str(e)}. Ensure the database is accessible."}), 500

    print("General info saved:", data)
    return jsonify({"message": "General info saved"}), 200

# ‚úÖ ADD to config.py - Modify save-sources endpoint to support overwrite

@config_bp.route('/save-sources', methods=['POST'])
def save_video_sources():
    """Save single active video source - ENHANCED: Support overwrite"""
    data = request.json
    sources = data.get('sources', [])
    
    if not sources:
        return jsonify({"error": "No sources provided"}), 400
    
    # Single Active Source: only process the first source
    source = sources[0]
    source_type = source.get('source_type')
    name = source.get('name')
    path = source.get('path')
    config_data = source.get('config', {})
    overwrite = source.get('overwrite', False)  # ‚úÖ NEW: Check overwrite flag
    
    print(f"=== SAVE SOURCE: {name} ({source_type}) ===")
    print(f"Connection path: {path}")
    print(f"Config data: {config_data}")
    print(f"Overwrite mode: {overwrite}")  # ‚úÖ NEW: Log overwrite mode
    
    if not all([source_type, name, path]):
        return jsonify({"error": "Source missing required fields"}), 400
    
    path_manager = PathManager()
    
    try:
        # ‚úÖ ENHANCED: Handle overwrite mode
        if overwrite:
            print(f"üîÑ Overwrite mode: Replacing existing source '{name}'")
            
            # Delete existing source with same name first
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("DELETE FROM video_sources WHERE name = ?", (name,))
            deleted_count = cursor.rowcount
            conn.commit()
            conn.close()
            
            print(f"‚úÖ Deleted {deleted_count} existing source(s) with name '{name}'")
        else:
            # ‚úÖ EXISTING: Disable all existing sources first (normal mode)
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("UPDATE video_sources SET active = 0")
            conn.commit()
            conn.close()
        
        # ‚úÖ COMMON: Add new source as active
        success, message = path_manager.add_source(source_type, name, path, config_data)
        
        if success:
            # Get source ID for database operations
            source_id = path_manager.get_source_id_by_name(name)
            
            # Calculate correct working path and update processing_config (NO NVR)
            working_path = get_working_path_for_source(source_type, name, path)
            
            # Update processing_config.input_path to point to working path
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE processing_config 
                SET input_path = ? 
                WHERE id = 1
            """, (working_path,))
            
            print(f"‚úÖ Updated processing_config.input_path to: {working_path}")
            
            # Handle different source types (existing logic)
            if source_type == 'cloud':
                print(f"üîß PROCESSING CLOUD SOURCE...")
                
                selected_folders = config_data.get('selected_folders', [])
                tree_folders = config_data.get('selected_tree_folders', [])
                all_folders = selected_folders + tree_folders
                
                if all_folders:
                    cloud_cameras = extract_cameras_from_cloud_folders(all_folders)
                    cloud_cameras = list(set(cloud_cameras))
                    
                    cursor.execute("""
                        UPDATE processing_config 
                        SET selected_cameras = ? 
                        WHERE id = 1
                    """, (json.dumps(cloud_cameras),))
                    
                    print(f"‚úÖ Cloud cameras synced to processing_config: {cloud_cameras}")
                    
                    # Create camera directories
                    try:
                        for camera_name in cloud_cameras:
                            camera_dir = os.path.join(working_path, camera_name)
                            os.makedirs(camera_dir, exist_ok=True)
                            print(f"üìÅ Created camera directory: {camera_dir}")
                    except Exception as dir_error:
                        print(f"‚ö†Ô∏è Could not create camera directories: {dir_error}")
            
            elif source_type == 'local':
                # Auto-detect cameras from file system
                try:
                    cameras = detect_camera_folders(working_path)
                    if cameras:
                        cursor.execute("""
                            UPDATE processing_config 
                            SET selected_cameras = ? 
                            WHERE id = 1
                        """, (json.dumps(cameras),))
                        print(f"‚úÖ Local cameras auto-selected: {cameras}")
                except Exception as camera_error:
                    print(f"Camera detection failed: {camera_error}")
                    cursor.execute("UPDATE processing_config SET selected_cameras = '[]' WHERE id = 1")
            
            conn.commit()
            conn.close()
            
            # ‚úÖ ENHANCED RESPONSE
            action_taken = "replaced" if overwrite else "added"
            response_data = {
                "message": f"Source '{name}' {action_taken} successfully",
                "source_type": source_type,
                "connection_path": path,
                "working_path": working_path,
                "action": action_taken,
                "overwrite_mode": overwrite
            }
            
            return jsonify(response_data), 200
        else:
            return jsonify({"error": f"Failed to save source: {message}"}), 400
            
    except Exception as e:
        print(f"Failed to save sources: {str(e)}")
        return jsonify({"error": f"Failed to save sources: {str(e)}"}), 500

@config_bp.route('/test-source', methods=['POST'])  
def test_source_connection():
    """Test connectivity for local and cloud source types only"""
    try:
        # Ensure we have valid JSON request
        if not request.is_json:
            return jsonify({
                "accessible": False,
                "message": "Invalid request format - JSON required",
                "source_type": "unknown"
            }), 400
        
        data = request.get_json()
        if not data:
            return jsonify({
                "accessible": False,
                "message": "No data provided",
                "source_type": "unknown"
            }), 400
        
        source_type = data.get('source_type')
        
        if not source_type:
            return jsonify({
                "accessible": False,
                "message": "source_type is required",
                "source_type": "unknown"
            }), 400
        
        # Handle different source types (NO NVR)
        if source_type == 'local':
            # Existing local path validation
            source_config = {
                'source_type': source_type,
                'path': data.get('path'),
                'config': data.get('config', {})
            }
            
            if not source_config['path']:
                return jsonify({
                    "accessible": False,
                    "message": "path is required for local sources",
                    "source_type": source_type
                }), 400
            
            path_manager = PathManager()
            is_accessible, message = path_manager.validate_source_accessibility(source_config)
            
            return jsonify({
                "accessible": is_accessible,
                "message": message,
                "source_type": source_type
            }), 200
            
        elif source_type == 'cloud':
            # ‚úÖ Cloud connection testing + folder discovery
            from modules.sources.cloud_manager import CloudManager
            cloud_manager = CloudManager(provider='google_drive')
            result = cloud_manager.test_connection_and_discover_folders(data)
            return jsonify(result), 200
            
        else:
            return jsonify({
                "accessible": False,
                "message": f"Source type '{source_type}' not supported. Only 'local' and 'cloud' are available.",
                "source_type": source_type
            }), 400
        
    except ImportError as e:
        return jsonify({
            "accessible": False,
            "message": f"Required module not available: {str(e)}",
            "source_type": data.get('source_type', 'unknown')
        }), 500
        
    except json.JSONDecodeError:
        return jsonify({
            "accessible": False,
            "message": "Invalid JSON format",
            "source_type": "unknown"
        }), 400
        
    except Exception as e:
        return jsonify({
            "accessible": False,
            "message": f"Test failed: {str(e)}",
            "source_type": data.get('source_type', 'unknown')
        }), 500

@config_bp.route('/get-sources', methods=['GET'])
def get_video_sources():
    """Get all video sources"""
    try:
        path_manager = PathManager()
        sources = path_manager.get_all_active_sources()
        
        return jsonify({"sources": sources}), 200
        
    except Exception as e:
        return jsonify({"error": f"Failed to retrieve sources: {str(e)}"}), 500

@config_bp.route('/update-source/<int:source_id>', methods=['PUT'])
def update_video_source(source_id):
    """üîß Simple update video source - same type only, mainly for camera selection"""
    try:
        data = request.json
        path_manager = PathManager()
        
        # Get current source for validation
        current_source = path_manager.get_source_by_id(source_id)
        if not current_source:
            return jsonify({"error": f"Source with id {source_id} not found"}), 404
        
        # For now, we only support updating the config (mainly for camera selection)
        # Path and source_type changes are handled by "Change" button workflow
        new_config = data.get('config', current_source['config'])
        
        # üîÑ Update source config only
        success, message = path_manager.update_source(source_id, config=new_config)
        
        if not success:
            return jsonify({"error": message}), 400
        
        return jsonify({
            "message": message,
            "source_id": source_id,
            "updated_fields": ["config"]
        }), 200
        
    except Exception as e:
        return jsonify({"error": f"Failed to update source: {str(e)}"}), 500

@config_bp.route('/delete-source/<int:source_id>', methods=['DELETE'])
def delete_video_source(source_id):
    """üîÑ Delete video source (used by Change button to reset workflow)"""
    path_manager = PathManager()
    
    try:
        # Get source info before deletion for logging
        source = path_manager.get_source_by_id(source_id)
        source_name = source.get('name', 'Unknown') if source else 'Unknown'
        
        success, message = path_manager.delete_source(source_id)
        
        if success:
            # üßπ Clean reset processing_config 
            try:
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE processing_config 
                    SET input_path = '', selected_cameras = '[]' 
                    WHERE id = 1
                """)
                conn.commit()
                conn.close()
                
                print(f"Source '{source_name}' deleted and processing_config reset")
                
            except Exception as config_error:
                print(f"Failed to reset processing_config: {config_error}")
            
            return jsonify({
                "message": f"Source '{source_name}' removed successfully. You can now add a new source.",
                "reset": True
            }), 200
        else:
            return jsonify({"error": message}), 400
            
    except Exception as e:
        return jsonify({"error": f"Failed to delete source: {str(e)}"}), 500

@config_bp.route('/toggle-source/<int:source_id>', methods=['POST'])
def toggle_source_status(source_id):
    """Toggle source active status"""
    data = request.json
    active = data.get('active', True)
    path_manager = PathManager()
    
    try:
        if active:
            # Disable all other sources first (Single Active Source)
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("UPDATE video_sources SET active = 0")
            conn.commit()
            conn.close()
        
        success, message = path_manager.toggle_source_status(source_id, active)
        
        if success and active:
            # Update input_path to this source
            source = path_manager.get_source_by_id(source_id)
            if source:
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE processing_config 
                    SET input_path = ? 
                    WHERE id = 1
                """, (source['path'],))
                
                # Auto-detect cameras for local sources
                if source['source_type'] == 'local':
                    try:
                        cameras = detect_camera_folders(source['path'])
                        if cameras:
                            cursor.execute("""
                                UPDATE processing_config 
                                SET selected_cameras = ? 
                                WHERE id = 1
                            """, (json.dumps(cameras),))
                        else:
                            cursor.execute("""
                                UPDATE processing_config 
                                SET selected_cameras = '[]' 
                                WHERE id = 1
                            """)
                    except Exception as camera_error:
                        print(f"Camera detection failed: {camera_error}")
                        cursor.execute("""
                            UPDATE processing_config 
                            SET selected_cameras = '[]' 
                            WHERE id = 1
                        """)
                else:
                    # Clear cameras for non-local sources
                    cursor.execute("""
                        UPDATE processing_config 
                        SET selected_cameras = '[]' 
                        WHERE id = 1
                    """)
                
                conn.commit()
                conn.close()
        
        if success:
            return jsonify({"message": message}), 200
        else:
            return jsonify({"error": message}), 400
            
    except Exception as e:
        return jsonify({"error": f"Failed to toggle source status: {str(e)}"}), 500
    
@config_bp.route('/get-processing-cameras', methods=['GET'])
def get_processing_cameras():
    """Get selected cameras from processing_config"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT selected_cameras FROM processing_config WHERE id = 1")
        result = cursor.fetchone()
        conn.close()
        
        if result and result[0]:
            selected_cameras = json.loads(result[0])
            return jsonify({
                "selected_cameras": selected_cameras,
                "count": len(selected_cameras)
            }), 200
        else:
            return jsonify({
                "selected_cameras": [],
                "count": 0
            }), 200
            
    except Exception as e:
        return jsonify({"error": f"Failed to get processing cameras: {str(e)}"}), 500

# üîß ADD THESE ENDPOINTS TO config.py (ch√®n sau existing endpoints)

@config_bp.route('/sync-cloud-cameras', methods=['POST'])
def sync_cloud_cameras():
    """üîß Manual sync cloud cameras from active cloud source"""
    try:
        # Get active cloud source
        path_manager = PathManager()
        sources = path_manager.get_all_active_sources()
        cloud_source = None
        
        for source in sources:
            if source['source_type'] == 'cloud':
                cloud_source = source
                break
        
        if not cloud_source:
            return jsonify({"error": "No active cloud source found"}), 404
        
        print(f"üîÑ Syncing cameras for cloud source: {cloud_source['name']}")
        
        # Extract cameras from cloud config
        config_data = cloud_source.get('config', {})
        selected_folders = config_data.get('selected_folders', [])
        tree_folders = config_data.get('selected_tree_folders', [])
        all_folders = selected_folders + tree_folders
        
        if all_folders:
            cloud_cameras = extract_cameras_from_cloud_folders(all_folders)
            cloud_cameras = list(set(cloud_cameras))  # Remove duplicates
            
            print(f"üé• Generated cloud cameras: {cloud_cameras}")
            
            # Update processing_config
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE processing_config 
                SET selected_cameras = ? 
                WHERE id = 1
            """, (json.dumps(cloud_cameras),))
            conn.commit()
            conn.close()
            
            print(f"‚úÖ Cloud cameras synced to processing_config")
            
            return jsonify({
                "success": True,
                "message": f"Synced {len(cloud_cameras)} cameras from cloud source",
                "cameras": cloud_cameras,
                "source_name": cloud_source['name']
            }), 200
        else:
            return jsonify({
                "success": False,
                "message": "No folders found in cloud source config"
            }), 400
            
    except Exception as e:
        print(f"‚ùå Error syncing cloud cameras: {e}")
        return jsonify({"error": f"Failed to sync cloud cameras: {str(e)}"}), 500

@config_bp.route('/refresh-cameras', methods=['POST'])
def refresh_cameras():
    """üîÑ Refresh cameras based on active source type"""
    try:
        # Get active source
        path_manager = PathManager()
        sources = path_manager.get_all_active_sources()
        
        if not sources:
            return jsonify({
                "success": False,
                "message": "No active source found",
                "cameras": []
            }), 404
        
        active_source = sources[0]  # Single active source
        source_type = active_source['source_type']
        source_name = active_source['name']
        
        print(f"üîÑ Refreshing cameras for {source_type} source: {source_name}")
        
        cameras = []
        
        if source_type == 'local':
            # Local: Scan directories
            working_path = active_source['path']
            if os.path.exists(working_path):
                detected_cameras = detect_camera_folders(working_path)
                cameras = detected_cameras
                print(f"üìÅ Local cameras detected: {cameras}")
            else:
                print(f"‚ö†Ô∏è Local path not found: {working_path}")
                
        elif source_type == 'cloud':
            # Cloud: Extract from config
            config_data = active_source.get('config', {})
            selected_folders = config_data.get('selected_folders', [])
            tree_folders = config_data.get('selected_tree_folders', [])
            all_folders = selected_folders + tree_folders
            
            if all_folders:
                cameras = extract_cameras_from_cloud_folders(all_folders)
                cameras = list(set(cameras))  # Remove duplicates
                print(f"‚òÅÔ∏è Cloud cameras extracted: {cameras}")
            else:
                # Also check existing selected_cameras in config
                cameras = config_data.get('selected_cameras', [])
                print(f"‚òÅÔ∏è Using existing cloud cameras: {cameras}")
        
        # Update processing_config if cameras found
        if cameras:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE processing_config 
                SET selected_cameras = ? 
                WHERE id = 1
            """, (json.dumps(cameras),))
            conn.commit()
            conn.close()
            
            print(f"‚úÖ Updated processing_config with {len(cameras)} cameras")
        
        return jsonify({
            "success": True,
            "message": f"Refreshed {len(cameras)} cameras from {source_type} source",
            "cameras": cameras,
            "source_type": source_type,
            "source_name": source_name
        }), 200
        
    except Exception as e:
        print(f"‚ùå Error refreshing cameras: {e}")
        return jsonify({
            "success": False,
            "error": f"Failed to refresh cameras: {str(e)}",
            "cameras": []
        }), 500

@config_bp.route('/camera-status', methods=['GET'])
def get_camera_status():
    """üìä Get comprehensive camera status for debugging"""
    try:
        # Get processing_config cameras
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT selected_cameras, input_path FROM processing_config WHERE id = 1")
        result = cursor.fetchone()
        conn.close()
        
        processing_cameras = []
        input_path = ""
        
        if result:
            selected_cameras, input_path = result
            try:
                processing_cameras = json.loads(selected_cameras) if selected_cameras else []
            except:
                processing_cameras = []
        
        # Get active sources
        path_manager = PathManager()
        active_sources = path_manager.get_all_active_sources()
        
        # Get source-specific camera info
        source_cameras = []
        source_info = None
        
        if active_sources:
            source = active_sources[0]
            source_info = {
                "name": source['name'],
                "type": source['source_type'],
                "path": source['path']
            }
            
            if source['source_type'] == 'cloud':
                config_data = source.get('config', {})
                selected_folders = config_data.get('selected_folders', [])
                tree_folders = config_data.get('selected_tree_folders', [])
                all_folders = selected_folders + tree_folders
                
                if all_folders:
                    source_cameras = extract_cameras_from_cloud_folders(all_folders)
                    source_cameras = list(set(source_cameras))
                
                source_info.update({
                    "folders_count": len(all_folders),
                    "legacy_folders": len(selected_folders),
                    "tree_folders": len(tree_folders)
                })
                
            elif source['source_type'] == 'local':
                working_path = source['path']
                if os.path.exists(working_path):
                    source_cameras = detect_camera_folders(working_path)
        
        # Check sync status
        cameras_synced = set(processing_cameras) == set(source_cameras)
        
        return jsonify({
            "processing_config": {
                "cameras": processing_cameras,
                "camera_count": len(processing_cameras),
                "input_path": input_path
            },
            "active_source": source_info,
            "source_cameras": {
                "cameras": source_cameras,
                "camera_count": len(source_cameras)
            },
            "sync_status": {
                "cameras_synced": cameras_synced,
                "needs_sync": not cameras_synced
            },
            "recommendations": {
                "action_needed": "sync_cloud_cameras" if source_info and source_info['type'] == 'cloud' and not cameras_synced else None,
                "message": "Cameras not synced - use /sync-cloud-cameras endpoint" if not cameras_synced else "Cameras are in sync"
            },
            "timestamp": datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        return jsonify({
            "error": f"Failed to get camera status: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }), 500
    
# Th√™m v√†o cu·ªëi file config.py:

if __name__ == "__main__":
    try:
        print("üöÄ Starting VTrack Config Server...")
        
        # Initialize app v√† config
        app, db_path, logger = init_app_and_config()
        
        # Register blueprint
        app.register_blueprint(config_bp, url_prefix='/api/config')
        
        # Import v√† register cloud blueprint n·∫øu c√≥
        try:
            from modules.sources.cloud_endpoints import cloud_bp
            app.register_blueprint(cloud_bp, url_prefix='/api/cloud')
            print("‚úÖ Cloud endpoints registered")
        except ImportError:
            print("‚ö†Ô∏è  Cloud endpoints not available")
        # Import v√† register sync blueprint
        try:
            from modules.sources.sync_endpoints import sync_bp
            app.register_blueprint(sync_bp, url_prefix='/api')
            print("‚úÖ Sync endpoints registered")
        except ImportError as e:
            print(f"‚ö†Ô∏è  Sync endpoints not available: {e}")

        
        # Initialize database manager
        try:
            from database import get_db_manager
            db_manager = get_db_manager()
            config_manager = init_config(db_manager)
            print("‚úÖ Database manager initialized")
        except Exception as e:
            print(f"‚ö†Ô∏è  Database manager error: {e}")
        
        # Add simple test endpoint
        @app.route('/api/test', methods=['GET'])
        def test():
            return jsonify({
                'status': 'ok',
                'message': 'VTrack Config Server is running!',
                'database': db_path,
                'timestamp': datetime.now().isoformat()
            })
        
        # Server info
        print(f"‚úÖ Database: {db_path}")
        print(f"üåê Server will start on: http://localhost:8080")
        print(f"üîß Test URL: http://localhost:8080/api/test")
        print(f"üì° API Base: http://localhost:8080/api/config/")
        print("="*50)
        
        # Start server
        app.run(
            host='0.0.0.0',  # Accept t·ª´ m·ªçi IP
            port=8080, 
            debug=True,      # Enable debug mode
            use_reloader=False  # Tr√°nh restart 2 l·∫ßn
        )
        
    except Exception as e:
        print(f"‚ùå Failed to start server: {e}")
        import traceback
        traceback.print_exc()
```
## üìÑ File: `logging_config.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/config/logging_config.py`

```python
import logging
import os
import sys
from datetime import datetime
from logging.handlers import RotatingFileHandler

class LogSizeFilter(logging.Filter):
    def __init__(self, log_file, max_size=10*1024*1024):
        super().__init__()
        self.log_file = log_file
        self.max_size = max_size
    
    def filter(self, record):
        if os.path.exists(self.log_file) and os.path.getsize(self.log_file) > self.max_size:
            if record.levelno < logging.INFO:
                return False
            print("Log file size exceeds 10MB, switching to INFO level", file=sys.stderr)
            return True
        return True

class ContextAdapter(logging.LoggerAdapter):
    def process(self, msg, kwargs):
        context = " ".join(f"{k}={v}" for k, v in self.extra.items())
        return f"[{context}] {msg}", kwargs

def setup_logging(base_dir, app_name="app", log_level=logging.INFO):
    log_dir = os.path.join(base_dir, "resources", "output_clips", "LOG")
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f"{app_name}_{datetime.now().strftime('%Y-%m-%d')}.log")
    
    handler = RotatingFileHandler(log_file, maxBytes=10*1024*1024, backupCount=5)
    handler.setFormatter(logging.Formatter(
        '%(asctime)sZ [%(levelname)s] %(name)s: %(message)s',
        datefmt='%Y-%m-%dT%H:%M:%S'
    ))
    handler.addFilter(LogSizeFilter(log_file))
    
    logging.basicConfig(level=log_level, handlers=[handler])

def get_logger(module_name, context=None, separate_log=None):
    logger = logging.getLogger("app")
    if separate_log:
        log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "resources", "output_clips", "LOG")
        os.makedirs(log_dir, exist_ok=True)
        log_file = os.path.join(log_dir, f"{separate_log}_{datetime.now().strftime('%Y-%m-%d')}.log")
        file_handler = RotatingFileHandler(log_file, maxBytes=10*1024*1024, backupCount=5)
        file_handler.setFormatter(logging.Formatter(
            '%(asctime)s,%(msecs)03d - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        ))
        file_handler.setLevel(logging.INFO)
        logger.addHandler(file_handler)
    return ContextAdapter(logger, context or {})
```
## üìÑ File: `__init__.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/config/__init__.py`

```python

```
## üìÑ File: `file_lister.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/scheduler/file_lister.py`

```python
import os
import sqlite3
import json
import logging
from datetime import datetime, timedelta
from statistics import median
from modules.db_utils import find_project_root, get_db_connection
from .db_sync import db_rwlock
import subprocess
import pytz

# C·∫•u h√¨nh m√∫i gi·ªù Vi·ªát Nam
VIETNAM_TZ = pytz.timezone('Asia/Ho_Chi_Minh')

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

logger = logging.getLogger("app")
logger.info("Logging initialized")

# H·∫±ng s·ªë cho qu√©t ƒë·ªông
BUFFER_SECONDS = 6 * 60
N_FILES_FOR_ESTIMATE = 3
DEFAULT_DAYS = 7

DB_PATH = os.path.join(BASE_DIR, "database", "events.db")
os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)

def get_db_path():
    try:
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT db_path FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            conn.close()
            return result[0] if result else DB_PATH
    except Exception as e:
        logger.error(f"L·ªói khi l·∫•y DB_PATH: {e}")
        return DB_PATH

DB_PATH = get_db_path()
logger.info(f"S·ª≠ d·ª•ng DB_PATH: {DB_PATH}")

def get_file_creation_time(file_path):
    """L·∫•y th·ªùi gian t·∫°o t·ªáp b·∫±ng FFmpeg, chu·∫©n h√≥a theo m√∫i gi·ªù Vi·ªát Nam."""
    if not os.path.isfile(file_path) or not file_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv')):
        return datetime.fromtimestamp(os.path.getctime(file_path), tz=VIETNAM_TZ)
    try:
        cmd = [
            "ffprobe",
            "-v", "quiet",
            "-print_format", "json",
            "-show_entries", "format_tags=creation_time:format=creation_time",
            file_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        metadata = json.loads(result.stdout)
        creation_time = (
            metadata.get("format", {})
                    .get("tags", {})
                    .get("creation_time")
            or metadata.get("format", {}).get("creation_time")  
        )  
        if creation_time:
            utc_time = datetime.strptime(creation_time, "%Y-%m-%dT%H:%M:%S.%fZ")
            utc_time = pytz.utc.localize(utc_time)
            return utc_time.astimezone(VIETNAM_TZ)
        else:
            logger.warning(f"Kh√¥ng t√¨m th·∫•y creation_time cho {file_path}, d√πng gi·ªù h·ªá th·ªëng")
            return datetime.fromtimestamp(os.path.getctime(file_path), tz=VIETNAM_TZ)
    except (subprocess.CalledProcessError, json.JSONDecodeError, ValueError) as e:
        logger.error(f"L·ªói khi l·∫•y creation_time cho {file_path}: {e}")
        return datetime.fromtimestamp(os.path.getctime(file_path), tz=VIETNAM_TZ)

def compute_chunk_interval(ctimes):
    ctimes = sorted(ctimes)[-N_FILES_FOR_ESTIMATE:]
    if len(ctimes) < 2:
        return 30
    intervals = [(ctimes[i+1] - ctimes[i]) / 60 for i in range(len(ctimes)-1)]
    return round(median(intervals))

def scan_files(root_path, video_root, time_threshold, max_ctime, restrict_to_current_date=False, camera_ctime_map=None, working_days=None, from_time=None, to_time=None, selected_cameras=None, strict_date_match=False):
    video_files = []
    file_ctimes = []
    video_extensions = ('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv')
    current_date = datetime.now(VIETNAM_TZ).date()
    skipped_by_ctime = 0
    skipped_by_camera = 0
    ffprobe_errors = 0

    for root, dirs, files in os.walk(root_path):
        relative_path = os.path.relpath(root, video_root)
        camera_name = relative_path.split(os.sep)[0] if relative_path != "." else os.path.basename(video_root)
        if selected_cameras and camera_name not in selected_cameras:
            skipped_by_camera += len([f for f in files if f.lower().endswith(video_extensions)])
            continue

        for file in files:
            if file.lower().endswith(video_extensions):
                file_path = os.path.join(root, file)
                try:
                    file_ctime = get_file_creation_time(file_path)
                except Exception:
                    ffprobe_errors += 1
                    file_ctime = datetime.fromtimestamp(os.path.getctime(file_path), tz=VIETNAM_TZ)

                logger.debug(f"Checking file {file_path}, ctime={file_ctime}, max_ctime={max_ctime}")
                if time_threshold and file_ctime < time_threshold:
                    skipped_by_ctime += 1
                    continue

                if max_ctime and file_ctime <= max_ctime:
                    skipped_by_ctime += 1
                    continue

                weekday = file_ctime.strftime('%A')
                if working_days and weekday not in working_days:
                    skipped_by_ctime += 1
                    logger.debug(f"Skipped file {file_path} due to non-working day: {weekday}")
                    continue

                file_time = file_ctime.time()
                if from_time and to_time and not (from_time <= file_time <= to_time):
                    skipped_by_ctime += 1
                    logger.debug(f"Skipped file {file_path} due to time outside working hours: {file_time}")
                    continue

                relative_path = os.path.relpath(file_path, video_root)
                video_files.append(relative_path)
                file_ctimes.append(file_ctime.timestamp())
                logger.info(f"T√¨m th·∫•y t·ªáp: {file_path}")

        if camera_ctime_map is not None:
            dir_ctime = datetime.fromtimestamp(os.path.getctime(root), tz=VIETNAM_TZ)
            camera_ctime_map[camera_name] = max(camera_ctime_map.get(camera_name, 0), dir_ctime.timestamp())

    logger.info(f"Th·ªëng k√™ qu√©t: {skipped_by_ctime} t·ªáp b·ªè qua do ctime, {skipped_by_camera} t·ªáp b·ªè qua do camera, {ffprobe_errors} l·ªói ffprobe")
    return video_files, file_ctimes

def save_files_to_db(conn, video_files, file_ctimes, scan_action, days, custom_path, video_root):
    if not video_files:
        return

    insert_data = []
    days_val = len(days) if isinstance(days, list) else days if days is not None else None
    for file_path, file_ctime in zip(video_files, file_ctimes):
        absolute_path = os.path.join(video_root, file_path) if scan_action != "custom" or not os.path.isfile(custom_path) else custom_path
        file_ctime_dt = datetime.fromtimestamp(file_ctime, tz=VIETNAM_TZ)
        priority = 1 if scan_action == "custom" else 0
        relative_path = os.path.relpath(absolute_path, video_root) if scan_action != "custom" else os.path.dirname(absolute_path)
        camera_name = relative_path.split(os.sep)[0] if relative_path != "." else os.path.basename(video_root)
        insert_data.append((
            scan_action, days_val, custom_path, absolute_path, datetime.now(VIETNAM_TZ), file_ctime_dt, priority, camera_name, 'pending', 0
        ))

    with conn:
        cursor = conn.cursor()
        cursor.executemany('''
            INSERT INTO file_list (program_type, days, custom_path, file_path, created_at, ctime, priority, camera_name, status, is_processed)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', insert_data)
        logger.info(f"ƒê√£ ch√®n {len(insert_data)} t·ªáp v√†o file_list")

def list_files(video_root, scan_action, custom_path, days, db_path, default_scan_days=7, camera_ctime_map=None, is_initial_scan=False):
    try:
        with db_rwlock.gen_wlock():
            conn = get_db_connection()
            cursor = conn.cursor()

            if not os.path.exists(video_root):
                try:
                    os.makedirs(video_root, exist_ok=True)
                    logger.info(f"ƒê√£ t·∫°o th∆∞ m·ª•c video root: {video_root}")
                except Exception as e:
                    logger.error(f"Kh√¥ng th·ªÉ t·∫°o th∆∞ m·ª•c video root: {video_root}, l·ªói: {str(e)}")
                    raise Exception(f"Kh√¥ng th·ªÉ t·∫°o th∆∞ m·ª•c video root: {str(e)}")

            cursor.execute('SELECT MAX(ctime) FROM file_list')
            last_ctime = cursor.fetchone()[0]
            max_ctime = datetime.fromisoformat(last_ctime.replace('Z', '+00:00')) if last_ctime else datetime.min.replace(tzinfo=VIETNAM_TZ)

            cursor.execute("SELECT input_path, selected_cameras FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            if result:
                video_root = result[0]
                selected_cameras = json.loads(result[1]) if result[1] else []
            else:
                selected_cameras = []
            logger.info(f"S·ª≠ d·ª•ng video_root: {video_root}, Camera ƒë∆∞·ª£c ch·ªçn: {selected_cameras}")

            cursor.execute("SELECT working_days, from_time, to_time FROM general_info WHERE id = 1")
            general_info = cursor.fetchone()
            if general_info:
                try:
                    working_days_raw = general_info[0].encode('utf-8').decode('utf-8') if general_info[0] else ''
                    working_days = json.loads(working_days_raw) if working_days_raw else []
                except json.JSONDecodeError as e:
                    logger.error(f"JSON kh√¥ng h·ª£p l·ªá trong working_days: {general_info[0]}, l·ªói: {e}")
                    working_days = []
                from_time = datetime.strptime(general_info[1], '%H:%M').time() if general_info[1] else None
                to_time = datetime.strptime(general_info[2], '%H:%M').time() if general_info[2] else None
            else:
                working_days, from_time, to_time = [], None, None
            logger.info(f"Ng√†y l√†m vi·ªác: {working_days}, from_time: {from_time}, to_time: {to_time}")

            video_files = []
            file_ctimes = []

            if scan_action == "custom" and custom_path:
                if not os.path.exists(custom_path):
                    raise Exception(f"ƒê∆∞·ªùng d·∫´n kh√¥ng t·ªìn t·∫°i: {custom_path}")
                if os.path.isfile(custom_path) and custom_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv')):
                    file_name = os.path.basename(custom_path)
                    file_ctime = get_file_creation_time(custom_path)
                    video_files.append(file_name)
                    file_ctimes.append(file_ctime.timestamp())
                    logger.info(f"T√¨m th·∫•y t·ªáp: {custom_path}")
                else:
                    video_files, file_ctimes = scan_files(
                        custom_path, video_root, None, None, False, None,
                        working_days, from_time, to_time, selected_cameras, strict_date_match=False
                    )
            elif scan_action == "first" and days:
                time_threshold = datetime.now(VIETNAM_TZ) - timedelta(days=days)
                video_files, file_ctimes = scan_files(
                    video_root, video_root, time_threshold, None, False, None,
                    working_days, from_time, to_time, selected_cameras, strict_date_match=False
                )
                cursor.execute('''
                    INSERT OR REPLACE INTO program_status (id, key, value)
                    VALUES ((SELECT id FROM program_status WHERE key = 'first_run_completed'), 'first_run_completed', 'true')
                ''')
                conn.commit()
            else:  # default
                time_threshold = datetime.now(VIETNAM_TZ) - timedelta(days=default_scan_days) if is_initial_scan else datetime.now(VIETNAM_TZ) - timedelta(days=1)
                restrict_to_current_date = not is_initial_scan
                video_files, file_ctimes = scan_files(
                    video_root, video_root, time_threshold, max_ctime, restrict_to_current_date, camera_ctime_map,
                    working_days, from_time, to_time, selected_cameras, strict_date_match=True
                )

            save_files_to_db(conn, video_files, file_ctimes, scan_action, days, custom_path, video_root)
            conn.close()
        logger.info(f"T√¨m th·∫•y {len(video_files)} t·ªáp video")
        return video_files, file_ctimes
    except Exception as e:
        logger.error(f"L·ªói trong list_files: {e}")
        raise Exception(f"L·ªói trong list_files: {str(e)}")

def cleanup_stale_jobs():
    try:
        with db_rwlock.gen_wlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE file_list 
                SET status = 'pending'
                WHERE status = 'ƒëang frame sampler ...'
                AND created_at < datetime('now', '-59 minutes')
            """)
            affected = cursor.rowcount
            conn.commit()
            conn.close()
            if affected > 0:
                logger.info(f"Reset {affected} stale jobs to pending")
    except Exception as e:
        logger.error(f"Error cleaning up stale jobs: {e}")

def run_file_scan(scan_action="default", days=None, custom_path=None):
    db_path = get_db_path()
    cleanup_stale_jobs()
    try:
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT input_path FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            conn.close()
            video_root = result[0] if result else ""
        if not video_root:
            raise Exception("Kh√¥ng t√¨m th·∫•y video_root trong processing_config")
        camera_ctime_map = {}
        is_initial_scan = scan_action == "default" and days is not None  # ƒê·∫∑t is_initial_scan=True cho l·∫ßn qu√©t ƒë·∫ßu ti√™n
        files, _ = list_files(video_root, scan_action, custom_path, days, db_path, camera_ctime_map=camera_ctime_map, is_initial_scan=is_initial_scan)
        return files
    except Exception as e:
        logger.error(f"L·ªói trong run_file_scan: {e}")
        raise

```
## üìÑ File: `db_sync.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/scheduler/db_sync.py`

```python
from readerwriterlock import rwlock
import threading

db_rwlock = rwlock.RWLockFairD()  # S·ª≠ d·ª•ng RWLockFairD ƒë·ªÉ tr√°nh deadlock
frame_sampler_event = threading.Event()
event_detector_event = threading.Event()
event_detector_done = threading.Event()
event_detector_done.set()  # Ban ƒë·∫ßu cho ph√©p Frame Sampler ch·∫°y

```
## üìÑ File: `system_monitor.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/scheduler/system_monitor.py`

```python
import psutil
import logging
import os
from datetime import datetime
from modules.config.logging_config import get_logger


# ƒê∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi t·ª´ project root
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class SystemMonitor:
    def __init__(self):
        self.min_batch_size = 2
        self.max_batch_size = 6
        self.cpu_threshold_low = 70  # TƒÉng batch_size n·∫øu CPU < 70%
        self.cpu_threshold_high = 90  # Gi·∫£m batch_size n·∫øu CPU > 90%
        self.setup_logging()

    def setup_logging(self):
        self.logger = get_logger(__name__, {"module": "system_monitor"})
        self.logger.info("SystemMonitor logging initialized")

    def get_system_metrics(self):
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            logging.info(f"System metrics retrieved: CPU={cpu_percent}%, Memory={memory_percent}%")
            return cpu_percent, memory_percent
        except Exception as e:
            logging.error(f"Error getting system metrics: {str(e)}")
            return 50.0, 50.0  # Gi√° tr·ªã m·∫∑c ƒë·ªãnh n·∫øu l·ªói

    def get_batch_size(self, current_batch_size=2):
        logging.info(f"Calculating batch size, current={current_batch_size}")
        cpu_percent, memory_percent = self.get_system_metrics()
        if cpu_percent < self.cpu_threshold_low and memory_percent < self.cpu_threshold_low:
            new_batch_size = min(current_batch_size + 1, self.max_batch_size)
        elif cpu_percent > self.cpu_threshold_high or memory_percent > self.cpu_threshold_high:
            new_batch_size = max(current_batch_size - 1, self.min_batch_size)
        else:
            new_batch_size = current_batch_size
        logging.info(f"Calculated batch_size: {new_batch_size}")
        return new_batch_size

    def log_timeout_warning(self, timeout_files, total_files):
        logging.info(f"Checking timeout: {timeout_files}/{total_files} files")
        if timeout_files > total_files * 0.1:
            logging.warning(f"Warning: {timeout_files}/{total_files} files timed out, consider increasing resources")
        else:
            logging.info("No timeout warning triggered")

```
## üìÑ File: `program_runner.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/scheduler/program_runner.py`

```python
import threading
import time
import os
import logging
from datetime import datetime
from modules.db_utils.db_utils import get_db_connection
from modules.technician.frame_sampler_trigger import FrameSamplerTrigger
from modules.technician.frame_sampler_no_trigger import FrameSamplerNoTrigger
from modules.technician.IdleMonitor import IdleMonitor
from modules.technician.event_detector import process_single_log
from .db_sync import db_rwlock, frame_sampler_event, event_detector_event, event_detector_done
import json
from modules.config.logging_config import  get_logger
import logging

logging.info("Logging initialized for program_runner")

# Bi·∫øn t·∫°m ƒë·ªÉ l∆∞u tr·∫°ng th√°i ch·∫°y v√† s·ªë ng√†y
running_state = {"current_running": None, "days": None, "custom_path": None, "files": []}
# Dictionary l∆∞u kh√≥a cho t·ª´ng nh√≥m video
video_locks = {}

def start_frame_sampler_thread(batch_size=1):
    logging.info(f"Starting {batch_size} frame sampler threads")
    threads = []
    for _ in range(batch_size):
        frame_sampler_thread = threading.Thread(target=run_frame_sampler)
        frame_sampler_thread.start()
        threads.append(frame_sampler_thread)
    return threads

def run_frame_sampler():
    logging.info("Frame sampler thread started")
    while True:  # V√≤ng l·∫∑p v√¥ h·∫°n ƒë·ªÉ thread lu√¥n ch·∫°y
        frame_sampler_event.wait()  # Ch·ªù th√¥ng b√°o t·ª´ event
        logging.debug("Frame sampler event received")
        try:
            with db_rwlock.gen_rlock():  # ƒê·ªìng b·ªô h√≥a truy c·∫≠p database
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("SELECT file_path, camera_name FROM file_list WHERE is_processed = 0 ORDER BY priority DESC, created_at ASC")
                video_files = [(row[0], row[1]) for row in cursor.fetchall()]
                conn.close()
                logging.info(f"Found {len(video_files)} unprocessed video files")

            if not video_files:
                logging.info("No video files to process, clearing event")
                frame_sampler_event.clear()  # X√≥a event v√† ti·∫øp t·ª•c ch·ªù
                continue

            for video_file, camera_name in video_files:
                # Ki·ªÉm tra tr·∫°ng th√°i video tr∆∞·ªõc khi x·ª≠ l√Ω
                with db_rwlock.gen_rlock():
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("SELECT status, is_processed FROM file_list WHERE file_path = ?", (video_file,))
                    result = cursor.fetchone()
                    conn.close()
                    if result and (result[0] == "ƒëang frame sampler ..." or result[1] == 1):
                        logging.info(f"Skipping video {video_file}: already being processed or completed")
                        continue

                # Kh√≥a theo video
                with db_rwlock.gen_wlock():
                    if video_file not in video_locks:
                        video_locks[video_file] = threading.Lock()
                video_lock = video_locks[video_file]
                if not video_lock.acquire(blocking=False):
                    logging.info(f"Skipping video {video_file}: locked by another thread")
                    continue

                try:
                    logging.info(f"Processing video: {video_file}")
                    # Ki·ªÉm tra qr_trigger_area v√† packing_area t·ª´ packing_profiles
                    with db_rwlock.gen_rlock():
                        conn = get_db_connection()
                        cursor = conn.cursor()
                        search_name = camera_name if camera_name else "CamTest"
                        if not camera_name:
                            logging.warning(f"No camera_name for {video_file}, falling back to CamTest")
                        cursor.execute("SELECT id, profile_name, qr_trigger_area, packing_area FROM packing_profiles WHERE profile_name LIKE ?", (f'%{search_name}%',))
                        profiles = cursor.fetchall()
                        conn.close()
                    
                    # Ch·ªçn profile c√≥ id l·ªõn nh·∫•t
                    trigger = [0, 0, 0, 0]
                    packing_area = None
                    selected_profile = None
                    if profiles:
                        selected_profile = max(profiles, key=lambda x: x[0])  # Ch·ªçn id l·ªõn nh·∫•t
                        profile_id, profile_name, qr_trigger_area, packing_area_raw = selected_profile
                        # Parse qr_trigger_area
                        try:
                            trigger = json.loads(qr_trigger_area) if qr_trigger_area else [0, 0, 0, 0]
                            if not isinstance(trigger, list) or len(trigger) != 4:
                                logging.error(f"Invalid qr_trigger_area for {profile_name}: {qr_trigger_area}, using default [0, 0, 0, 0]")
                                trigger = [0, 0, 0, 0]
                        except json.JSONDecodeError as e:
                            logging.error(f"Failed to parse qr_trigger_area for {profile_name}: {e}, using default [0, 0, 0, 0]")
                            trigger = [0, 0, 0, 0]
                        # Parse packing_area
                        try:
                            if packing_area_raw:
                                parsed = json.loads(packing_area_raw)
                                if isinstance(parsed, list) and len(parsed) == 4:
                                    packing_area = tuple(parsed)
                                else:
                                    logging.error(f"Invalid packing_area format for {profile_name}: {packing_area_raw}, using default None")
                                    packing_area = None
                            logging.info(f"Selected profile id={profile_id}, profile_name={profile_name}, qr_trigger_area={trigger}, packing_area={packing_area}")
                        except (ValueError, json.JSONDecodeError, KeyError, TypeError) as e:
                            logging.error(f"Failed to parse packing_area for {profile_name}: {e}, using default None")
                            packing_area = None
                    else:
                        logging.warning(f"No profile found for camera {search_name}, using default qr_trigger_area=[0, 0, 0, 0], packing_area=None")
                    
                    # Ch·∫°y IdleMonitor tr∆∞·ªõc FrameSampler, truy·ªÅn packing_area
                    idle_monitor = IdleMonitor()
                    logging.info(f"Running IdleMonitor for {video_file}")
                    idle_monitor.process_video(video_file, camera_name, packing_area)
                    work_block_queue = idle_monitor.get_work_block_queue()

                    # b·ªè qua file kh√¥ng c√≥ work block
                    if work_block_queue.empty():
                        logging.info(f"No work blocks found for {video_file}, skipping FrameSampler and log file creation")
                        with db_rwlock.gen_wlock():
                            conn = get_db_connection()
                            cursor = conn.cursor()
                            cursor.execute("UPDATE file_list SET status = ?, is_processed = 1 WHERE file_path = ?", ("xong", video_file))
                            conn.commit()
                            conn.close()
                        continue  # B·ªè qua FrameSampler v√† chuy·ªÉn sang video ti·∫øp theo
                    # Ch·ªçn FrameSampler d·ª±a tr√™n trigger
                    if trigger != [0, 0, 0, 0]:
                        frame_sampler = FrameSamplerTrigger()
                        logging.info(f"Using FrameSamplerTrigger for {video_file}")
                    else:
                        frame_sampler = FrameSamplerNoTrigger()
                        logging.info(f"Using FrameSamplerNoTrigger for {video_file}")

                    with db_rwlock.gen_wlock():  # Kh√≥a khi c·∫≠p nh·∫≠t tr·∫°ng th√°i
                        conn = get_db_connection()
                        cursor = conn.cursor()
                        cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("ƒëang frame sampler ...", video_file))
                        conn.commit()
                        conn.close()
                        logging.debug(f"Updated status for {video_file} to 'ƒëang frame sampler ...'")
                    
                    # G·ªçi process_video v·ªõi work block t·ª´ queue
                    log_file = None
                    while not work_block_queue.empty():
                        work_block = work_block_queue.get()
                        start_time = work_block['start_time']
                        end_time = work_block['end_time']
                        logging.info(f"Processing video block: start_time={start_time}, end_time={end_time}")
                        log_file = frame_sampler.process_video(
                            video_file,
                            video_lock=frame_sampler.video_lock,
                            get_packing_area_func=frame_sampler.get_packing_area,
                            process_frame_func=frame_sampler.process_frame,
                            frame_interval=frame_sampler.frame_interval,
                            start_time=start_time,
                            end_time=end_time
                        )
                    
                    with db_rwlock.gen_wlock():  # Kh√≥a khi c·∫≠p nh·∫≠t tr·∫°ng th√°i cu·ªëi
                        conn = get_db_connection()
                        cursor = conn.cursor()
                        if log_file:
                            cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("xong", video_file))
                            event_detector_event.set()  # K√≠ch ho·∫°t Event Detector sau m·ªói video
                            logging.info(f"Video {video_file} processed successfully, log file: {log_file}")
                        else:
                            cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("l·ªói", video_file))
                            logging.error(f"Failed to process video {video_file}")
                        conn.commit()
                        conn.close()
                    
                    # T·∫°m d·ª´ng sau khi x·ª≠ l√Ω xong m·ªôt video
                    logging.info(f"Frame Sampler pausing after processing {video_file}, waiting for Event Detector...")
                    while not event_detector_done.is_set():
                        time.sleep(1)  # Ch·ªù Event Detector ho√†n t·∫•t

                finally:
                    video_lock.release()
                    with db_rwlock.gen_wlock():
                        video_locks.pop(video_file, None)  # X√≥a kh√≥a sau khi x·ª≠ l√Ω xong
                    logging.debug(f"Released lock for {video_file}")

            frame_sampler_event.clear()  # X√≥a event sau khi x·ª≠ l√Ω h·∫øt file
            logging.info("All video files processed, clearing frame sampler event")
        except Exception as e:
            logging.error(f"Error in Frame Sampler thread: {str(e)}")
            frame_sampler_event.clear()  # ƒê·∫£m b·∫£o thread "ng·ªß" l·∫°i n·∫øu c√≥ l·ªói

def start_event_detector_thread():
    logging.info("Starting event detector thread")
    event_detector_thread = threading.Thread(target=run_event_detector)
    event_detector_thread.start()
    return event_detector_thread

def run_event_detector():
    logging.info("Event detector thread started")
    while True:
        event_detector_event.wait()
        logging.debug("Event detector event received")
        try:
            with db_rwlock.gen_rlock():  # ƒê·ªìng b·ªô h√≥a truy c·∫≠p database
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("SELECT log_file FROM processed_logs WHERE is_processed = 0")
                log_files = [row[0] for row in cursor.fetchall()]
                conn.close()
                logging.info(f"Found {len(log_files)} unprocessed log files")

            if not log_files:
                event_detector_event.clear()
                event_detector_done.set()  # B√°o hi·ªáu Frame Sampler ti·∫øp t·ª•c
                logging.info("No log files to process, clearing event and signaling done")
                continue

            for log_file in log_files:
                logging.info(f"Event Detector processing: {log_file}")
                process_single_log(log_file)
            event_detector_event.clear()
            event_detector_done.set()  # B√°o hi·ªáu Frame Sampler ti·∫øp t·ª•c sau khi x·ª≠ l√Ω h·∫øt log
            logging.info("All log files processed, clearing event and signaling done")
        except Exception as e:
            logging.error(f"Error in Event Detector thread: {str(e)}")
            event_detector_event.clear()
            event_detector_done.set()  # V·∫´n b√°o hi·ªáu Frame Sampler ti·∫øp t·ª•c n·∫øu c√≥ l·ªói

```
## üìÑ File: `program.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/scheduler/program.py`

```python
from flask import Blueprint, request, jsonify
import os
import json
import threading
import pytz
from datetime import datetime, timedelta
import logging
from modules.db_utils import find_project_root, get_db_connection
from .file_lister import run_file_scan, get_db_path
from .batch_scheduler import BatchScheduler
from .db_sync import frame_sampler_event, event_detector_event
import logging

VIETNAM_TZ = pytz.timezone('Asia/Ho_Chi_Minh')

program_bp = Blueprint('program', __name__)

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DB_PATH = os.path.join(BASE_DIR, "database", "events.db")
os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)

DB_PATH = get_db_path()
LOG_DIR = os.path.join(BASE_DIR, "../../resources/output_clips/LOG")
os.makedirs(LOG_DIR, exist_ok=True)

logger = logging.getLogger("app")
logger.info("Program logging initialized")

db_rwlock = threading.RLock()
running_state = {
    "days": None,
    "custom_path": None,
    "current_running": None,
    "files": []
}

scheduler = BatchScheduler()

def init_default_program():
    logger.info("Initializing default program")
    try:
        with db_rwlock:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('SELECT value FROM program_status WHERE key = "first_run_completed"')
            result = cursor.fetchone()
            conn.close()
        first_run_completed = result[0] == 'true' if result else False
        logger.info(f"First run completed: {first_run_completed}, Scheduler running: {scheduler.running}")
        if first_run_completed and not scheduler.running:
            logger.info("Chuy·ªÉn sang ch·∫ø ƒë·ªô ch·∫°y m·∫∑c ƒë·ªãnh (qu√©t l·∫∑p)")
            running_state["current_running"] = "M·∫∑c ƒë·ªãnh"
            scheduler.start()
    except Exception as e:
        logger.error(f"Error initializing default program: {e}")

init_default_program()

@program_bp.route('/program', methods=['POST'])
def program():
    logger.info(f"POST /program called, Current state before action: scheduler_running={scheduler.running}, running_state={running_state}")
    data = request.json
    card = data.get('card')
    action = data.get('action')
    custom_path = data.get('custom_path', '')
    days = data.get('days')

    if card == "L·∫ßn ƒë·∫ßu" and action == "run":
        try:
            with db_rwlock:
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute('SELECT value FROM program_status WHERE key = "first_run_completed"')
                result = cursor.fetchone()
                first_run_completed = result[0] == 'true' if result else False
                conn.close()
            if first_run_completed:
                logger.warning("First run already completed")
                return jsonify({"error": "L·∫ßn ƒë·∫ßu ƒë√£ ch·∫°y tr∆∞·ªõc ƒë√≥, kh√¥ng th·ªÉ ch·∫°y l·∫°i"}), 400
        except Exception as e:
            logger.error(f"Failed to check first run status: {str(e)}")
            return jsonify({"error": f"Failed to check first run status: {str(e)}"}), 500

    if action == "run":
        logger.info(f"Action run for card: {card}, scheduler_running={scheduler.running}")
        if scheduler.running and card == "Ch·ªâ ƒë·ªãnh":
            scheduler.pause()
            running_state["current_running"] = None
            running_state["files"] = []
        if card == "L·∫ßn ƒë·∫ßu":
            if not days:
                logger.error("Days required for L·∫ßn ƒë·∫ßu")
                return jsonify({"error": "Days required for L·∫ßn ƒë·∫ßu"}), 400
            running_state["days"] = days
            running_state["custom_path"] = None
            try:
                run_file_scan(scan_action="first", days=days)
            except Exception as e:
                logger.error(f"Failed to run first scan: {str(e)}")
                return jsonify({"error": f"Failed to run first scan: {str(e)}"}), 500
        elif card == "Ch·ªâ ƒë·ªãnh":
            if not custom_path:
                logger.error("Custom path required for Ch·ªâ ƒë·ªãnh")
                return jsonify({"error": "Custom path required cho Ch·ªâ ƒë·ªãnh"}), 400
            abs_path = os.path.abspath(custom_path)
            if not os.path.exists(abs_path):
                logger.error(f"Custom path {abs_path} does not exist")
                return jsonify({"error": f"Custom path {abs_path} does not exist"}), 400
            try:
                with db_rwlock:
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("SELECT status, is_processed FROM file_list WHERE file_path = ? AND (status = 'xong' OR is_processed = 1)", (abs_path,))
                    result = cursor.fetchone()
                    conn.close()
                    if result:
                        logger.warning(f"File {abs_path} already processed with status {result[0]}")
                        return jsonify({"error": "File ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω"}), 400
            except Exception as e:
                logger.error(f"Error checking file status: {str(e)}")
                return jsonify({"error": f"Error checking file status: {str(e)}"}), 500
            running_state["custom_path"] = abs_path
            running_state["days"] = None
            try:
                scheduler.pause()
                run_file_scan(scan_action="custom", custom_path=abs_path)
                with db_rwlock:
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("SELECT file_path FROM file_list WHERE custom_path = ? AND status = 'pending' ORDER BY created_at DESC LIMIT 1", (abs_path,))
                    result = cursor.fetchone()
                    conn.close()
                if result:
                    from .program_runner import start_frame_sampler_thread, start_event_detector_thread
                    frame_sampler_event.set()
                    start_frame_sampler_thread()
                    start_event_detector_thread()
                    logger.info(f"[Ch·ªâ ƒë·ªãnh] Processing started: {result[0]}")
                    import time
                    while True:
                        with db_rwlock:
                            conn = get_db_connection()
                            cursor = conn.cursor()
                            cursor.execute("SELECT status FROM file_list WHERE file_path = ?", (result[0],))
                            status_result = cursor.fetchone()
                            conn.close()
                        if status_result and status_result[0] == 'xong':
                            break
                        time.sleep(2)
                    logger.info(f"[Ch·ªâ ƒë·ªãnh] Processing completed: {result[0]}")
                    scheduler.resume()
                    try:
                        if not scheduler.running:
                            scheduler.start()
                            logger.info("Restarted scheduler for default mode")
                        run_file_scan(scan_action="default")
                        frame_sampler_event.set()
                        event_detector_event.set()
                        logger.info(f"Completed Ch·ªâ ƒë·ªãnh, transitioning to default: scheduler_running={scheduler.running}, running_state={running_state}")
                    except Exception as e:
                        logger.error(f"Error triggering default scan: {str(e)}")
                else:
                    logger.error(f"[Ch·ªâ ƒë·ªãnh] No pending file found at: {abs_path}")
                    return jsonify({"error": "Kh√¥ng t√¨m th·∫•y file pending ƒë·ªÉ x·ª≠ l√Ω"}), 404
            except Exception as e:
                logger.error(f"[Ch·ªâ ƒë·ªãnh] Error: {str(e)}")
                scheduler.resume()
                return jsonify({"error": f"X·ª≠ l√Ω ch·ªâ ƒë·ªãnh th·∫•t b·∫°i: {str(e)}"}), 500
        else:
            running_state["days"] = None
            running_state["custom_path"] = None

        running_state["current_running"] = card
        if not scheduler.running:
            running_state["current_running"] = "M·∫∑c ƒë·ªãnh"
            scheduler.start()

        if card == "L·∫ßn ƒë·∫ßu":
            try:
                with db_rwlock:
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("UPDATE program_status SET value = ? WHERE key = ?", ("true", "first_run_completed"))
                    conn.commit()
                    conn.close()
                logger.info("Chuy·ªÉn sang ch·∫ø ƒë·ªô ch·∫°y m·∫∑c ƒë·ªãnh (qu√©t l·∫∑p) sau khi ho√†n th√†nh L·∫ßn ƒë·∫ßu")
            except Exception as e:
                logger.error(f"Error updating first_run_completed: {e}")

    elif action == "stop":
        logger.info(f"Action stop called, current_state={running_state}, scheduler_running={scheduler.running}")
        running_state["current_running"] = None
        running_state["days"] = None
        running_state["custom_path"] = None
        running_state["files"] = []
        if not scheduler.running:
            scheduler.start()
            logger.info("Scheduler restarted for default mode")
        logger.info(f"State after stop: running_state={running_state}, scheduler_running={scheduler.running}")

    logger.info(f"Program action completed: {card} {action}, final_state={running_state}, scheduler_running={scheduler.running}")
    return jsonify({
        "current_running": running_state["current_running"],
        "days": running_state.get("days"),
        "custom_path": running_state.get("custom_path")
    }), 200

@program_bp.route('/program', methods=['GET'])
def get_program_status():
    logger.info("GET /program called")
    return jsonify({
        "current_running": running_state["current_running"],
        "days": running_state.get("days"),
        "custom_path": running_state.get("custom_path")
    }), 200

@program_bp.route('/confirm-run', methods=['POST'])
def confirm_run():
    logger.info("POST /confirm-run called")
    data = request.json
    card = data.get('card')

    scan_action = "first" if card == "L·∫ßn ƒë·∫ßu" else "default" if card == "M·∫∑c ƒë·ªãnh" else "custom"
    days = running_state.get("days") if card == "L·∫ßn ƒë·∫ßu" else None
    custom_path = running_state.get("custom_path", '')
    try:
        run_file_scan(scan_action=scan_action, days=days, custom_path=custom_path)
        logger.info(f"Files queued for {scan_action}")
    except Exception as e:
        logger.error(f"Failed to list files: {str(e)}")
        return jsonify({"error": f"Failed to list files: {str(e)}"}), 500

    return jsonify({
        "program_type": scan_action
    }), 200

@program_bp.route('/program-progress', methods=['GET'])
def get_program_progress():
    logger.info("GET /program-progress called")
    try:
        with db_rwlock:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT file_path, status FROM file_list WHERE is_processed = 0 ORDER BY created_at DESC")
            files_status = [{"file": row[0], "status": row[1]} for row in cursor.fetchall()]
            conn.close()
        logger.info(f"Retrieved {len(files_status)} files for status")
        return jsonify({"files": files_status}), 200
    except Exception as e:
        logger.error(f"Failed to retrieve program progress: {str(e)}")
        return jsonify({"error": f"Failed to retrieve program progress: {str(e)}"}), 500

@program_bp.route('/check-first-run', methods=['GET'])
def check_first_run():
    logger.info("GET /check-first-run called")
    try:
        with db_rwlock:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('SELECT value FROM program_status WHERE key = "first_run_completed"')
            result = cursor.fetchone()
            conn.close()
            first_run_completed = result[0] == 'true' if result else False
        logger.info(f"First run completed: {first_run_completed}")
        return jsonify({"first_run_completed": first_run_completed}), 200
    except Exception as e:
        logger.error(f"Failed to check first run status: {str(e)}")
        return jsonify({"error": f"Failed to check first run status: {str(e)}"}), 500

@program_bp.route('/get-cameras', methods=['GET'])
def get_cameras():
    logger.info("GET /get-cameras called")
    try:
        with db_rwlock:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT selected_cameras FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            conn.close()
            cameras = result[0] if result else "[]"
            cameras_list = json.loads(cameras) if cameras else []
        logger.info(f"Retrieved {len(cameras_list)} cameras")
        return jsonify({"cameras": cameras_list}), 200
    except Exception as e:
        logger.error(f"Failed to retrieve cameras: {str(e)}")
        return jsonify({"error": f"Failed to retrieve cameras: {str(e)}"}), 500

@program_bp.route('/get-camera-folders', methods=['GET'])
def get_camera_folders():
    logger.info("GET /get-camera-folders called")
    try:
        with db_rwlock:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT input_path FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            video_root = result[0] if result else os.path.join(BASE_DIR, "Inputvideo")
            conn.close()

        if not os.path.exists(video_root):
            logger.error(f"Directory {video_root} does not exist")
            return jsonify({"error": f"Directory {video_root} does not exist. Ensure the path is correct or create the directory."}), 400

        folders = []
        for folder_name in os.listdir(video_root):
            folder_path = os.path.join(video_root, folder_name)
            if os.path.isdir(folder_path):
                folders.append({"name": folder_name, "path": folder_path})
        logger.info(f"Retrieved {len(folders)} camera folders")
        return jsonify({"folders": folders}), 200
    except Exception as e:
        logger.error(f"Failed to retrieve camera folders: {str(e)}")
        return jsonify({"error": f"Failed to retrieve camera folders: {str(e)}"}), 500

if __name__ == "__main__":
    logger.info("Main program started")
    if not scheduler.running:
        running_state["current_running"] = "M·∫∑c ƒë·ªãnh"
        scheduler.start()

```
## üìÑ File: `batch_scheduler.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/scheduler/batch_scheduler.py`

```python
import os
import threading
import time
import logging
import sqlite3
import pytz
import psutil  # TH√äM IMPORT M·ªöI
from datetime import datetime, timedelta
from modules.db_utils import get_db_connection
from .db_sync import db_rwlock, frame_sampler_event, event_detector_event
from .file_lister import run_file_scan
from .program_runner import start_frame_sampler_thread, start_event_detector_thread
import logging

logger = logging.getLogger("app")
logger.info("BatchScheduler logging initialized")

# C·∫•u h√¨nh m√∫i gi·ªù Vi·ªát Nam
VIETNAM_TZ = pytz.timezone('Asia/Ho_Chi_Minh')

class SystemMonitor:
    """Theo d√µi t√†i nguy√™n h·ªá th·ªëng v√† t√≠nh batch_size ƒë·ªông."""
    def __init__(self):
        self.cpu_threshold_low = 70
        self.cpu_threshold_high = 90
        self.base_batch_size = 2
        self.max_batch_size = 6

    def get_cpu_usage(self):
        """L·∫•y CPU usage th·ª±c t·∫ø t·ª´ h·ªá th·ªëng"""
        try:
            # L·∫•y CPU usage trung b√¨nh trong 1 gi√¢y
            cpu_percent = psutil.cpu_percent(interval=1)
            logger.debug(f"Current CPU usage: {cpu_percent}%")
            return cpu_percent
        except Exception as e:
            logger.error(f"Error getting CPU usage: {e}")
            return 50  # Fallback value n·∫øu c√≥ l·ªói

    def get_memory_usage(self):
        """L·∫•y memory usage th·ª±c t·∫ø"""
        try:
            memory_percent = psutil.virtual_memory().percent
            logger.debug(f"Current memory usage: {memory_percent}%")
            return memory_percent
        except Exception as e:
            logger.error(f"Error getting memory usage: {e}")
            return 50  # Fallback value

    def get_batch_size(self, current_batch_size):
        cpu_usage = self.get_cpu_usage()
        memory_usage = self.get_memory_usage()
        
        logger.debug(f"Resource usage - CPU: {cpu_usage}%, Memory: {memory_usage}%")
        
        # Ki·ªÉm tra n·∫øu t√†i nguy√™n qu√° t·∫£i
        if cpu_usage > self.cpu_threshold_high or memory_usage > 85:
            if current_batch_size > self.base_batch_size:
                new_batch_size = current_batch_size - 1
                logger.warning(f"High resource usage (CPU: {cpu_usage}%, RAM: {memory_usage}%), reducing batch size: {current_batch_size} -> {new_batch_size}")
                return new_batch_size
        
        # Ki·ªÉm tra n·∫øu t√†i nguy√™n nh√†n r·ªói
        elif cpu_usage < self.cpu_threshold_low and memory_usage < 70:
            if current_batch_size < self.max_batch_size:
                new_batch_size = current_batch_size + 1
                logger.info(f"Low resource usage (CPU: {cpu_usage}%, RAM: {memory_usage}%), increasing batch size: {current_batch_size} -> {new_batch_size}")
                return new_batch_size
        
        # Gi·ªØ nguy√™n n·∫øu t√†i nguy√™n ·ªïn ƒë·ªãnh
        return current_batch_size

    def log_system_info(self):
        """Log th√¥ng tin h·ªá th·ªëng khi kh·ªüi ƒë·ªông"""
        try:
            cpu_count = psutil.cpu_count()
            memory_total = psutil.virtual_memory().total / (1024**3)  # GB
            logger.info(f"System info - CPU cores: {cpu_count}, Total RAM: {memory_total:.1f}GB")
            logger.info(f"Batch size config - Base: {self.base_batch_size}, Max: {self.max_batch_size}")
            logger.info(f"CPU thresholds - Low: {self.cpu_threshold_low}%, High: {self.cpu_threshold_high}%")
        except Exception as e:
            logger.error(f"Error logging system info: {e}")

class BatchScheduler:
    def __init__(self):
        self.batch_size = 2
        self.sys_monitor = SystemMonitor()
        self.scan_interval = 60
        self.timeout_seconds = 3600
        self.running = False
        self.queue_limit = 5
        self.sampler_threads = []
        self.detector_thread = None
        self.pause_event = threading.Event()
        self.pause_event.set()

    def pause(self):
        logger.info(f"BatchScheduler paused, current_batch_size={self.batch_size}")
        self.pause_event.clear()

    def resume(self):
        logger.info(f"BatchScheduler resumed, current_batch_size={self.batch_size}")
        self.pause_event.set()

    def get_pending_files(self):
        """L·∫•y danh s√°ch file ch∆∞a x·ª≠ l√Ω, gi·ªõi h·∫°n queue_limit."""
        try:
            with db_rwlock.gen_rlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("SELECT file_path, camera_name FROM file_list WHERE status = 'pending' AND is_processed = 0 ORDER BY priority DESC, created_at ASC LIMIT ?", 
                             (self.queue_limit,))
                files = [(row[0], row[1]) for row in cursor.fetchall()]
                conn.close()
            return files
        except Exception as e:
            logger.error(f"Error retrieving pending files: {e}")
            return []

    def update_file_status(self, file_path, status):
        """C·∫≠p nh·∫≠t tr·∫°ng th√°i file trong file_list."""
        try:
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("UPDATE file_list SET status = ?, is_processed = ? WHERE file_path = ?",
                             (status, 1 if status in ['xong', 'l·ªói', 'timeout'] else 0, file_path))
                conn.commit()
                conn.close()
        except Exception as e:
            logger.error(f"Error updating file status for {file_path}: {e}")

    def check_timeout(self):
        """Ki·ªÉm tra v√† c·∫≠p nh·∫≠t tr·∫°ng th√°i timeout cho file qu√° 900s."""
        try:
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("SELECT file_path, created_at FROM file_list WHERE status = 'ƒëang frame sampler ...'")
                for row in cursor.fetchall():
                    created_at = datetime.fromisoformat(row[1].replace('Z', '+00:00')) if row[1] else datetime.min.replace(tzinfo=VIETNAM_TZ)
                    if (datetime.now(VIETNAM_TZ) - created_at).total_seconds() > self.timeout_seconds:
                        cursor.execute("UPDATE file_list SET status = ?, is_processed = 1 WHERE file_path = ?", 
                                     ('timeout', row[0]))
                        logger.warning(f"Timeout processing {row[0]} after {self.timeout_seconds}s")
                conn.commit()
                conn.close()
        except Exception as e:
            logger.error(f"Error checking timeout: {e}")

    def scan_files(self):
        """Qu√©t file m·ªõi ƒë·ªãnh k·ª≥ (15 ph√∫t)."""
        logger.info("B·∫Øt ƒë·∫ßu qu√©t l·∫∑p")
        while self.running:
            try:
                logger.debug("Ki·ªÉm tra qu√©t l·∫∑p, running=%s, paused=%s", self.running, not self.pause_event.is_set())
                self.pause_event.wait()
                with db_rwlock.gen_rlock():
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("SELECT COUNT(*) FROM file_list WHERE status = 'pending' AND is_processed = 0")
                    pending_count = cursor.fetchone()[0]
                    conn.close()

                if pending_count >= self.queue_limit:
                    logger.warning(f"Queue full ({pending_count}/{self.queue_limit}), skipping file scan")
                else:
                    run_file_scan("default")
                    frame_sampler_event.set()
                time.sleep(self.scan_interval)
            except Exception as e:
                logger.error(f"Error in file scan: {e}")

    def run_batch(self):
        """Ch·∫°y batch x·ª≠ l√Ω file, s·ª≠ d·ª•ng run_frame_sampler."""
        while self.running:
            try:
                self.pause_event.wait()
                self.batch_size = self.sys_monitor.get_batch_size(self.batch_size)

                if not self.sampler_threads or len(self.sampler_threads) != self.batch_size:
                    for thread in self.sampler_threads:
                        if thread.is_alive():
                            thread.join(timeout=1)
                    self.sampler_threads = start_frame_sampler_thread(self.batch_size)
                    logger.info(f"Started {self.batch_size} frame sampler threads")

                if not self.detector_thread or not self.detector_thread.is_alive():
                    self.detector_thread = start_event_detector_thread()
                    logger.info("Started event detector thread")

                self.check_timeout()

                files = self.get_pending_files()
                if not files:
                    frame_sampler_event.clear()
                    frame_sampler_event.wait()
                    continue

                frame_sampler_event.set()
                event_detector_event.set()
                time.sleep(60)
            except Exception as e:
                logger.error(f"Error in batch processing: {e}")

    def start(self):
        """Kh·ªüi ƒë·ªông BatchScheduler."""
        if not self.running:
            # Log system info khi kh·ªüi ƒë·ªông
            self.sys_monitor.log_system_info()
            
            self.running = True
            days = [(datetime.now(VIETNAM_TZ) - timedelta(days=i)).date().isoformat() for i in range(6, -1, -1)]
            logger.info(f"Initial scan for days: {days}")
            try:
                run_file_scan("default", days=days)
                frame_sampler_event.set()
            except Exception as e:
                logger.error(f"Initial scan failed: {e}")

            scan_thread = threading.Thread(target=self.scan_files)
            batch_thread = threading.Thread(target=self.run_batch)
            scan_thread.start()
            batch_thread.start()
            logger.info(f"BatchScheduler started, batch_size={self.batch_size}, scan_interval={self.scan_interval}")

    def stop(self):
        """D·ª´ng BatchScheduler m·ªôt c√°ch an to√†n."""
        if not self.running:
            return  # ƒê√£ d·ª´ng r·ªìi
            
        logger.info("Stopping BatchScheduler...")
        self.running = False
        
        # Clear events ƒë·ªÉ c√°c thread c√≥ th·ªÉ tho√°t
        frame_sampler_event.clear()
        event_detector_event.clear()
        
        # ƒê·∫∑t pause_event ƒë·ªÉ c√°c thread kh√¥ng b·ªã block
        self.pause_event.set()
        
        # D·ª´ng sampler threads v·ªõi timeout ng·∫Øn
        for i, thread in enumerate(self.sampler_threads):
            if thread and thread.is_alive():
                try:
                    thread.join(timeout=0.5)  # Timeout ng·∫Øn
                    if thread.is_alive():
                        logger.warning(f"Sampler thread {i} did not stop gracefully")
                except Exception as e:
                    logger.warning(f"Error stopping sampler thread {i}: {e}")
        
        # D·ª´ng detector thread
        if self.detector_thread and self.detector_thread.is_alive():
            try:
                self.detector_thread.join(timeout=0.5)
                if self.detector_thread.is_alive():
                    logger.warning("Detector thread did not stop gracefully")
            except Exception as e:
                logger.warning(f"Error stopping detector thread: {e}")
        
        logger.info("BatchScheduler stopped")
```
## üìÑ File: `file_parser.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/utils/file_parser.py`

```python
import pandas as pd
import base64
import os
from io import BytesIO
import logging
import csv

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

def force_split_excel_text(df: pd.DataFrame) -> pd.DataFrame:
    if df.shape[1] == 1 and isinstance(df.columns[0], str) and "\t" in df.columns[0]:
        raw = df.to_csv(index=False, header=False)
        reader = csv.reader(raw.splitlines(), delimiter='\t')
        rows = list(reader)
        df_fixed = pd.DataFrame(rows[1:], columns=rows[0])
        logger.debug(f"[DEBUG] After force_split_excel_text - df.columns: {df_fixed.columns.tolist()}")
        logger.debug(f"[DEBUG] After force_split_excel_text - df.shape: {df_fixed.shape}")
        return df_fixed
    return df

def parse_uploaded_file(file_content: str = None, file_path: str = None, is_excel: bool = False) -> pd.DataFrame:
    """
    Decode base64-encoded file content or read from file path and return a pandas DataFrame.
    Supports both CSV and Excel formats.
    Raises detailed exceptions instead of falling back silently.
    """
    logger.debug(f"parse_uploaded_file called with is_excel={is_excel}, file_content={'provided' if file_content else 'not provided'}, file_path={file_path}")

    if not file_content and not file_path:
        raise ValueError("Either file_content or file_path must be provided.")

    if file_content:
        try:
            file_bytes = base64.b64decode(file_content)
        except Exception as e:
            raise ValueError(f"Failed to decode base64 content: {e}")
        buffer = BytesIO(file_bytes)
    elif file_path:
        if not os.path.exists(file_path):
            raise ValueError(f"File not found at path: {file_path}")
        buffer = file_path  # pandas can read directly from file path

    try:
        if is_excel:
            logger.debug("Reading file as Excel (pd.read_excel)")
            try:
                df = pd.read_excel(buffer, header=None, engine='openpyxl')  # X√≥a encoding
            except Exception as e:
                logger.debug(f"Failed to read Excel with pd.read_excel: {str(e)}")
                logger.debug("Falling back to pd.read_csv")
                buffer.seek(0)  # Reset con tr·ªè file ƒë·ªÉ ƒë·ªçc l·∫°i
                df = pd.read_csv(buffer, sep=",", encoding='utf-8-sig', engine='python')
            logger.debug(f"[DEBUG] Raw DataFrame before processing: {df.to_dict()}")
            logger.debug(f"[DEBUG] Raw first row (potential header): {df.iloc[0].tolist()}")
            if df.shape[0] > 1:
                df.columns = df.iloc[0].values.tolist()
                df = df[1:]
                df = force_split_excel_text(df)  # Fix n·∫øu d√≠nh l·ªói tab
            else:
                raise ValueError("Excel file missing header/data")
            logger.debug(f"[DEBUG] df.columns: {df.columns.tolist()}")
            logger.debug(f"[DEBUG] df.shape: {df.shape}")
            logger.debug(f"[DEBUG] df.head(2): {df.head(2).to_dict()}")
            return df
        else:
            logger.debug("Reading file as CSV (pd.read_csv)")
            try:
                df = pd.read_csv(buffer, sep=",", encoding='latin1', engine='python')  # Th·ª≠ d·∫•u ph·∫©y tr∆∞·ªõc
                # Ki·ªÉm tra n·∫øu ch·ªâ c√≥ 1 c·ªôt v√† c·ªôt ƒë√≥ ch·ª©a d·∫•u ;
                if len(df.columns) == 1 and df.columns[0] and isinstance(df.columns[0], str) and ";" in df.columns[0]:
                    logger.debug("Detected single column with semicolon, retrying with sep=';'")
                    if file_content:
                        buffer.seek(0)  # Reset con tr·ªè file ƒë·ªÉ ƒë·ªçc l·∫°i n·∫øu d√πng file_content
                    df = pd.read_csv(buffer, sep=";", encoding='latin1', engine='python')
                return df
            except pd.errors.ParserError:
                if file_content:
                    buffer.seek(0)  # Reset con tr·ªè file ƒë·ªÉ ƒë·ªçc l·∫°i n·∫øu d√πng file_content
                return pd.read_csv(buffer, sep=";", encoding='latin1', engine='python')  # N·∫øu l·ªói, th·ª≠ d·∫•u ch·∫•m ph·∫©y
    except Exception as e:
        file_type = 'Excel' if is_excel else 'CSV'
        raise ValueError(f"Failed to read {file_type} file: {e}")
```
## üìÑ File: `__init__.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/utils/__init__.py`

```python

```
## üìÑ File: `path_validator.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/utils/path_validator.py`

```python
# backend/modules/utils/path_validator.py
import os
import shutil
import json
import logging
from typing import Dict, List, Tuple
from pathlib import Path
import stat

logger = logging.getLogger(__name__)

class PathValidator:
    """
    Path validation and directory management for VTrack video sources
    Handles directory creation, permission checks, and disk space validation
    """
    
    def __init__(self, base_path: str = None):
        """
        Initialize PathValidator with base path for video storage
        
        Args:
            base_path: Base directory for all video sources (default: project root)
        """
        if base_path is None:
            # Default to project root + storage directories
            from modules.db_utils import find_project_root
            project_root = find_project_root(os.path.abspath(__file__))
            base_path = project_root
        
        self.base_path = Path(base_path)
        self.logger = logging.getLogger(__name__)
        
        # Default storage directories
        self.nvr_downloads_dir = self.base_path / "nvr_downloads"
        self.cloud_sync_dir = self.base_path / "cloud_sync"
        self.output_clips_dir = self.base_path / "output_clips"
        
        self.logger.info(f"PathValidator initialized with base_path: {self.base_path}")
    
    def validate_source_path(self, source_type: str, source_name: str) -> Dict:
        """
        Validate and prepare working path for a video source
        
        Args:
            source_type: Type of source ('nvr', 'local', 'cloud')
            source_name: Unique name for the source
            
        Returns:
            Dict with validation results and working path
        """
        try:
            self.logger.info(f"üîç Validating source path: {source_type}/{source_name}")
            
            # Determine working directory based on source type
            if source_type == 'nvr':
                working_path = self.nvr_downloads_dir / source_name
            elif source_type == 'cloud':
                working_path = self.cloud_sync_dir / source_name
            elif source_type == 'local':
                # Local sources use their own paths, no validation needed
                return {
                    'success': True,
                    'working_path': None,
                    'message': 'Local source uses existing path',
                    'disk_space_gb': self._get_disk_space_gb(str(self.base_path)),
                    'permissions': 'read-only'
                }
            else:
                return {
                    'success': False,
                    'message': f'Unknown source type: {source_type}',
                    'working_path': None
                }
            
            # Create working directory if it doesn't exist
            working_path.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"‚úÖ Working directory ready: {working_path}")
            
            # Validate permissions and disk space
            permissions_check = self.check_permissions(str(working_path))
            disk_check = self.check_disk_space(str(working_path), required_gb=1.0)
            
            if not permissions_check['writable']:
                return {
                    'success': False,
                    'message': f'Directory not writable: {working_path}',
                    'working_path': str(working_path),
                    'permissions': permissions_check
                }
            
            if not disk_check['sufficient']:
                return {
                    'success': False,
                    'message': f'Insufficient disk space: {disk_check["available_gb"]:.1f} GB available',
                    'working_path': str(working_path),
                    'disk_space': disk_check
                }
            
            return {
                'success': True,
                'working_path': str(working_path),
                'message': f'Source path validated successfully',
                'disk_space_gb': disk_check['available_gb'],
                'permissions': 'read-write',
                'created_directories': [str(working_path)]
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Path validation failed: {e}")
            return {
                'success': False,
                'message': f'Path validation error: {str(e)}',
                'working_path': None
            }
    
    def create_camera_directories(self, source_path: str, camera_names: List[str]) -> Dict:
        """
        Create individual directories for each camera under source path
        
        Args:
            source_path: Working directory for the source
            camera_names: List of camera names to create directories for
            
        Returns:
            Dict with created directories and camera path mapping
        """
        try:
            self.logger.info(f"üìÅ Creating camera directories in: {source_path}")
            self.logger.info(f"üìπ Cameras: {camera_names}")
            
            if not camera_names:
                return {
                    'success': True,
                    'camera_paths': {},
                    'created_directories': [],
                    'message': 'No cameras to create directories for'
                }
            
            source_dir = Path(source_path)
            if not source_dir.exists():
                source_dir.mkdir(parents=True, exist_ok=True)
                self.logger.info(f"‚úÖ Created source directory: {source_dir}")
            
            camera_paths = {}
            created_directories = []
            
            for camera_name in camera_names:
                # Sanitize camera name for file system
                safe_name = self._sanitize_directory_name(camera_name)
                camera_dir = source_dir / safe_name
                
                # Create camera directory
                camera_dir.mkdir(parents=True, exist_ok=True)
                
                # Store mapping
                camera_paths[camera_name] = str(camera_dir)
                created_directories.append(str(camera_dir))
                
                self.logger.info(f"‚úÖ Created camera directory: {camera_name} ‚Üí {camera_dir}")
            
            return {
                'success': True,
                'camera_paths': camera_paths,
                'created_directories': created_directories,
                'message': f'Created {len(camera_paths)} camera directories',
                'total_cameras': len(camera_names)
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Camera directory creation failed: {e}")
            return {
                'success': False,
                'message': f'Camera directory creation error: {str(e)}',
                'camera_paths': {},
                'created_directories': []
            }
    
    def check_disk_space(self, path: str, required_gb: float = 1.0) -> Dict:
        """
        Check available disk space at given path
        
        Args:
            path: Path to check disk space for
            required_gb: Minimum required space in GB
            
        Returns:
            Dict with disk space information
        """
        try:
            # Get disk usage statistics
            statvfs = shutil.disk_usage(path)
            
            # Convert to GB
            total_gb = statvfs.total / (1024**3)
            used_gb = (statvfs.total - statvfs.free) / (1024**3)
            available_gb = statvfs.free / (1024**3)
            
            usage_percent = (used_gb / total_gb) * 100
            sufficient = available_gb >= required_gb
            
            return {
                'sufficient': sufficient,
                'available_gb': available_gb,
                'used_gb': used_gb,
                'total_gb': total_gb,
                'usage_percent': usage_percent,
                'required_gb': required_gb,
                'path': path
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Disk space check failed: {e}")
            return {
                'sufficient': False,
                'available_gb': 0,
                'error': str(e),
                'path': path
            }
    
    def check_permissions(self, path: str) -> Dict:
        """
        Check read/write permissions for given path
        
        Args:
            path: Path to check permissions for
            
        Returns:
            Dict with permission information
        """
        try:
            path_obj = Path(path)
            
            # Check if path exists
            if not path_obj.exists():
                # Try to create it to test permissions
                try:
                    path_obj.mkdir(parents=True, exist_ok=True)
                    created = True
                except Exception:
                    return {
                        'readable': False,
                        'writable': False,
                        'executable': False,
                        'error': 'Cannot create directory',
                        'path': path
                    }
            else:
                created = False
            
            # Test permissions
            readable = os.access(path, os.R_OK)
            writable = os.access(path, os.W_OK)
            executable = os.access(path, os.X_OK)
            
            # Get file mode
            try:
                stat_info = path_obj.stat()
                file_mode = stat.filemode(stat_info.st_mode)
            except Exception:
                file_mode = 'unknown'
            
            return {
                'readable': readable,
                'writable': writable,
                'executable': executable,
                'file_mode': file_mode,
                'created': created,
                'path': path
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Permission check failed: {e}")
            return {
                'readable': False,
                'writable': False,
                'executable': False,
                'error': str(e),
                'path': path
            }
    
    def get_camera_paths(self, source_path: str, camera_names: List[str]) -> Dict:
        """
        Get mapping of camera names to their directory paths
        
        Args:
            source_path: Working directory for the source
            camera_names: List of camera names
            
        Returns:
            Dict mapping camera names to directory paths
        """
        try:
            source_dir = Path(source_path)
            camera_paths = {}
            
            for camera_name in camera_names:
                safe_name = self._sanitize_directory_name(camera_name)
                camera_dir = source_dir / safe_name
                camera_paths[camera_name] = str(camera_dir)
            
            return {
                'success': True,
                'camera_paths': camera_paths,
                'source_path': source_path
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Get camera paths failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'camera_paths': {}
            }
    
    def cleanup_unused_directories(self, source_path: str, active_cameras: List[str]) -> Dict:
        """
        Remove directories for cameras that are no longer active
        
        Args:
            source_path: Working directory for the source
            active_cameras: List of currently active camera names
            
        Returns:
            Dict with cleanup results
        """
        try:
            self.logger.info(f"üßπ Cleaning up unused directories in: {source_path}")
            
            source_dir = Path(source_path)
            if not source_dir.exists():
                return {
                    'success': True,
                    'removed_directories': [],
                    'message': 'Source directory does not exist'
                }
            
            # Get all existing directories
            existing_dirs = [d for d in source_dir.iterdir() if d.is_dir()]
            
            # Sanitize active camera names
            active_safe_names = [self._sanitize_directory_name(name) for name in active_cameras]
            
            removed_directories = []
            
            for dir_path in existing_dirs:
                if dir_path.name not in active_safe_names:
                    try:
                        shutil.rmtree(dir_path)
                        removed_directories.append(str(dir_path))
                        self.logger.info(f"üóëÔ∏è Removed unused directory: {dir_path}")
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è Failed to remove directory {dir_path}: {e}")
            
            return {
                'success': True,
                'removed_directories': removed_directories,
                'message': f'Cleaned up {len(removed_directories)} unused directories',
                'active_cameras': active_cameras,
                'remaining_directories': len(existing_dirs) - len(removed_directories)
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Directory cleanup failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'removed_directories': []
            }
    
    def get_directory_health_status(self, path: str) -> Dict:
        """
        Get comprehensive health status of a directory
        
        Args:
            path: Directory path to check
            
        Returns:
            Dict with health status information
        """
        try:
            path_obj = Path(path)
            
            if not path_obj.exists():
                return {
                    'healthy': False,
                    'exists': False,
                    'message': 'Directory does not exist',
                    'path': path
                }
            
            # Check permissions
            permissions = self.check_permissions(path)
            
            # Check disk space
            disk_space = self.check_disk_space(path, required_gb=0.5)
            
            # Count files and subdirectories
            try:
                files_count = len([f for f in path_obj.rglob('*') if f.is_file()])
                dirs_count = len([d for d in path_obj.rglob('*') if d.is_dir()])
            except Exception:
                files_count = 0
                dirs_count = 0
            
            # Calculate directory size
            try:
                total_size = sum(f.stat().st_size for f in path_obj.rglob('*') if f.is_file())
                size_mb = total_size / (1024**2)
            except Exception:
                size_mb = 0
            
            # Determine overall health
            healthy = (
                permissions.get('readable', False) and
                permissions.get('writable', False) and
                disk_space.get('sufficient', False)
            )
            
            return {
                'healthy': healthy,
                'exists': True,
                'permissions': permissions,
                'disk_space': disk_space,
                'files_count': files_count,
                'directories_count': dirs_count,
                'size_mb': size_mb,
                'path': path,
                'message': 'Directory health check complete'
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Directory health check failed: {e}")
            return {
                'healthy': False,
                'error': str(e),
                'path': path
            }
    
    def _sanitize_directory_name(self, name: str) -> str:
        """
        Sanitize camera name to be safe for file system directory names
        
        Args:
            name: Original camera name
            
        Returns:
            Sanitized directory name
        """
        # Replace problematic characters
        import re
        
        # Replace spaces and special chars with underscores
        sanitized = re.sub(r'[<>:"/\\|?*]', '_', name)
        sanitized = re.sub(r'\s+', '_', sanitized)
        
        # Remove leading/trailing underscores and dots
        sanitized = sanitized.strip('_.')
        
        # Ensure not empty
        if not sanitized:
            sanitized = 'camera'
        
        # Limit length to 50 characters
        if len(sanitized) > 50:
            sanitized = sanitized[:50].rstrip('_')
        
        return sanitized
    
    def _get_disk_space_gb(self, path: str) -> float:
        """
        Helper method to get available disk space in GB
        
        Args:
            path: Path to check
            
        Returns:
            Available space in GB
        """
        try:
            statvfs = shutil.disk_usage(path)
            return statvfs.free / (1024**3)
        except Exception:
            return 0.0
    
    def get_base_directories(self) -> Dict:
        """
        Get all base directories managed by PathValidator
        
        Returns:
            Dict with base directory paths and their status
        """
        directories = {
            'base_path': str(self.base_path),
            'nvr_downloads': str(self.nvr_downloads_dir),
            'cloud_sync': str(self.cloud_sync_dir),
            'output_clips': str(self.output_clips_dir)
        }
        
        status = {}
        for name, path in directories.items():
            try:
                Path(path).mkdir(parents=True, exist_ok=True)
                status[name] = {
                    'path': path,
                    'exists': True,
                    'writable': os.access(path, os.W_OK)
                }
            except Exception as e:
                status[name] = {
                    'path': path,
                    'exists': False,
                    'error': str(e)
                }
        
        return {
            'directories': directories,
            'status': status,
            'all_healthy': all(s.get('exists', False) and s.get('writable', False) 
                             for s in status.values())
        }

# Global instance for easy import
path_validator = PathValidator()
```
## üìÑ File: `LicenseGuard.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/system/LicenseGuard.py`

```python
# LicenseGuard.py - Module ki·ªÉm tra b·∫£n quy·ªÅn (QR watermark, MAC address)

```
## üìÑ File: `SystemMonitor.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/system/SystemMonitor.py`

```python
# SystemMonitor.py - Module theo d√µi hi·ªáu su·∫•t v√† t√†i nguy√™n h·ªá th·ªëng

```
## üìÑ File: `SystemCalendar.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/system/SystemCalendar.py`

```python
# SystemCalendar.py - Module qu·∫£n l√Ω l·ªãch l√†m vi·ªác (ng√†y ngh·ªâ, ca l√†m vi·ªác)

```
## üìÑ File: `AuditLogger.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/system/AuditLogger.py`

```python
# AuditLogger.py - Module ghi log h·ªá th·ªëng (l·ªãch s·ª≠ x·ª≠ l√Ω, l·ªói, ng∆∞·ªùi d√πng)

```
## üìÑ File: `pydrive_downloader.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/pydrive_downloader.py`

```python
#!/usr/bin/env python3
"""
PyDrive2 Downloader for VTrack - Simple replacement for AutoSyncService
Integrates with existing OAuth credentials and database structure
"""

import os
import json
import logging
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Optional

from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive
from google.oauth2.credentials import Credentials

from modules.db_utils import get_db_connection
from database import get_sync_status, initialize_sync_status

logger = logging.getLogger(__name__)

class PyDriveDownloader:
    """Simple PyDrive2-based downloader that works with existing VTrack infrastructure"""
    
    def __init__(self):
        self.sync_timers = {}  # Store timers for each source
        self.sync_locks = {}   # Locks to prevent concurrent syncs
        self.drive_clients = {} # Cache PyDrive clients
        
        logger.info("üöÄ PyDriveDownloader initialized")
    
    def start_auto_sync(self, source_id: int) -> bool:
        """Start auto-sync for a cloud source"""
        try:
            if source_id in self.sync_timers:
                logger.warning(f"Sync already running for source {source_id}")
                return True
            
            # Get source config from database
            source_config = self._get_source_config(source_id)
            if not source_config:
                logger.error(f"Source {source_id} not found")
                return False
            
            if source_config['source_type'] != 'cloud':
                logger.error(f"Source {source_id} is not a cloud source")
                return False
            
            # Initialize sync status if not exists
            current_status = get_sync_status(source_id)
            if not current_status:
                initialize_sync_status(source_id, sync_enabled=True, interval_minutes=15)
            
            # Create sync lock
            self.sync_locks[source_id] = threading.Lock()
            
            # Schedule first sync
            self._schedule_next_sync(source_id)
            
            logger.info(f"‚úÖ Auto-sync started for source {source_id}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to start auto-sync for {source_id}: {e}")
            return False
    
    def stop_auto_sync(self, source_id: int) -> bool:
        """Stop auto-sync for a source"""
        try:
            if source_id not in self.sync_timers:
                logger.warning(f"No active sync for source {source_id}")
                return True
            
            # Cancel timer
            self.sync_timers[source_id].cancel()
            del self.sync_timers[source_id]
            del self.sync_locks[source_id]
            
            # Update database status
            self._update_sync_status(source_id, 'stopped', 'Auto-sync stopped by user')
            
            logger.info(f"‚úÖ Auto-sync stopped for source {source_id}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error stopping sync for {source_id}: {e}")
            return False
    
    def force_sync_now(self, source_id: int) -> Dict:
        """Force immediate sync for a source"""
        logger.info(f"üöÄ Force sync requested for source {source_id}")
        return self._perform_sync(source_id)
    
    def _perform_sync(self, source_id: int) -> Dict:
        """Perform actual sync operation"""
        with self.sync_locks.get(source_id, threading.Lock()):
            try:
                logger.info(f"üîÑ Starting sync for source {source_id}")
                
                # Update status to in_progress
                self._update_sync_status(source_id, 'in_progress', 'Sync started')
                
                # Get source configuration
                source_config = self._get_source_config(source_id)
                if not source_config:
                    return {'success': False, 'message': 'Source not found'}
                
                # Get PyDrive client
                drive = self._get_drive_client(source_id)
                if not drive:
                    return {'success': False, 'message': 'Failed to authenticate with Google Drive'}
                
                # Download files
                download_result = self._download_files(source_id, source_config, drive)
                
                # Update sync status
                status = 'success' if download_result['success'] else 'failed'
                self._update_sync_status(
                    source_id, 
                    status, 
                    download_result['message'],
                    download_result.get('files_downloaded', 0),
                    download_result.get('total_size_mb', 0.0)
                )
                
                logger.info(f"‚úÖ Sync completed for source {source_id}: {download_result['message']}")
                return download_result
                
            except Exception as e:
                error_msg = f"Sync failed: {str(e)}"
                logger.error(f"‚ùå {error_msg}")
                self._update_sync_status(source_id, 'failed', error_msg)
                return {'success': False, 'message': error_msg}
    
    def _download_files(self, source_id: int, source_config: Dict, drive: GoogleDrive) -> Dict:
        """Download files from Google Drive to local storage"""
        try:
            # Get target directory
            source_name = source_config['name']
            base_path = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'cloud_sync', source_name)
            os.makedirs(base_path, exist_ok=True)
            
            # Get selected folders from config
            config_data = json.loads(source_config.get('config', '{}'))
            selected_folders = config_data.get('selected_folders', [])
            tree_folders = config_data.get('selected_tree_folders', [])
            all_folders = selected_folders + tree_folders
            
            if not all_folders:
                return {'success': False, 'message': 'No folders selected for sync'}
            
            total_files = 0
            total_size = 0
            downloaded_files = []
            
            # Process each selected folder
            for folder_info in all_folders:
                folder_id = folder_info.get('id') if isinstance(folder_info, dict) else folder_info
                folder_name = folder_info.get('name', f'folder_{folder_id}') if isinstance(folder_info, dict) else folder_id
                
                # Create camera subdirectory
                camera_dir = os.path.join(base_path, self._sanitize_filename(folder_name))
                os.makedirs(camera_dir, exist_ok=True)
                
                # List files in this folder
                files_in_folder = self._list_video_files(drive, folder_id)
                
                # Download new files
                for file_info in files_in_folder:
                    if self._should_download_file(source_id, file_info):
                        download_path = os.path.join(camera_dir, self._sanitize_filename(file_info['title']))
                        
                        if self._download_single_file(drive, file_info, download_path):
                            file_size = int(file_info.get('fileSize', 0))
                            total_files += 1
                            total_size += file_size
                            downloaded_files.append({
                                'filename': file_info['title'],
                                'size': file_size,
                                'path': download_path,
                                'camera': folder_name
                            })
                            
                            # Track in database
                            self._track_downloaded_file(source_id, folder_name, file_info, download_path)
            
            total_size_mb = total_size / (1024 * 1024)
            
            return {
                'success': True,
                'message': f'Downloaded {total_files} files ({total_size_mb:.1f} MB)',
                'files_downloaded': total_files,
                'total_size_mb': total_size_mb,
                'downloaded_files': downloaded_files
            }
            
        except Exception as e:
            logger.error(f"‚ùå Download failed: {e}")
            return {'success': False, 'message': f'Download error: {str(e)}'}
    
    def _get_drive_client(self, source_id: int) -> Optional[GoogleDrive]:
        """Get authenticated PyDrive client using existing VTrack credentials (FIXED for PyDrive2)"""
        try:
            if source_id in self.drive_clients:
                return self.drive_clients[source_id]
            
            # Get stored credentials from VTrack database
            credentials = self._get_stored_credentials(source_id)
            if not credentials:
                logger.error(f"No stored credentials found for source {source_id}")
                return None
            
            logger.info(f"üîê Setting up PyDrive2 authentication for source {source_id}")
            
            # Create PyDrive auth with proper credential setup
            gauth = GoogleAuth()
            
            # PyDrive2 compatible credential setup
            gauth.access_token_expired = credentials.expired
            gauth.credentials = credentials
            
            # Set up credential attributes that PyDrive2 expects
            if hasattr(credentials, 'token'):
                gauth.access_token = credentials.token
            if hasattr(credentials, 'refresh_token'):
                gauth.refresh_token = credentials.refresh_token
                
            # Refresh credentials if needed
            if credentials.expired and credentials.refresh_token:
                logger.info("üîÑ Refreshing expired credentials before PyDrive use...")
                from google.auth.transport.requests import Request
                credentials.refresh(Request())
                gauth.access_token = credentials.token
                gauth.access_token_expired = False
            
            # Create drive client
            drive = GoogleDrive(gauth)
            
            # Test the connection with error handling
            try:
                about_info = drive.GetAbout()
                logger.info(f"‚úÖ PyDrive connection test successful")
                logger.info(f"   User: {about_info.get('user', {}).get('emailAddress', 'Unknown')}")
            except Exception as test_error:
                logger.error(f"‚ùå PyDrive connection test failed: {test_error}")
                # Try alternative connection test
                try:
                    # Alternative: List root files (minimal request)
                    file_list = drive.ListFile({'q': "'root' in parents and trashed=false"}).GetList()
                    logger.info(f"‚úÖ Alternative connection test successful ({len(file_list)} items in root)")
                except Exception as alt_error:
                    logger.error(f"‚ùå Alternative connection test also failed: {alt_error}")
                    return None
            
            # Cache the client
            self.drive_clients[source_id] = drive
            logger.info(f"‚úÖ PyDrive client created and cached for source {source_id}")
            return drive
            
        except Exception as e:
            logger.error(f"‚ùå Failed to create drive client: {e}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            return None
    
    def _get_stored_credentials(self, source_id: int) -> Optional[Credentials]:
        """Get stored Google credentials from file system (FIXED to match cloud_endpoints.py)"""
        try:
            # Get source config to extract user email
            conn = get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT config FROM video_sources 
                WHERE id = ? AND source_type = 'cloud' AND active = 1
            """, (source_id,))
            
            result = cursor.fetchone()
            conn.close()
            
            if not result or not result[0]:
                logger.error(f"No source config found for source {source_id}")
                return None
            
            # Parse config to get user email
            import json
            config_data = json.loads(result[0])
            user_email = config_data.get('user_email')
            
            if not user_email:
                logger.error(f"No user_email in source {source_id} config")
                return None
            
            # Load credentials from file system (matching cloud_endpoints.py)
            import hashlib
            import os
            from google.oauth2.credentials import Credentials
            from google.auth.transport.requests import Request
            
            tokens_dir = os.path.join(os.path.dirname(__file__), 'tokens')
            email_hash = hashlib.sha256(user_email.encode()).hexdigest()[:16]
            token_filename = f"google_drive_{email_hash}.json"
            token_filepath = os.path.join(tokens_dir, token_filename)
            
            if not os.path.exists(token_filepath):
                logger.error(f"No credentials file found: {token_filepath}")
                return None
            
            # Load encrypted storage
            with open(token_filepath, 'r') as f:
                encrypted_storage = json.load(f)
            
            # Decrypt credentials using cloud_endpoints decrypt function
            from modules.sources.cloud_endpoints import decrypt_credentials
            credential_data = decrypt_credentials(encrypted_storage['encrypted_data'])
            if not credential_data:
                logger.error(f"Failed to decrypt credentials for: {user_email}")
                return None
            
            # Reconstruct credentials object
            credentials = Credentials(
                token=credential_data['token'],
                refresh_token=credential_data['refresh_token'],
                token_uri=credential_data['token_uri'],
                client_id=credential_data['client_id'],
                client_secret=credential_data['client_secret'],
                scopes=credential_data['scopes']
            )
            
            # Refresh if expired
            if credentials.expired and credentials.refresh_token:
                logger.info("üîÑ Refreshing expired credentials...")
                credentials.refresh(Request())
                
                # Update stored credentials with new token
                credential_data['token'] = credentials.token
                credential_data['expires_at'] = credentials.expiry.isoformat() if credentials.expiry else None
                
                from modules.sources.cloud_endpoints import encrypt_credentials
                encrypted_updated = encrypt_credentials(credential_data)
                if encrypted_updated:
                    encrypted_storage['encrypted_data'] = encrypted_updated
                    with open(token_filepath, 'w') as f:
                        json.dump(encrypted_storage, f, indent=2)
                    os.chmod(token_filepath, 0o600)
            
            logger.info(f"‚úÖ Successfully loaded credentials for: {user_email}")
            return credentials
            
        except Exception as e:
            logger.error(f"‚ùå Error getting stored credentials: {e}")
            return None
    
    def _list_video_files(self, drive: GoogleDrive, folder_id: str) -> List[Dict]:
        """List video files in a Google Drive folder"""
        try:
            video_mimes = [
                'video/mp4', 'video/avi', 'video/mov', 'video/mkv', 
                'video/flv', 'video/wmv', 'video/m4v'
            ]
            
            # Build query for video files in folder
            mime_query = " or ".join([f"mimeType='{mime}'" for mime in video_mimes])
            query = f"'{folder_id}' in parents and ({mime_query}) and trashed=false"
            
            file_list = drive.ListFile({'q': query, 'maxResults': 100}).GetList()
            
            logger.info(f"üìÅ Found {len(file_list)} video files in folder {folder_id}")
            return file_list
            
        except Exception as e:
            logger.error(f"‚ùå Error listing files: {e}")
            return []
    
    def _should_download_file(self, source_id: int, file_info: Dict) -> bool:
        """Check if file should be downloaded (not already downloaded)"""
        try:
            filename = file_info['title']
            file_id = file_info['id']
            
            # Check if already downloaded
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                SELECT COUNT(*) FROM downloaded_files 
                WHERE source_id = ? AND (original_filename = ? OR local_file_path LIKE ?)
            """, (source_id, filename, f'%{filename}'))
            
            count = cursor.fetchone()[0]
            conn.close()
            
            should_download = count == 0
            if not should_download:
                logger.debug(f"‚è≠Ô∏è Skipping {filename} (already downloaded)")
            
            return should_download
            
        except Exception as e:
            logger.error(f"‚ùå Error checking download status: {e}")
            return True  # Download by default if unsure
    
    def _download_single_file(self, drive: GoogleDrive, file_info: Dict, local_path: str) -> bool:
        """Download a single file from Google Drive"""
        try:
            file_obj = drive.CreateFile({'id': file_info['id']})
            file_obj.GetContentFile(local_path)
            
            logger.info(f"üì• Downloaded: {file_info['title']} ‚Üí {local_path}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to download {file_info['title']}: {e}")
            return False
    
    def _track_downloaded_file(self, source_id: int, camera_name: str, file_info: Dict, local_path: str):
        """Track downloaded file in database"""
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO downloaded_files (
                    source_id, camera_name, original_filename, local_file_path,
                    file_size_bytes, download_timestamp, file_format
                ) VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                source_id,
                camera_name,
                file_info['title'],
                local_path,
                int(file_info.get('fileSize', 0)),
                datetime.now().isoformat(),
                os.path.splitext(file_info['title'])[1]
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"‚ùå Error tracking file: {e}")
    
    def _get_source_config(self, source_id: int) -> Optional[Dict]:
        """Get source configuration from database"""
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                SELECT source_type, name, path, config FROM video_sources 
                WHERE id = ? AND active = 1
            """, (source_id,))
            result = cursor.fetchone()
            conn.close()
            
            if result:
                return {
                    'source_type': result[0],
                    'name': result[1], 
                    'path': result[2],
                    'config': result[3]
                }
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Error getting source config: {e}")
            return None
    
    def _update_sync_status(self, source_id: int, status: str, message: str, files_count: int = 0, size_mb: float = 0.0):
        """Update sync status in database"""
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute("""
                UPDATE sync_status 
                SET last_sync_timestamp = ?,
                    last_sync_status = ?,
                    last_sync_message = ?,
                    files_downloaded_count = files_downloaded_count + ?,
                    total_download_size_mb = total_download_size_mb + ?
                WHERE source_id = ?
            """, (
                datetime.now().isoformat(),
                status,
                message,
                files_count,
                size_mb,
                source_id
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"‚ùå Error updating sync status: {e}")
    
    def _schedule_next_sync(self, source_id: int):
        """Schedule next sync run"""
        try:
            status = get_sync_status(source_id)
            if not status or not status.get('sync_enabled', True):
                return
            
            interval = status.get('sync_interval_minutes', 15)
            next_sync_time = datetime.now() + timedelta(minutes=interval)
            
            # Update next sync timestamp in database
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE sync_status 
                SET next_sync_timestamp = ?
                WHERE source_id = ?
            """, (next_sync_time.isoformat(), source_id))
            conn.commit()
            conn.close()
            
            # Schedule timer
            timer = threading.Timer(interval * 60, self._timer_callback, args=(source_id,))
            timer.daemon = True
            self.sync_timers[source_id] = timer
            timer.start()
            
            logger.info(f"‚è∞ Next sync scheduled for source {source_id} at {next_sync_time}")
            
        except Exception as e:
            logger.error(f"‚ùå Error scheduling next sync: {e}")
    
    def _timer_callback(self, source_id: int):
        """Timer callback to perform sync and schedule next"""
        self._perform_sync(source_id)
        self._schedule_next_sync(source_id)
    
    @staticmethod
    def _sanitize_filename(filename: str) -> str:
        """Sanitize filename for file system"""
        import re
        # Remove/replace invalid characters
        sanitized = re.sub(r'[<>:"/\\|?*]', '_', filename)
        return sanitized.strip()

# Global instance
pydrive_downloader = PyDriveDownloader()

# Convenience functions for backward compatibility
def start_source_sync(source_id: int) -> bool:
    """Start auto-sync for a source"""
    return pydrive_downloader.start_auto_sync(source_id)

def stop_source_sync(source_id: int) -> bool:
    """Stop auto-sync for a source"""
    return pydrive_downloader.stop_auto_sync(source_id)

def force_sync_source(source_id: int) -> Dict:
    """Force immediate sync for a source"""
    return pydrive_downloader.force_sync_now(source_id)

```
## üìÑ File: `nvr_downloader.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/nvr_downloader.py`

```python
import os
import json
import sqlite3
from datetime import datetime, timedelta
from modules.db_utils import get_db_connection
from .mock_video_generator import MockVideoGenerator

class NVRDownloader:
    """
    NVR Video Downloader v·ªõi support cho c·∫£ mock v√† real ONVIF downloads
    OPTIMIZED: S·ª≠ d·ª•ng last_downloaded_file tracking thay v√¨ full file tracking
    """
    
    def __init__(self, mock_mode=True, testing_intervals=True):
        """
        Initialize NVRDownloader
        
        Args:
            mock_mode (bool): True = use mock data, False = real ONVIF downloads
            testing_intervals (bool): True = use short intervals for testing
        """
        self.mock_mode = mock_mode
        self.testing_intervals = testing_intervals
        
        if self.mock_mode:
            self.mock_generator = MockVideoGenerator()
            print(f"üé≠ NVRDownloader initialized in MOCK MODE")
        else:
            print(f"üìπ NVRDownloader initialized in REAL MODE")
    
    def download_recordings(self, source_config):
        """
        Main download method - routes to mock or real implementation
        
        Args:
            source_config (dict): Source configuration
                - source_id: Database ID
                - name: Source name
                - selected_cameras: List of camera names
                - working_path: Base download directory
                
        Returns:
            dict: Download results with success status, files, and stats
        """
        print(f"üöÄ Starting download for source: {source_config.get('name')}")
        print(f"üìÅ Working path: {source_config.get('working_path')}")
        print(f"üìπ Cameras: {source_config.get('selected_cameras', [])}")
        
        if self.mock_mode:
            return self._mock_download(source_config)
        else:
            return self._real_onvif_download(source_config)
    
    def _mock_download(self, source_config):
        """
        Mock download implementation using MockVideoGenerator
        OPTIMIZED: Uses last_downloaded_file tracking
        
        Args:
            source_config (dict): Source configuration
            
        Returns:
            dict: Mock download results
        """
        print(f"üé¨ MOCK DOWNLOAD: Generating recordings...")
        
        results = {
            'success': True,
            'downloaded_files': [],
            'total_size': 0,
            'cameras_processed': [],
            'mode': 'mock'
        }
        
        try:
            source_id = source_config.get('source_id')
            selected_cameras = source_config.get('selected_cameras', [])
            working_path = source_config.get('working_path')
            
            if not selected_cameras:
                print("‚ö†Ô∏è No cameras selected for download")
                return results
            
            # Process each camera
            for camera in selected_cameras:
                print(f"üé• Processing camera: {camera}")
                
                # Create camera directory
                camera_dir = os.path.join(working_path, camera.replace(' ', '_'))
                os.makedirs(camera_dir, exist_ok=True)
                print(f"üìÅ Camera directory: {camera_dir}")
                
                # Generate mock files for this camera
                if self.testing_intervals:
                    # Testing mode: Short intervals for development
                    mock_files = self.mock_generator.generate_realtime_testing_videos(
                        camera, 
                        camera_dir, 
                        interval_seconds=120,  # 2 minutes for testing
                        count=15  # 15 files per camera
                    )
                else:
                    # Normal mode: Daily videos with realistic intervals
                    mock_files = self.mock_generator.generate_daily_videos(
                        camera,
                        camera_dir,
                        days=1,
                        schedule='security'  # 4 times per day
                    )
                
                # üÜï OPTIMIZED: Track only latest file instead of all files
                tracked_count = self._track_latest_file_only(source_id, camera, mock_files)
                
                # Update results
                results['downloaded_files'].extend(mock_files)
                results['total_size'] += sum(f['size'] for f in mock_files)
                results['cameras_processed'].append(camera)
                
                print(f"‚úÖ Camera {camera}: {len(mock_files)} files, {tracked_count} efficient tracking")
            
            print(f"üéâ MOCK DOWNLOAD COMPLETED:")
            print(f"   üìä Total files: {len(results['downloaded_files'])}")
            print(f"   üìÅ Total size: {results['total_size']} bytes ({results['total_size']/1024:.1f} KB)")
            print(f"   üé• Cameras processed: {len(results['cameras_processed'])}")
            print(f"   üóÑÔ∏è Database records: {len(results['cameras_processed'])} (optimized)")
            
        except Exception as e:
            print(f"‚ùå Mock download error: {e}")
            results['success'] = False
            results['error'] = str(e)
        
        return results
    
    def _real_onvif_download(self, source_config):
        """
        Real ONVIF download implementation (placeholder for future)
        
        Args:
            source_config (dict): Source configuration
            
        Returns:
            dict: Real download results
        """
        print(f"üìπ REAL ONVIF DOWNLOAD: Not implemented yet")
        
        # TODO: Implement real ONVIF download logic
        # 1. Connect to ONVIF cameras
        # 2. Get last downloaded timestamp from last_downloaded_file table
        # 3. Query ONVIF for recordings newer than last timestamp
        # 4. Download only new files
        # 5. Update last_downloaded_file with latest info
        
        return {
            'success': False,
            'error': 'Real ONVIF download not implemented yet',
            'downloaded_files': [],
            'total_size': 0,
            'cameras_processed': [],
            'mode': 'real'
        }
    
    def _track_latest_file_only(self, source_id, camera_name, file_list):
        """
        üÜï OPTIMIZED: Track only the latest file per camera in last_downloaded_file table
        TEMPORARILY DISABLED: Database tracking disabled for clean workflow
        
        Args:
            source_id (int): Source database ID
            camera_name (str): Camera name
            file_list (list): List of file info dicts
            
        Returns:
            int: Number of files successfully tracked (for compatibility)
        """
        if not file_list:
            return 0
            
        try:
            # Calculate totals for logging
            total_count = len(file_list)
            total_size_mb = sum(f['size'] for f in file_list) / (1024 * 1024)
            
            # Find latest file by timestamp for logging
            latest_file = max(file_list, key=lambda f: f['timestamp'])
            
            # üîß TEMPORARILY DISABLED: Database tracking
            print(f"üìä Efficient tracking DISABLED: Latest file '{latest_file['filename']}' | Total: {total_count} files, {total_size_mb:.1f} MB")
            print(f"‚ö†Ô∏è Database tracking temporarily disabled for clean workflow")
            
            # Return total count for compatibility (without database update)
            return total_count
            
            # üö´ DISABLED CODE BELOW:
            # # Import helper function from database
            # from database import update_last_downloaded_file
            # 
            # # Update database with only latest file info (1 DB operation instead of 15!)
            # success = update_last_downloaded_file(
            #     source_id, camera_name, latest_file, total_count, total_size_mb
            # )
            # 
            # if success:
            #     print(f"üìä Efficient tracking: Latest file '{latest_file['filename']}' | Total: {total_count} files, {total_size_mb:.1f} MB")
            #     return total_count
            # else:
            #     print(f"‚ùå Failed to track latest file for camera: {camera_name}")
            #     return 0
                
        except Exception as e:
            print(f"‚ùå Latest file tracking error: {e}")
            return 0
    
    def _track_downloaded_files(self, source_id, camera_name, file_list):
        """
        ‚ö†Ô∏è DEPRECATED: Legacy method for full file tracking
        Use _track_latest_file_only() instead for better performance
        
        Args:
            source_id (int): Source database ID
            camera_name (str): Camera name
            file_list (list): List of file info dicts
            
        Returns:
            int: Number of files successfully tracked
        """
        print(f"‚ö†Ô∏è Using legacy full file tracking - consider using _track_latest_file_only()")
        
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            
            tracked_count = 0
            
            for file_info in file_list:
                try:
                    cursor.execute("""
                        INSERT INTO downloaded_files (
                            source_id, camera_name, local_file_path, file_size_bytes, 
                            download_timestamp, recording_start_time, original_filename
                        ) VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (
                        source_id,
                        camera_name,
                        file_info['path'],
                        file_info['size'],
                        datetime.now().isoformat(),
                        file_info['timestamp'].isoformat(),
                        file_info['filename']
                    ))
                    tracked_count += 1
                    
                except sqlite3.IntegrityError:
                    print(f"‚ö†Ô∏è File already tracked: {file_info['filename']}")
                except Exception as e:
                    print(f"‚ùå Failed to track file {file_info['filename']}: {e}")
            
            conn.commit()
            print(f"‚úÖ DB commit & close success for camera: {camera_name}")
            conn.close()
            
            print(f"üìä Legacy tracking: {tracked_count}/{len(file_list)} files")
            return tracked_count
            
        except Exception as e:
            print(f"‚ùå Database tracking error: {e}")
            return 0
    
    def get_download_statistics(self, source_id):
        """
        üÜï OPTIMIZED: Get download statistics using last_downloaded_file table
        
        Args:
            source_id (int): Source database ID
            
        Returns:
            dict: Download statistics
        """
        try:
            # Import helper function from database
            from database import get_camera_download_stats
            
            stats = get_camera_download_stats(source_id)
            
            # Convert to expected format for compatibility
            camera_stats = {}
            for camera_name, camera_info in stats['camera_stats'].items():
                camera_stats[camera_name] = {
                    'file_count': camera_info['files_count'],
                    'total_size': camera_info['size_mb'] * 1024 * 1024  # Convert back to bytes
                }
            
            # Get latest download time
            latest_download = None
            for camera_info in stats['camera_stats'].values():
                if camera_info['last_download']:
                    if not latest_download or camera_info['last_download'] > latest_download:
                        latest_download = camera_info['last_download']
            
            return {
                'total_files': stats['total_files'],
                'total_size': int(stats['total_size_mb'] * 1024 * 1024),  # Convert to bytes
                'total_size_mb': stats['total_size_mb'],
                'camera_stats': camera_stats,
                'latest_download': latest_download,
                'cameras_count': stats['cameras_count']
            }
            
        except Exception as e:
            print(f"‚ùå Statistics error: {e}")
            return {
                'total_files': 0,
                'total_size': 0,
                'total_size_mb': 0,
                'camera_stats': {},
                'latest_download': None,
                'cameras_count': 0
            }
    
    def get_last_downloaded_timestamp(self, source_id, camera_name):
        """
        üÜï NEW: Get last downloaded file timestamp for incremental sync
        
        Args:
            source_id (int): Source database ID
            camera_name (str): Camera name
            
        Returns:
            str: ISO timestamp of last downloaded file
        """
        try:
            from database import get_last_downloaded_timestamp
            return get_last_downloaded_timestamp(source_id, camera_name)
        except Exception as e:
            print(f"‚ùå Error getting last timestamp: {e}")
            return "1970-01-01T00:00:00"  # Default to epoch if error
    
    def cleanup_old_downloads(self, source_id, keep_days=30):
        """
        üÜï OPTIMIZED: Clean up old downloaded files using filesystem + last_downloaded_file
        
        Args:
            source_id (int): Source database ID
            keep_days (int): Number of days to keep
            
        Returns:
            dict: Cleanup results
        """
        try:
            from database import get_camera_download_stats
            
            stats = get_camera_download_stats(source_id)
            cutoff_date = datetime.now() - timedelta(days=keep_days)
            
            deleted_files = 0
            deleted_size = 0
            
            # For each camera, clean up old files from filesystem
            for camera_name, camera_info in stats['camera_stats'].items():
                if camera_info['last_download']:
                    last_download_date = datetime.fromisoformat(camera_info['last_download'])
                    
                    # If last download is older than cutoff, could clean up
                    if last_download_date < cutoff_date:
                        print(f"üßπ Camera {camera_name}: Last download {camera_info['last_download']} is old")
                        # Here you could implement filesystem cleanup logic
                        # For now, just report what would be cleaned
            
            # Update last_downloaded_file table to remove old entries
            conn = get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute("""
                DELETE FROM last_downloaded_file 
                WHERE source_id = ? AND last_download_time < ?
            """, (source_id, cutoff_date.isoformat()))
            
            db_deleted = cursor.rowcount
            conn.commit()
            conn.close()
            
            result = {
                'files_deleted': deleted_files,
                'size_freed': deleted_size,
                'size_freed_mb': round(deleted_size / (1024 * 1024), 2),
                'db_records_deleted': db_deleted
            }
            
            print(f"üßπ Cleanup completed: {deleted_files} files, {result['size_freed_mb']} MB freed, {db_deleted} DB records removed")
            return result
            
        except Exception as e:
            print(f"‚ùå Cleanup error: {e}")
            return {
                'files_deleted': 0,
                'size_freed': 0,
                'size_freed_mb': 0,
                'db_records_deleted': 0,
                'error': str(e)
            }
    
    def test_mock_download(self, test_source_config=None):
        """
        Test method ƒë·ªÉ ki·ªÉm tra mock download functionality
        
        Args:
            test_source_config (dict): Optional test configuration
            
        Returns:
            dict: Test results
        """
        if test_source_config is None:
            test_source_config = {
                'source_id': 999,  # Test source ID
                'name': 'test_nvr',
                'selected_cameras': ['Test Camera 1', 'Test Camera 2'],
                'working_path': '/tmp/nvr_test_download'
            }
        
        print(f"üß™ TESTING OPTIMIZED MOCK DOWNLOAD...")
        print(f"üìã Test config: {test_source_config}")
        
        # Ensure test directory exists
        os.makedirs(test_source_config['working_path'], exist_ok=True)
        
        # Run mock download
        results = self._mock_download(test_source_config)
        
        # Verify results
        if results['success']:
            print(f"‚úÖ Test passed: {len(results['downloaded_files'])} files created")
            print(f"üìä Efficient DB tracking: {len(results['cameras_processed'])} records instead of {len(results['downloaded_files'])}")
            
            # Check actual files exist
            sample_files = results['downloaded_files'][:5]  # Show first 5 files
            for file_info in sample_files:
                if os.path.exists(file_info['path']):
                    actual_size = os.path.getsize(file_info['path'])
                    print(f"   üìÑ {file_info['filename']}: {actual_size} bytes")
                else:
                    print(f"   ‚ùå Missing: {file_info['filename']}")
            
            if len(results['downloaded_files']) > 5:
                print(f"   ... and {len(results['downloaded_files']) - 5} more files")
        else:
            print(f"‚ùå Test failed: {results.get('error', 'Unknown error')}")
        
        return results

# Usage examples v√† test function
def test_nvr_downloader():
    """Test function ƒë·ªÉ verify optimized NVRDownloader functionality"""
    print("=== TESTING OPTIMIZED NVRDownloader ===")
    
    # Test 1: Mock download v·ªõi testing intervals
    print("\n--- Test 1: Optimized Mock Download (Testing Mode) ---")
    downloader = NVRDownloader(mock_mode=True, testing_intervals=True)
    
    test_config = {
        'source_id': 37,
        'name': 'nvr_localhost',
        'selected_cameras': ['Front Door Camera', 'Parking Lot Camera'],
        'working_path': '/tmp/test_nvr_downloads'
    }
    
    results = downloader.test_mock_download(test_config)
    
    # Test 2: Optimized Statistics
    print("\n--- Test 2: Optimized Download Statistics ---")
    stats = downloader.get_download_statistics(37)
    print(f"Statistics: {stats}")
    
    # Test 3: Last downloaded timestamp
    print("\n--- Test 3: Last Downloaded Timestamp ---")
    for camera in test_config['selected_cameras']:
        last_timestamp = downloader.get_last_downloaded_timestamp(37, camera)
        print(f"Camera '{camera}' last download: {last_timestamp}")
    
    # Test 4: Normal intervals
    print("\n--- Test 4: Normal Intervals ---")
    downloader_normal = NVRDownloader(mock_mode=True, testing_intervals=False)
    results_normal = downloader_normal.test_mock_download({
        'source_id': 38,
        'name': 'nvr_normal_test',
        'selected_cameras': ['Normal Test Camera'],
        'working_path': '/tmp/test_nvr_normal'
    })
    
    print("\n‚úÖ All optimized tests completed!")

if __name__ == "__main__":
    test_nvr_downloader()
```
## üìÑ File: `cloud_manager.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/cloud_manager.py`

```python
#!/usr/bin/env python3
"""
Cloud Manager for VTrack - Unified Cloud Interface
Handles connection management, folder discovery, and authentication validation
Supports multiple cloud providers with Google Drive as primary implementation
"""

import os
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path

# Import existing Google Drive client
from .google_drive_client import GoogleDriveClient

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CloudManager:
    """
    Unified cloud interface for VTrack video source management
    Provides consistent API across different cloud providers
    """
    
    # Supported cloud providers
    SUPPORTED_PROVIDERS = {
        'google_drive': {
            'name': 'Google Drive',
            'client_class': GoogleDriveClient,
            'auth_type': 'oauth2',
            'supports_folders': True,
            'supports_nested': True
        }
        # Future: Dropbox, OneDrive, etc.
    }
    
    def __init__(self, provider: str = 'google_drive'):
        """
        Initialize CloudManager for specified provider
        
        Args:
            provider (str): Cloud provider name ('google_drive', etc.)
        """
        self.provider = provider
        self.client = None
        self.authenticated = False
        self.user_info = {}
        
        # Validate provider
        if provider not in self.SUPPORTED_PROVIDERS:
            raise ValueError(f"Unsupported provider: {provider}. Supported: {list(self.SUPPORTED_PROVIDERS.keys())}")
        
        # Initialize provider-specific client
        self._initialize_client()
        
        logger.info(f"CloudManager initialized for provider: {provider}")
    
    def _initialize_client(self):
        """Initialize the provider-specific client"""
        try:
            provider_config = self.SUPPORTED_PROVIDERS[self.provider]
            client_class = provider_config['client_class']
            
            if self.provider == 'google_drive':
                self.client = client_class()
            else:
                # Future provider initialization
                raise NotImplementedError(f"Provider {self.provider} not yet implemented")
                
            logger.info(f"‚úÖ {provider_config['name']} client initialized")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize {self.provider} client: {e}")
            raise
    
    def test_connection_and_discover_folders(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Test cloud connection and discover available folders
        Main method called by VTrack's /test-source endpoint
        
        Args:
            config (dict): Source configuration containing provider settings
            
        Returns:
            dict: Connection test results with folder discovery
        """
        try:
            logger.info(f"üîç Testing {self.provider} connection and discovering folders...")
            
            # Step 1: Test basic connection
            connection_result = self.test_connection(config)
            
            if not connection_result['success']:
                return {
                    'accessible': False,
                    'message': connection_result['message'],
                    'provider': self.provider,
                    'folders': [],
                    'cameras': []
                }
            
            # Step 2: Discover root folders
            root_folders = self.discover_root_folders()
            
            # Step 3: Analyze folder structure for camera folders
            folder_analysis = self._analyze_folder_structure(root_folders)
            
            logger.info(f"‚úÖ Connection successful: {len(root_folders)} root folders discovered")
            
            return {
                'accessible': True,
                'message': f"{self.provider.title()} connection successful",
                'provider': self.provider,
                'user_info': self.user_info,
                'folders': root_folders,
                'folder_analysis': folder_analysis,
                'cameras': [],  # Will be populated when specific folder is selected
                'connection_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Connection test failed: {e}")
            return {
                'accessible': False,
                'message': f"Connection failed: {str(e)}",
                'provider': self.provider,
                'folders': [],
                'cameras': [],
                'error': str(e)
            }
    
    def test_connection(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Test basic cloud provider connection
        
        Args:
            config (dict): Provider configuration
            
        Returns:
            dict: Connection test result
        """
        try:
            if self.provider == 'google_drive':
                return self._test_google_drive_connection(config)
            else:
                return {
                    'success': False,
                    'message': f"Provider {self.provider} not implemented"
                }
                
        except Exception as e:
            logger.error(f"‚ùå Connection test error: {e}")
            return {
                'success': False,
                'message': f"Connection test failed: {str(e)}"
            }
    
    def _test_google_drive_connection(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Test Google Drive specific connection"""
        try:
            # Attempt authentication
            auth_success = self.client.authenticate()
            
            if not auth_success:
                return {
                    'success': False,
                    'message': 'Google Drive authentication failed'
                }
            
            # Test API access
            connection_test = self.client.test_connection()
            
            if connection_test['success']:
                self.authenticated = True
                self.user_info = {
                    'email': connection_test.get('user_email', 'Unknown'),
                    'storage_used_gb': connection_test.get('storage_used_gb', 0),
                    'storage_total_gb': connection_test.get('storage_total_gb', 'Unknown')
                }
                
                logger.info(f"‚úÖ Google Drive authenticated: {self.user_info['email']}")
                return {
                    'success': True,
                    'message': f"Connected to Google Drive: {self.user_info['email']}",
                    'user_info': self.user_info
                }
            else:
                return {
                    'success': False,
                    'message': connection_test['message']
                }
                
        except Exception as e:
            logger.error(f"‚ùå Google Drive connection error: {e}")
            return {
                'success': False,
                'message': f"Google Drive connection failed: {str(e)}"
            }
    
    def discover_root_folders(self) -> List[Dict[str, Any]]:
        """
        Discover root-level folders in cloud storage
        
        Returns:
            list: List of root folder information
        """
        try:
            if not self.authenticated:
                logger.warning("‚ö†Ô∏è Not authenticated - cannot discover folders")
                return []
            
            if self.provider == 'google_drive':
                return self._discover_google_drive_folders()
            else:
                logger.warning(f"‚ö†Ô∏è Folder discovery not implemented for {self.provider}")
                return []
                
        except Exception as e:
            logger.error(f"‚ùå Folder discovery error: {e}")
            return []
    
    def _discover_google_drive_folders(self) -> List[Dict[str, Any]]:
        """Discover Google Drive folders"""
        try:
            # Get root-level folders from Google Drive
            folders = self.client.list_files(folder_id='root', limit=50)
            
            root_folders = []
            for folder in folders:
                if folder.get('mimeType') == 'application/vnd.google-apps.folder':
                    folder_info = {
                        'id': folder['id'],
                        'name': folder['name'],
                        'created_time': folder.get('createdTime'),
                        'size': folder.get('size', 0),
                        'provider': 'google_drive',
                        'type': 'folder',
                        'description': f"Google Drive folder: {folder['name']}"
                    }
                    root_folders.append(folder_info)
            
            logger.info(f"üìÅ Discovered {len(root_folders)} Google Drive root folders")
            return root_folders
            
        except Exception as e:
            logger.error(f"‚ùå Google Drive folder discovery error: {e}")
            return []
    
    def discover_subfolders(self, folder_id: str, credentials: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Discover subfolders within a specific folder
        Used for camera folder detection in nested structures
        
        Args:
            folder_id (str): Parent folder ID
            credentials (dict): Optional authentication credentials
            
        Returns:
            dict: Subfolder discovery results
        """
        try:
            logger.info(f"üîç Discovering subfolders in folder: {folder_id}")
            
            if self.provider == 'google_drive':
                return self._discover_google_drive_subfolders(folder_id, credentials)
            else:
                return {
                    'success': False,
                    'message': f"Subfolder discovery not implemented for {self.provider}",
                    'subfolders': []
                }
                
        except Exception as e:
            logger.error(f"‚ùå Subfolder discovery error: {e}")
            return {
                'success': False,
                'message': f"Subfolder discovery failed: {str(e)}",
                'subfolders': [],
                'error': str(e)
            }
    
    def _discover_google_drive_subfolders(self, folder_id: str, credentials: Optional[Dict] = None) -> Dict[str, Any]:
        """Discover Google Drive subfolders"""
        try:
            # Re-authenticate if credentials provided
            if credentials and not self.authenticated:
                # TODO: Use provided credentials for authentication
                pass
            
            # Get subfolders
            subfolders_raw = self.client.list_files(folder_id=folder_id, limit=100)
            
            subfolders = []
            camera_folders = []
            
            for item in subfolders_raw:
                if item.get('mimeType') == 'application/vnd.google-apps.folder':
                    folder_info = {
                        'id': item['id'],
                        'name': item['name'],
                        'created_time': item.get('createdTime'),
                        'size': item.get('size', 0),
                        'parent_id': folder_id,
                        'provider': 'google_drive',
                        'type': 'folder'
                    }
                    
                    # Check if this looks like a camera folder
                    if self._is_camera_folder(item['name']):
                        folder_info['is_camera_folder'] = True
                        camera_folders.append(folder_info)
                    else:
                        folder_info['is_camera_folder'] = False
                    
                    subfolders.append(folder_info)
            
            logger.info(f"üìÅ Found {len(subfolders)} subfolders, {len(camera_folders)} camera folders")
            
            return {
                'success': True,
                'message': f"Found {len(subfolders)} subfolders",
                'subfolders': subfolders,
                'camera_folders': camera_folders,
                'total_folders': len(subfolders),
                'camera_count': len(camera_folders)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Google Drive subfolder discovery error: {e}")
            return {
                'success': False,
                'message': f"Google Drive subfolder discovery failed: {str(e)}",
                'subfolders': [],
                'error': str(e)
            }
    
    def _is_camera_folder(self, folder_name: str) -> bool:
        """
        Determine if a folder name indicates it contains camera videos
        
        Args:
            folder_name (str): Folder name to analyze
            
        Returns:
            bool: True if likely a camera folder
        """
        camera_keywords = [
            'cam', 'camera', 'channel', 'ch', 'zone', 'area',
            'front', 'back', 'door', 'entrance', 'parking',
            'office', 'lobby', 'security', 'surveillance',
            'nvr', 'dvr', 'cctv', 'ip_cam', 'ipcam'
        ]
        
        folder_lower = folder_name.lower()
        
        # Check for camera keywords
        for keyword in camera_keywords:
            if keyword in folder_lower:
                return True
        
        # Check for camera patterns (e.g., "Camera_01", "CH01", "Zone1")
        import re
        camera_patterns = [
            r'camera[_\s]*\d+',
            r'cam[_\s]*\d+',
            r'ch[_\s]*\d+',
            r'channel[_\s]*\d+',
            r'zone[_\s]*\d+',
            r'area[_\s]*\d+'
        ]
        
        for pattern in camera_patterns:
            if re.search(pattern, folder_lower):
                return True
        
        return False
    
    def _analyze_folder_structure(self, root_folders: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Analyze folder structure to provide insights for UI
        
        Args:
            root_folders (list): List of root folders
            
        Returns:
            dict: Folder structure analysis
        """
        analysis = {
            'total_folders': len(root_folders),
            'camera_folders_at_root': 0,
            'security_related_folders': 0,
            'recommended_folders': [],
            'structure_type': 'unknown'
        }
        
        security_keywords = ['security', 'surveillance', 'camera', 'nvr', 'dvr', 'cctv']
        
        for folder in root_folders:
            folder_name_lower = folder['name'].lower()
            
            # Check if root folder is camera folder
            if self._is_camera_folder(folder['name']):
                analysis['camera_folders_at_root'] += 1
            
            # Check if security-related
            if any(keyword in folder_name_lower for keyword in security_keywords):
                analysis['security_related_folders'] += 1
                analysis['recommended_folders'].append(folder)
        
        # Determine structure type
        if analysis['camera_folders_at_root'] > 0:
            analysis['structure_type'] = 'flat_cameras'
        elif analysis['security_related_folders'] > 0:
            analysis['structure_type'] = 'nested_security'
        else:
            analysis['structure_type'] = 'general'
        
        return analysis
    
    def get_authentication_status(self) -> Dict[str, Any]:
        """
        Get current authentication status
        
        Returns:
            dict: Authentication status information
        """
        return {
            'authenticated': self.authenticated,
            'provider': self.provider,
            'user_info': self.user_info if self.authenticated else {},
            'last_check': datetime.now().isoformat()
        }
    
    def disconnect(self) -> Dict[str, Any]:
        """
        Disconnect from cloud provider
        
        Returns:
            dict: Disconnection result
        """
        try:
            # Provider-specific disconnection logic
            if self.provider == 'google_drive':
                # TODO: Implement Google Drive token revocation
                pass
            
            # Reset local state
            self.authenticated = False
            self.user_info = {}
            self.client = None
            
            # Reinitialize client
            self._initialize_client()
            
            logger.info(f"üîå Disconnected from {self.provider}")
            
            return {
                'success': True,
                'message': f"Disconnected from {self.provider}",
                'provider': self.provider
            }
            
        except Exception as e:
            logger.error(f"‚ùå Disconnection error: {e}")
            return {
                'success': False,
                'message': f"Disconnection failed: {str(e)}",
                'error': str(e)
            }
    
    def get_provider_info(self) -> Dict[str, Any]:
        """
        Get information about the current cloud provider
        
        Returns:
            dict: Provider information
        """
        if self.provider in self.SUPPORTED_PROVIDERS:
            provider_config = self.SUPPORTED_PROVIDERS[self.provider]
            return {
                'provider': self.provider,
                'name': provider_config['name'],
                'auth_type': provider_config['auth_type'],
                'supports_folders': provider_config['supports_folders'],
                'supports_nested': provider_config['supports_nested'],
                'authenticated': self.authenticated,
                'available': True
            }
        else:
            return {
                'provider': self.provider,
                'available': False,
                'error': 'Provider not supported'
            }


def test_cloud_manager():
    """
    Test function for CloudManager functionality
    """
    print("üîß Testing CloudManager...")
    
    try:
        # Initialize CloudManager
        print("\n1. Initializing CloudManager...")
        cloud_manager = CloudManager('google_drive')
        print(f"‚úÖ CloudManager initialized for: {cloud_manager.provider}")
        
        # Get provider info
        print("\n2. Getting provider info...")
        provider_info = cloud_manager.get_provider_info()
        print(f"‚úÖ Provider: {provider_info['name']}")
        print(f"   Supports folders: {provider_info['supports_folders']}")
        print(f"   Supports nested: {provider_info['supports_nested']}")
        
        # Test connection (will attempt authentication)
        print("\n3. Testing connection...")
        test_config = {}  # Empty config for basic test
        connection_result = cloud_manager.test_connection_and_discover_folders(test_config)
        
        if connection_result['accessible']:
            print(f"‚úÖ Connection successful!")
            print(f"   User: {connection_result.get('user_info', {}).get('email', 'Unknown')}")
            print(f"   Folders found: {len(connection_result.get('folders', []))}")
        else:
            print(f"‚ùå Connection failed: {connection_result['message']}")
        
        # Get authentication status
        print("\n4. Checking authentication status...")
        auth_status = cloud_manager.get_authentication_status()
        print(f"‚úÖ Authenticated: {auth_status['authenticated']}")
        
        print("\nüéâ CloudManager test completed!")
        
    except Exception as e:
        print(f"‚ùå CloudManager test failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_cloud_manager()
```
## üìÑ File: `mock_video_generator.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/mock_video_generator.py`

```python
import os
import shutil
from datetime import datetime, timedelta
import json

class MockVideoGenerator:
    """
    T·∫°o mock video files ƒë·ªÉ gi·∫£ l·∫≠p ONVIF camera recordings
    S·ª≠ d·ª•ng cho testing v√† development khi ONVIF Profile G kh√¥ng kh·∫£ d·ª•ng
    """
    
    def __init__(self, base_samples_path=None):
        """
        Initialize MockVideoGenerator
        
        Args:
            base_samples_path (str): Path to sample video files (optional)
        """
        self.samples_path = base_samples_path
        self.mock_file_size = 15 * 1024  # 15KB per mock file
        
        # Common camera recording patterns
        self.recording_schedule = {
            'continuous': list(range(0, 24)),  # Every hour
            'business': [8, 9, 10, 11, 12, 13, 14, 15, 16, 17],  # Business hours
            'security': [6, 12, 18, 22],  # 4 times per day
            'minimal': [9, 15, 21],  # 3 times per day
            'testing': 'every_minute'  # üÜï Special mode for testing
        }
    
    def generate_daily_videos(self, camera_name, target_dir, days=7, schedule='security'):
        """
        T·∫°o mock videos cho m·ªôt camera trong X ng√†y
        
        Args:
            camera_name (str): T√™n camera (VD: "Front Door Camera")
            target_dir (str): Th∆∞ m·ª•c ƒë√≠ch
            days (int): S·ªë ng√†y t·∫°o videos (m·∫∑c ƒë·ªãnh 7)
            schedule (str): Lo·∫°i l·ªãch recording ('continuous', 'business', 'security', 'minimal', 'testing')
            
        Returns:
            list: Danh s√°ch c√°c file ƒë√£ t·∫°o v·ªõi metadata
        """
        print(f"üé¨ Generating mock videos for {camera_name} (schedule: {schedule})...")
        
        # Ensure target directory exists
        os.makedirs(target_dir, exist_ok=True)
        
        videos_created = []
        
        # üÜï TESTING MODE: T·∫°o files v·ªõi interval ng·∫Øn
        if schedule == 'testing':
            return self._generate_testing_videos(camera_name, target_dir, days)
        
        # Original schedule logic for other modes
        hours_schedule = self.recording_schedule.get(schedule, self.recording_schedule['security'])
        
        for i in range(days):
            date = datetime.now() - timedelta(days=i)
            
            for hour in hours_schedule:
                # T·∫°o timestamp cho recording
                timestamp = date.replace(hour=hour, minute=0, second=0, microsecond=0)
                
                # Format filename theo convention th·ª±c t·∫ø
                safe_camera_name = camera_name.replace(' ', '_').replace('/', '_')
                filename = f"{safe_camera_name}_{timestamp.strftime('%Y%m%d_%H%M%S')}.mp4"
                
                target_file = os.path.join(target_dir, filename)
                
                # T·∫°o mock file
                file_info = self._create_mock_video_file(
                    target_file, 
                    camera_name, 
                    timestamp
                )
                
                videos_created.append(file_info)
                
        print(f"‚úÖ Created {len(videos_created)} mock videos for {camera_name}")
    def generate_recent_videos(self, camera_name, target_dir, hours=24):
        """
        T·∫°o videos cho X gi·ªù g·∫ßn ƒë√¢y (ƒë·ªÉ simulate realtime download)
        
        Args:
            camera_name (str): T√™n camera
            target_dir (str): Th∆∞ m·ª•c ƒë√≠ch  
            hours (int): S·ªë gi·ªù g·∫ßn ƒë√¢y (m·∫∑c ƒë·ªãnh 24)
            
        Returns:
            list: Danh s√°ch files ƒë∆∞·ª£c t·∫°o
        """
        print(f"üïê Generating recent {hours}h videos for {camera_name}...")
        
        os.makedirs(target_dir, exist_ok=True)
        videos_created = []
        
        # T·∫°o videos m·ªói 2 gi·ªù trong kho·∫£ng th·ªùi gian ch·ªâ ƒë·ªãnh
        for i in range(0, hours, 2):
            timestamp = datetime.now() - timedelta(hours=i)
            
            safe_camera_name = camera_name.replace(' ', '_').replace('/', '_')
            filename = f"{safe_camera_name}_{timestamp.strftime('%Y%m%d_%H%M%S')}.mp4"
            
            target_file = os.path.join(target_dir, filename)
            
            file_info = self._create_mock_video_file(
                target_file, 
                camera_name, 
                timestamp
            )
            
            videos_created.append(file_info)
        
        print(f"‚úÖ Created {len(videos_created)} recent videos for {camera_name}")
        return videos_created
    
    def _generate_testing_videos(self, camera_name, target_dir, days=1):
        """
        üß™ TESTING MODE: T·∫°o videos v·ªõi interval ng·∫Øn ƒë·ªÉ test nhanh
        
        Args:
            camera_name (str): T√™n camera
            target_dir (str): Th∆∞ m·ª•c ƒë√≠ch
            days (int): S·ªë ng√†y (cho testing th∆∞·ªùng l√† 1)
            
        Returns:
            list: Danh s√°ch files ƒë∆∞·ª£c t·∫°o
        """
        print(f"üß™ TESTING MODE: Creating videos every 1-2 minutes for {camera_name}")
        
        videos_created = []
        base_time = datetime.now()
        
        # T·∫°o videos cho 30 ph√∫t g·∫ßn ƒë√¢y, m·ªói 2 ph√∫t 1 file
        for i in range(15):  # 15 files, m·ªói file c√°ch 2 ph√∫t
            timestamp = base_time - timedelta(minutes=i * 2)
            
            safe_camera_name = camera_name.replace(' ', '_').replace('/', '_')
            filename = f"{safe_camera_name}_{timestamp.strftime('%Y%m%d_%H%M%S')}.mp4"
            
            target_file = os.path.join(target_dir, filename)
            
            file_info = self._create_mock_video_file(
                target_file, 
                camera_name, 
                timestamp
            )
            
            videos_created.append(file_info)
        
        print(f"‚úÖ TESTING: Created {len(videos_created)} videos (2-minute intervals)")
        return videos_created
    
    def generate_realtime_testing_videos(self, camera_name, target_dir, interval_seconds=60, count=5):
        """
        üöÄ REALTIME TESTING: T·∫°o videos v·ªõi kho·∫£ng c√°ch r·∫•t ng·∫Øn cho testing realtime
        
        Args:
            camera_name (str): T√™n camera
            target_dir (str): Th∆∞ m·ª•c ƒë√≠ch
            interval_seconds (int): Kho·∫£ng c√°ch gi·ªØa c√°c file (gi√¢y) - m·∫∑c ƒë·ªãnh 60s
            count (int): S·ªë l∆∞·ª£ng files t·∫°o - m·∫∑c ƒë·ªãnh 5
            
        Returns:
            list: Danh s√°ch files ƒë∆∞·ª£c t·∫°o
        """
        print(f"üöÄ REALTIME TESTING: Creating {count} videos every {interval_seconds}s for {camera_name}")
        
        os.makedirs(target_dir, exist_ok=True)
        videos_created = []
        
        base_time = datetime.now()
        
        for i in range(count):
            # T·∫°o timestamp l√πi l·∫°i theo interval
            timestamp = base_time - timedelta(seconds=i * interval_seconds)
            
            safe_camera_name = camera_name.replace(' ', '_').replace('/', '_')
            filename = f"{safe_camera_name}_{timestamp.strftime('%Y%m%d_%H%M%S')}.mp4"
            
            target_file = os.path.join(target_dir, filename)
            
            file_info = self._create_mock_video_file(
                target_file, 
                camera_name, 
                timestamp
            )
            
            videos_created.append(file_info)
        
        print(f"‚úÖ REALTIME TESTING: Created {len(videos_created)} videos ({interval_seconds}s intervals)")
        return videos_created
        """
        T·∫°o videos cho X gi·ªù g·∫ßn ƒë√¢y (ƒë·ªÉ simulate realtime download)
        
        Args:
            camera_name (str): T√™n camera
            target_dir (str): Th∆∞ m·ª•c ƒë√≠ch  
            hours (int): S·ªë gi·ªù g·∫ßn ƒë√¢y (m·∫∑c ƒë·ªãnh 24)
            
        Returns:
            list: Danh s√°ch files ƒë∆∞·ª£c t·∫°o
        """
        print(f"üïê Generating recent {hours}h videos for {camera_name}...")
        
        os.makedirs(target_dir, exist_ok=True)
        videos_created = []
        
        # T·∫°o videos m·ªói 2 gi·ªù trong kho·∫£ng th·ªùi gian ch·ªâ ƒë·ªãnh
        for i in range(0, hours, 2):
            timestamp = datetime.now() - timedelta(hours=i)
            
            safe_camera_name = camera_name.replace(' ', '_').replace('/', '_')
            filename = f"{safe_camera_name}_{timestamp.strftime('%Y%m%d_%H%M%S')}.mp4"
            
            target_file = os.path.join(target_dir, filename)
            
            file_info = self._create_mock_video_file(
                target_file, 
                camera_name, 
                timestamp
            )
            
            videos_created.append(file_info)
        
        print(f"‚úÖ Created {len(videos_created)} recent videos for {camera_name}")
        return videos_created
    
    def _create_mock_video_file(self, filepath, camera_name, timestamp):
        """
        T·∫°o m·ªôt mock video file v·ªõi metadata th·ª±c t·∫ø
        
        Args:
            filepath (str): ƒê∆∞·ªùng d·∫´n file ƒë·∫ßy ƒë·ªß
            camera_name (str): T√™n camera
            timestamp (datetime): Th·ªùi gian recording
            
        Returns:
            dict: Th√¥ng tin file ƒë√£ t·∫°o
        """
        # T·∫°o mock video content v·ªõi metadata
        video_metadata = {
            "camera": camera_name,
            "timestamp": timestamp.isoformat(),
            "duration_minutes": 60,  # Gi·∫£ l·∫≠p recording 1 gi·ªù
            "resolution": "1920x1080",
            "codec": "H.264",
            "framerate": "30fps",
            "bitrate": "2000kbps",
            "file_type": "MP4",
            "mock_data": True
        }
        
        # T·∫°o file content
        header = f"MOCK VIDEO FILE - {camera_name}\n"
        header += f"Recording Time: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n"
        header += f"Metadata: {json.dumps(video_metadata, indent=2)}\n"
        header += "=" * 50 + "\n"
        
        # Mock binary data (gi·∫£ l·∫≠p video data)
        mock_binary = b'\x00\x01\x02\x03' * (self.mock_file_size // 4)
        
        # Write file
        with open(filepath, 'wb') as f:
            f.write(header.encode('utf-8'))
            f.write(mock_binary)
        
        # Return file info
        file_info = {
            'filename': os.path.basename(filepath),
            'path': filepath,
            'size': os.path.getsize(filepath),
            'timestamp': timestamp,
            'camera': camera_name,
            'metadata': video_metadata
        }
        
        return file_info
    
    def simulate_continuous_recording(self, camera_name, target_dir, start_time=None, duration_hours=6):
        """
        Gi·∫£ l·∫≠p continuous recording v·ªõi files nh·ªè m·ªói 30 ph√∫t
        
        Args:
            camera_name (str): T√™n camera
            target_dir (str): Th∆∞ m·ª•c ƒë√≠ch
            start_time (datetime): Th·ªùi gian b·∫Øt ƒë·∫ßu (m·∫∑c ƒë·ªãnh l√† 6h tr∆∞·ªõc)
            duration_hours (int): T·ªïng th·ªùi gian recording (gi·ªù)
            
        Returns:
            list: Danh s√°ch files ƒë√£ t·∫°o
        """
        if start_time is None:
            start_time = datetime.now() - timedelta(hours=duration_hours)
        
        print(f"üìπ Simulating continuous recording for {camera_name} ({duration_hours}h)")
        
        os.makedirs(target_dir, exist_ok=True)
        videos_created = []
        
        # T·∫°o file m·ªói 30 ph√∫t
        intervals = duration_hours * 2  # 2 files per hour
        
        for i in range(intervals):
            timestamp = start_time + timedelta(minutes=i * 30)
            
            safe_camera_name = camera_name.replace(' ', '_').replace('/', '_')
            filename = f"{safe_camera_name}_{timestamp.strftime('%Y%m%d_%H%M%S')}.mp4"
            
            target_file = os.path.join(target_dir, filename)
            
            file_info = self._create_mock_video_file(
                target_file, 
                camera_name, 
                timestamp
            )
            
            videos_created.append(file_info)
        
        print(f"‚úÖ Created {len(videos_created)} continuous recording files for {camera_name}")
        return videos_created
    
    def cleanup_old_files(self, target_dir, keep_days=30):
        """
        D·ªçn d·∫πp c√°c mock files c≈© h∆°n X ng√†y
        
        Args:
            target_dir (str): Th∆∞ m·ª•c c·∫ßn d·ªçn d·∫πp
            keep_days (int): S·ªë ng√†y gi·ªØ l·∫°i (m·∫∑c ƒë·ªãnh 30)
            
        Returns:
            int: S·ªë files ƒë√£ x√≥a
        """
        if not os.path.exists(target_dir):
            return 0
        
        cutoff_time = datetime.now() - timedelta(days=keep_days)
        deleted_count = 0
        
        for filename in os.listdir(target_dir):
            filepath = os.path.join(target_dir, filename)
            
            if os.path.isfile(filepath):
                # Get file modification time
                file_time = datetime.fromtimestamp(os.path.getmtime(filepath))
                
                if file_time < cutoff_time:
                    try:
                        os.remove(filepath)
                        deleted_count += 1
                        print(f"üóëÔ∏è Removed old mock file: {filename}")
                    except Exception as e:
                        print(f"‚ùå Failed to remove {filename}: {e}")
        
        if deleted_count > 0:
            print(f"‚úÖ Cleanup completed: {deleted_count} old files removed")
        
        return deleted_count
    
    def get_mock_statistics(self, target_dir):
        """
        L·∫•y th·ªëng k√™ v·ªÅ mock files trong th∆∞ m·ª•c
        
        Args:
            target_dir (str): Th∆∞ m·ª•c c·∫ßn th·ªëng k√™
            
        Returns:
            dict: Th·ªëng k√™ chi ti·∫øt
        """
        if not os.path.exists(target_dir):
            return {
                'total_files': 0,
                'total_size': 0,
                'size_mb': 0,
                'date_range': None
            }
        
        files = [f for f in os.listdir(target_dir) if f.endswith('.mp4')]
        
        if not files:
            return {
                'total_files': 0,
                'total_size': 0,
                'size_mb': 0,
                'date_range': None
            }
        
        total_size = sum(
            os.path.getsize(os.path.join(target_dir, f)) 
            for f in files
        )
        
        # Extract dates from filenames for range
        dates = []
        for filename in files:
            try:
                # Extract date from filename format: CameraName_YYYYMMDD_HHMMSS.mp4
                date_part = filename.split('_')[-2]  # YYYYMMDD
                date_obj = datetime.strptime(date_part, '%Y%m%d')
                dates.append(date_obj)
            except:
                continue
        
        date_range = None
        if dates:
            date_range = {
                'earliest': min(dates).strftime('%Y-%m-%d'),
                'latest': max(dates).strftime('%Y-%m-%d')
            }
        
        return {
            'total_files': len(files),
            'total_size': total_size,
            'size_mb': round(total_size / (1024 * 1024), 2),
            'date_range': date_range,
            'files': files[:10]  # Sample of filenames
        }

# Usage example v√† test functions
def test_mock_generator():
    """Test function ƒë·ªÉ ki·ªÉm tra MockVideoGenerator"""
    generator = MockVideoGenerator()
    
    test_dir = "/tmp/mock_camera_test"
    
    # Test 1: Testing mode (2-minute intervals)
    print("=== Test 1: Testing Mode (2-minute intervals) ===")
    testing_files = generator.generate_daily_videos(
        "Front Door Camera", 
        test_dir, 
        days=1, 
        schedule='testing'
    )
    print(f"Created {len(testing_files)} testing files")
    
    # Test 2: Realtime testing (1-minute intervals)
    print("\n=== Test 2: Realtime Testing Mode (1-minute intervals) ===")
    realtime_files = generator.generate_realtime_testing_videos(
        "Parking Camera",
        test_dir,
        interval_seconds=60,  # 1 ph√∫t
        count=10  # 10 files
    )
    print(f"Created {len(realtime_files)} realtime files")
    
    # Test 3: Ultra-fast testing (30-second intervals)
    print("\n=== Test 3: Ultra-Fast Testing (30-second intervals) ===")
    ultrafast_files = generator.generate_realtime_testing_videos(
        "Test Camera",
        test_dir,
        interval_seconds=30,  # 30 gi√¢y
        count=5  # 5 files
    )
    print(f"Created {len(ultrafast_files)} ultra-fast files")
    
    # Test 4: Statistics
    print("\n=== Test 4: Statistics ===")
    stats = generator.get_mock_statistics(test_dir)
    print(f"Statistics: {stats}")
    
    # Show actual files created
    print("\n=== Files Created ===")
    if os.path.exists(test_dir):
        files = sorted([f for f in os.listdir(test_dir) if f.endswith('.mp4')])
        for i, filename in enumerate(files[:10]):  # Show first 10
            filepath = os.path.join(test_dir, filename)
            size = os.path.getsize(filepath)
            print(f"{i+1:2d}. {filename} ({size} bytes)")
        if len(files) > 10:
            print(f"    ... and {len(files) - 10} more files")
    
    # Cleanup
    import shutil
    if os.path.exists(test_dir):
        shutil.rmtree(test_dir)
        print(f"\n‚úÖ Cleaned up test directory: {test_dir}")

if __name__ == "__main__":
    test_mock_generator()
```
## üìÑ File: `google_drive_client.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/google_drive_client.py`

```python
# Copy the code from the artifact above
#!/usr/bin/env python3
"""
Google Drive Client for VTrack Cloud Integration
Handles authentication, upload, download, and folder management
"""

import os
import json
import logging
from datetime import datetime
from pathlib import Path

from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GoogleDriveClient:
    """Google Drive API client for VTrack video backup"""
    
    # Google Drive API scopes
    SCOPES = ['https://www.googleapis.com/auth/drive.file']
    
    def __init__(self, credentials_path='google_drive_credentials_web.json', token_path='token.json'):
        """
        Initialize Google Drive client
        
        Args:
            credentials_path (str): Path to OAuth2 credentials file
            token_path (str): Path to store access tokens
        """
        self.credentials_path = os.path.join(os.path.dirname(__file__), credentials_path)
        self.token_path = os.path.join(os.path.dirname(__file__), token_path)
        self.service = None
        self.creds = None
        
        logger.info(f"GoogleDriveClient initialized")
        logger.info(f"Credentials path: {self.credentials_path}")
        logger.info(f"Token path: {self.token_path}")
    
    def authenticate(self):
        """
        Authenticate with Google Drive API using OAuth2
        
        Returns:
            bool: True if authentication successful, False otherwise
        """
        try:
            # Load existing token if available
            if os.path.exists(self.token_path):
                self.creds = Credentials.from_authorized_user_file(self.token_path, self.SCOPES)
            
            # If no valid credentials, start OAuth flow
            if not self.creds or not self.creds.valid:
                if self.creds and self.creds.expired and self.creds.refresh_token:
                    logger.info("Refreshing expired credentials...")
                    self.creds.refresh(Request())
                else:
                    logger.info("Starting OAuth2 authentication flow...")
                    flow = InstalledAppFlow.from_client_secrets_file(
                        self.credentials_path, self.SCOPES)
                    self.creds = flow.run_local_server(port=0)
                
                # Save credentials for next time
                with open(self.token_path, 'w') as token:
                    token.write(self.creds.to_json())
                logger.info("Credentials saved successfully")
            
            # Build Drive service
            self.service = build('drive', 'v3', credentials=self.creds)
            logger.info("‚úÖ Google Drive authentication successful")
            return True
            
        except FileNotFoundError:
            logger.error(f"‚ùå Credentials file not found: {self.credentials_path}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Authentication failed: {e}")
            return False
    
    def test_connection(self):
        """
        Test Google Drive API connection
        
        Returns:
            dict: Connection test result
        """
        try:
            if not self.service:
                if not self.authenticate():
                    return {"success": False, "message": "Authentication failed"}
            
            # Test API call - get user info
            about = self.service.about().get(fields='user, storageQuota').execute()
            user_email = about.get('user', {}).get('emailAddress', 'Unknown')
            
            # Storage info
            quota = about.get('storageQuota', {})
            total_gb = int(quota.get('limit', 0)) / (1024**3) if quota.get('limit') else 'Unlimited'
            used_gb = int(quota.get('usage', 0)) / (1024**3)
            
            logger.info(f"‚úÖ Connected to Google Drive: {user_email}")
            
            return {
                "success": True,
                "message": f"Connected to Google Drive: {user_email}",
                "user_email": user_email,
                "storage_total_gb": total_gb,
                "storage_used_gb": round(used_gb, 2)
            }
            
        except HttpError as e:
            logger.error(f"‚ùå Google Drive API error: {e}")
            return {"success": False, "message": f"API error: {e}"}
        except Exception as e:
            logger.error(f"‚ùå Connection test failed: {e}")
            return {"success": False, "message": f"Connection failed: {str(e)}"}
    
    def create_folder(self, folder_name, parent_folder_id='root'):
        """
        Create folder in Google Drive
        
        Args:
            folder_name (str): Name of folder to create
            parent_folder_id (str): Parent folder ID ('root' for root directory)
            
        Returns:
            str: Created folder ID, None if failed
        """
        try:
            folder_metadata = {
                'name': folder_name,
                'mimeType': 'application/vnd.google-apps.folder',
                'parents': [parent_folder_id]
            }
            
            folder = self.service.files().create(body=folder_metadata, fields='id').execute()
            folder_id = folder.get('id')
            
            logger.info(f"‚úÖ Folder created: {folder_name} (ID: {folder_id})")
            return folder_id
            
        except HttpError as e:
            logger.error(f"‚ùå Failed to create folder {folder_name}: {e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Folder creation error: {e}")
            return None
    
    def find_folder(self, folder_name, parent_folder_id='root'):
        """
        Find folder by name in Google Drive
        
        Args:
            folder_name (str): Name of folder to find
            parent_folder_id (str): Parent folder ID to search in
            
        Returns:
            str: Folder ID if found, None otherwise
        """
        try:
            query = f"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and '{parent_folder_id}' in parents"
            results = self.service.files().list(q=query, fields='files(id, name)').execute()
            folders = results.get('files', [])
            
            if folders:
                folder_id = folders[0]['id']
                logger.info(f"‚úÖ Found folder: {folder_name} (ID: {folder_id})")
                return folder_id
            else:
                logger.info(f"üìÅ Folder not found: {folder_name}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Folder search error: {e}")
            return None
    
    def get_or_create_folder(self, folder_name, parent_folder_id='root'):
        """
        Get existing folder or create new one
        
        Args:
            folder_name (str): Name of folder
            parent_folder_id (str): Parent folder ID
            
        Returns:
            str: Folder ID
        """
        folder_id = self.find_folder(folder_name, parent_folder_id)
        if not folder_id:
            folder_id = self.create_folder(folder_name, parent_folder_id)
        return folder_id
    
    def upload_file(self, file_path, folder_id='root', custom_name=None):
        """
        Upload file to Google Drive
        
        Args:
            file_path (str): Local file path to upload
            folder_id (str): Google Drive folder ID to upload to
            custom_name (str): Custom name for uploaded file
            
        Returns:
            dict: Upload result with file ID and metadata
        """
        try:
            if not os.path.exists(file_path):
                return {"success": False, "message": f"File not found: {file_path}"}
            
            file_name = custom_name or os.path.basename(file_path)
            file_size = os.path.getsize(file_path)
            
            logger.info(f"üì§ Uploading {file_name} ({file_size} bytes) to Google Drive...")
            
            # File metadata
            file_metadata = {
                'name': file_name,
                'parents': [folder_id]
            }
            
            # Create media upload
            media = MediaFileUpload(file_path, resumable=True)
            
            # Upload file
            file = self.service.files().create(
                body=file_metadata,
                media_body=media,
                fields='id, name, size, createdTime'
            ).execute()
            
            file_id = file.get('id')
            upload_size = file.get('size')
            created_time = file.get('createdTime')
            
            logger.info(f"‚úÖ Upload successful: {file_name}")
            logger.info(f"   File ID: {file_id}")
            logger.info(f"   Size: {upload_size} bytes")
            
            return {
                "success": True,
                "message": f"File uploaded successfully: {file_name}",
                "file_id": file_id,
                "file_name": file_name,
                "file_size": upload_size,
                "created_time": created_time,
                "drive_url": f"https://drive.google.com/file/d/{file_id}/view"
            }
            
        except HttpError as e:
            logger.error(f"‚ùå Upload failed: {e}")
            return {"success": False, "message": f"Upload failed: {e}"}
        except Exception as e:
            logger.error(f"‚ùå Upload error: {e}")
            return {"success": False, "message": f"Upload error: {str(e)}"}
    
    def upload_video(self, video_path, camera_name=None):
        """
        Upload video file with organized folder structure
        
        Args:
            video_path (str): Path to video file
            camera_name (str): Camera name for folder organization
            
        Returns:
            dict: Upload result
        """
        try:
            # Create VTrack main folder
            vtrack_folder_id = self.get_or_create_folder("VTrack_Videos")
            
            # Create camera folder if specified
            if camera_name:
                camera_folder_id = self.get_or_create_folder(camera_name, vtrack_folder_id)
                upload_folder_id = camera_folder_id
            else:
                upload_folder_id = vtrack_folder_id
            
            # Generate custom filename with timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            original_name = Path(video_path).stem
            extension = Path(video_path).suffix
            custom_name = f"{original_name}_{timestamp}{extension}"
            
            # Upload file
            result = self.upload_file(video_path, upload_folder_id, custom_name)
            
            if result["success"]:
                result["folder_structure"] = f"VTrack_Videos/{camera_name or 'General'}"
                
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Video upload error: {e}")
            return {"success": False, "message": f"Video upload failed: {str(e)}"}
    
    def list_files(self, folder_id='root', limit=10):
        """
        List files in Google Drive folder
        
        Args:
            folder_id (str): Folder ID to list files from
            limit (int): Maximum number of files to return
            
        Returns:
            list: List of file metadata
        """
        try:
            query = f"'{folder_id}' in parents"
            results = self.service.files().list(
                q=query,
                pageSize=limit,
                fields='files(id, name, size, createdTime, mimeType)'
            ).execute()
            
            files = results.get('files', [])
            logger.info(f"üìã Found {len(files)} files in folder")
            
            return files
            
        except Exception as e:
            logger.error(f"‚ùå File listing error: {e}")
            return []
    
    def download_file(self, file_id, download_path):
        """
        Download file from Google Drive
        
        Args:
            file_id (str): Google Drive file ID
            download_path (str): Local path to save file
            
        Returns:
            dict: Download result
        """
        try:
            # Get file metadata
            file_metadata = self.service.files().get(fileId=file_id).execute()
            file_name = file_metadata.get('name')
            
            logger.info(f"üì• Downloading {file_name} from Google Drive...")
            
            # Download file content
            request = self.service.files().get_media(fileId=file_id)
            
            with open(download_path, 'wb') as file:
                downloader = MediaIoBaseDownload(file, request)
                done = False
                while done is False:
                    status, done = downloader.next_chunk()
                    if status:
                        logger.info(f"Download progress: {int(status.progress() * 100)}%")
            
            logger.info(f"‚úÖ Download completed: {download_path}")
            
            return {
                "success": True,
                "message": f"File downloaded successfully: {file_name}",
                "file_name": file_name,
                "local_path": download_path
            }
            
        except Exception as e:
            logger.error(f"‚ùå Download error: {e}")
            return {"success": False, "message": f"Download failed: {str(e)}"}
    
    def delete_file(self, file_id):
        """
        Delete file from Google Drive
        
        Args:
            file_id (str): Google Drive file ID
            
        Returns:
            dict: Deletion result
        """
        try:
            self.service.files().delete(fileId=file_id).execute()
            logger.info(f"‚úÖ File deleted: {file_id}")
            
            return {
                "success": True,
                "message": f"File deleted successfully: {file_id}"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Delete error: {e}")
            return {"success": False, "message": f"Delete failed: {str(e)}"}


def test_google_drive_client():
    """
    Test function for Google Drive client
    """
    print("üîß Testing Google Drive Client...")
    
    # Initialize client
    client = GoogleDriveClient()
    
    # Test authentication
    print("\n1. Testing authentication...")
    auth_result = client.authenticate()
    print(f"Authentication: {'‚úÖ Success' if auth_result else '‚ùå Failed'}")
    
    if not auth_result:
        return
    
    # Test connection
    print("\n2. Testing connection...")
    conn_result = client.test_connection()
    print(f"Connection: {'‚úÖ Success' if conn_result['success'] else '‚ùå Failed'}")
    print(f"Message: {conn_result['message']}")
    
    if conn_result['success']:
        print(f"User: {conn_result.get('user_email', 'Unknown')}")
        print(f"Storage: {conn_result.get('storage_used_gb', 0):.2f}GB used")
    
    # Test folder creation
    print("\n3. Testing folder creation...")
    vtrack_folder_id = client.get_or_create_folder("VTrack_Test")
    if vtrack_folder_id:
        print(f"‚úÖ VTrack_Test folder ready: {vtrack_folder_id}")
    
    # Test file listing
    print("\n4. Testing file listing...")
    files = client.list_files(limit=5)
    print(f"‚úÖ Found {len(files)} files in root directory")
    
    print("\nüéâ Google Drive Client test completed!")


if __name__ == "__main__":
    test_google_drive_client()
```
## üìÑ File: `cloud_auth.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/cloud_auth.py`

```python
#!/usr/bin/env python3
"""
Cloud Authentication Handler for VTrack
Manages OAuth2 flows, token storage, and session management
Supports Google Drive OAuth2 with extensible design for other providers
"""

import os
import json
import uuid
import logging
import hashlib
import secrets
from datetime import datetime, timedelta
from typing import Dict, Optional, Any, Tuple
from pathlib import Path
import threading
import time
import jwt
from cryptography.fernet import Fernet
import base64
from flask import g

# OAuth2 and Google API imports
try:
    from google.auth.transport.requests import Request
    from google.oauth2.credentials import Credentials
    from google_auth_oauthlib.flow import Flow
    from googleapiclient.discovery import build
    from googleapiclient.errors import HttpError
except ImportError:
    logging.warning("Google API libraries not installed. Run: pip install google-auth google-auth-oauthlib google-api-python-client")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Secret key for JWT - In production, use environment variable
JWT_SECRET_KEY = os.getenv('JWT_SECRET_KEY', 'your-secret-key-change-in-production')

# Encryption key for credentials - In production, use environment variable  
ENCRYPTION_KEY = os.getenv('ENCRYPTION_KEY', Fernet.generate_key())
if isinstance(ENCRYPTION_KEY, str):
    ENCRYPTION_KEY = ENCRYPTION_KEY.encode()

class CloudAuthManager:
    """
    Manages OAuth2 authentication flows for cloud providers
    Handles token storage, refresh, and session management
    """
    
    # OAuth2 scopes for different providers
    PROVIDER_SCOPES = {
        'google_drive': [
            'https://www.googleapis.com/auth/drive.readonly',
            'https://www.googleapis.com/auth/drive.metadata.readonly'
        ]
    }
    
    # OAuth2 endpoints
    OAUTH_ENDPOINTS = {
        'google_drive': {
            'auth_uri': 'https://accounts.google.com/o/oauth2/auth',
            'token_uri': 'https://oauth2.googleapis.com/token',
            'client_secrets_file': 'google_drive_credentials_web.json'
        }
    }
    
    def __init__(self, provider: str = 'google_drive', base_dir: Optional[str] = None):
        """
        Initialize CloudAuthManager
        
        Args:
            provider (str): Cloud provider ('google_drive', etc.)
            base_dir (str): Base directory for credential storage
        """
        self.provider = provider
        self.base_dir = base_dir or os.path.dirname(__file__)
        
        # Authentication state
        self.auth_sessions = {}  # Active auth sessions
        self.credentials_cache = {}  # Cached credentials
        
        # Thread safety
        self.auth_lock = threading.Lock()
        
        # Credential file paths
        self.credentials_dir = os.path.join(self.base_dir, 'credentials')
        self.tokens_dir = os.path.join(self.base_dir, 'tokens')
        
        # Ensure directories exist
        os.makedirs(self.credentials_dir, exist_ok=True)
        os.makedirs(self.tokens_dir, exist_ok=True)
        
        logger.info(f"CloudAuthManager initialized for {provider}")

    def generate_session_token(self, user_email: str, user_info: Dict[str, Any], expires_minutes: int = 30) -> Optional[str]:
        """Generate JWT session token instead of returning raw credentials"""
        try:
            payload = {
                'user_email': user_email,
                'user_name': user_info.get('name', 'Unknown'),
                'photo_url': user_info.get('photo_url'),
                'provider': self.provider,
                'exp': datetime.utcnow() + timedelta(minutes=expires_minutes),
                'iat': datetime.utcnow(),
                'iss': 'vtrack-cloud-auth',
                'type': 'session'
            }
            token = jwt.encode(payload, JWT_SECRET_KEY, algorithm='HS256')
            logger.info(f"‚úÖ Generated session token for: {user_email} (expires in {expires_minutes}min)")
            return token
        except Exception as e:
            logger.error(f"‚ùå JWT generation error: {e}")
            return None

    def verify_session_token(self, token: str) -> Optional[Dict[str, Any]]:
        """Verify and decode JWT session token"""
        try:
            payload = jwt.decode(token, JWT_SECRET_KEY, algorithms=['HS256'])
            return payload
        except jwt.ExpiredSignatureError:
            logger.warning("‚ö†Ô∏è Session token expired")
            return None
        except jwt.InvalidTokenError as e:
            logger.warning(f"‚ö†Ô∏è Invalid session token: {e}")
            return None

    def encrypt_credentials(self, credentials_dict: Dict[str, Any]) -> Optional[str]:
        """Encrypt credentials before storage"""
        try:
            fernet = Fernet(ENCRYPTION_KEY)
            credentials_json = json.dumps(credentials_dict).encode()
            encrypted_data = fernet.encrypt(credentials_json)
            return base64.b64encode(encrypted_data).decode()
        except Exception as e:
            logger.error(f"‚ùå Credential encryption error: {e}")
            return None

    def decrypt_credentials(self, encrypted_data: str) -> Optional[Dict[str, Any]]:
        """Decrypt stored credentials"""
        try:
            fernet = Fernet(ENCRYPTION_KEY)
            encrypted_bytes = base64.b64decode(encrypted_data.encode())
            decrypted_data = fernet.decrypt(encrypted_bytes)
            return json.loads(decrypted_data.decode())
        except Exception as e:
            logger.error(f"‚ùå Credential decryption error: {e}")
            return None

    def initiate_oauth_flow(self, redirect_uri: str = 'http://localhost:8080/oauth/callback') -> Dict[str, Any]:
        """
        Initiate OAuth2 authentication flow
        
        Args:
            redirect_uri (str): OAuth2 redirect URI
            
        Returns:
            dict: OAuth flow information including auth URL
        """
        try:
            logger.info(f"üîê Initiating OAuth2 flow for {self.provider}")
            
            if self.provider == 'google_drive':
                return self._initiate_google_oauth(redirect_uri)
            else:
                return {
                    'success': False,
                    'message': f"OAuth2 not implemented for {self.provider}"
                }
                
        except Exception as e:
            logger.error(f"‚ùå OAuth2 initiation failed: {e}")
            return {
                'success': False,
                'message': f"OAuth2 initiation failed: {str(e)}",
                'error': str(e)
            }
    
    def _initiate_google_oauth(self, redirect_uri: str) -> Dict[str, Any]:
        """Initiate Google Drive OAuth2 flow"""
        try:
            # Get client secrets file path
            client_secrets_file = os.path.join(
                self.credentials_dir, 
                self.OAUTH_ENDPOINTS['google_drive']['client_secrets_file']
            )
            
            if not os.path.exists(client_secrets_file):
                return {
                    'success': False,
                    'message': f"Google Drive credentials file not found: {client_secrets_file}",
                    'setup_required': True
                }
            
            # Create OAuth2 flow
            flow = Flow.from_client_secrets_file(
                client_secrets_file,
                scopes=self.PROVIDER_SCOPES['google_drive'],
                redirect_uri=redirect_uri
            )
            
            # Generate state parameter for security
            state = secrets.token_urlsafe(32)
            
            # Generate authorization URL
            auth_url, _ = flow.authorization_url(
                access_type='offline',
                include_granted_scopes='true',
                state=state,
                prompt='consent'  # Force consent to get refresh token
            )
            
            # Store flow in session for later completion
            session_id = str(uuid.uuid4())
            
            with self.auth_lock:
                self.auth_sessions[session_id] = {
                    'flow': flow,
                    'state': state,
                    'provider': self.provider,
                    'created_at': datetime.now().isoformat(),
                    'redirect_uri': redirect_uri,
                    'status': 'pending'
                }
            
            logger.info(f"‚úÖ Google OAuth2 flow created with session: {session_id}")
            
            return {
                'success': True,
                'auth_url': auth_url,
                'session_id': session_id,
                'state': state,
                'provider': self.provider,
                'message': 'OAuth2 flow initiated successfully'
            }
            
        except Exception as e:
            logger.error(f"‚ùå Google OAuth2 initiation error: {e}")
            return {
                'success': False,
                'message': f"Google OAuth2 initiation failed: {str(e)}",
                'error': str(e)
            }

    def complete_oauth_flow(self, session_id: str, authorization_code: str, state: str) -> Dict[str, Any]:
        """üîê PHASE 1 SECURITY: Complete OAuth2 flow with session tokens instead of raw credentials"""
        try:
            logger.info(f"üîê Completing OAuth2 flow for session: {session_id}")
            
            # Retrieve and validate session
            with self.auth_lock:
                if session_id not in self.auth_sessions:
                    return {
                        'success': False,
                        'message': 'OAuth session not found or expired',
                        'requires_reauth': True
                    }
                
                session = self.auth_sessions[session_id]
                
                # Enhanced state validation with timing check
                if session['state'] != state:
                    return {
                        'success': False,
                        'message': 'OAuth state mismatch - potential security issue',
                        'security_error': True
                    }
                
                # Check session age (max 1 hour)
                session_created = datetime.fromisoformat(session['created_at'])
                if datetime.now() - session_created > timedelta(hours=1):
                    return {
                        'success': False,
                        'message': 'OAuth session expired',
                        'requires_reauth': True
                    }
                
                flow = session['flow']
            
            # Exchange authorization code for tokens
            flow.fetch_token(code=authorization_code)
            credentials = flow.credentials
            
            # Validate credentials
            if not credentials.valid:
                return {
                    'success': False,
                    'message': 'Invalid credentials received from OAuth flow'
                }
            
            # Get user information
            user_info = self._get_user_info(credentials)
            
            # üîê PHASE 1: Secure credential storage with encryption
            credential_storage_result = self._store_credentials_securely(credentials, user_info)
            
            if not credential_storage_result['success']:
                return credential_storage_result
            
            # Generate session token instead of returning raw credentials
            session_token = self.generate_session_token(user_info['email'], user_info)
            if not session_token:
                return {
                    'success': False,
                    'message': 'Failed to generate session token',
                    'error': 'token_generation_failed'
                }
            
            # Update session status
            with self.auth_lock:
                if session_id in self.auth_sessions:
                    self.auth_sessions[session_id]['status'] = 'completed'
                    self.auth_sessions[session_id]['completed_at'] = datetime.now().isoformat()
            
            # Audit logging
            self._log_authentication_event(
                event_type='oauth_completed',
                user_email=user_info.get('email', 'Unknown'),
                session_id=session_id,
                success=True
            )
            
            logger.info(f"‚úÖ OAuth2 flow completed securely for: {user_info.get('email', 'Unknown')}")
            
            # üîê Return session data instead of raw credentials
            return {
                'success': True,
                'message': 'OAuth2 authentication completed successfully',
                'user_info': user_info,
                'session_token': session_token,  # JWT token only
                'user_email': user_info['email'],
                'provider': self.provider,
                'authenticated_at': datetime.now().isoformat(),
                'security_mode': 'encrypted_storage',
                'token_expires_minutes': 30
                # ‚ùå REMOVED: 'credentials' field - no longer exposed
            }
            
        except Exception as e:
            logger.error(f"‚ùå OAuth2 completion error: {e}")
            
            # Audit logging for failures
            self._log_authentication_event(
                event_type='oauth_failed',
                user_email='Unknown',
                session_id=session_id,
                success=False,
                error=str(e)
            )
            
            return {
                'success': False,
                'message': f'OAuth2 completion failed: {str(e)}',
                'error': str(e),
                'security_mode': 'encrypted_storage'
            }
            
    def _get_user_info(self, credentials: Credentials) -> Dict[str, Any]:
        """
        Get user information from OAuth2 credentials
        
        Args:
            credentials: OAuth2 credentials
            
        Returns:
            dict: User information
        """
        try:
            if self.provider == 'google_drive':
                return self._get_google_user_info(credentials)
            else:
                return {'email': 'unknown', 'name': 'Unknown User'}
                
        except Exception as e:
            logger.error(f"‚ùå Error getting user info: {e}")
            return {'email': 'unknown', 'name': 'Unknown User', 'error': str(e)}
    
    def _get_google_user_info(self, credentials: Credentials) -> Dict[str, Any]:
        """Get Google user information"""
        try:
            # Build Drive service to get user info
            service = build('drive', 'v3', credentials=credentials)
            about = service.about().get(fields='user,storageQuota').execute()
            
            user = about.get('user', {})
            quota = about.get('storageQuota', {})
            
            return {
                'email': user.get('emailAddress', 'unknown'),
                'name': user.get('displayName', 'Unknown User'),
                'photo_url': user.get('photoLink'),
                'storage_used_gb': int(quota.get('usage', 0)) / (1024**3) if quota.get('usage') else 0,
                'storage_total_gb': int(quota.get('limit', 0)) / (1024**3) if quota.get('limit') else 'Unlimited'
            }
            
        except Exception as e:
            logger.error(f"‚ùå Google user info error: {e}")
            return {'email': 'unknown', 'name': 'Unknown User', 'error': str(e)}
    
    def _store_credentials_securely(self, credentials: Credentials, user_info: Dict[str, Any]) -> Dict[str, Any]:
        """üîê PHASE 1 SECURITY: Securely store OAuth2 credentials with AES-256 encryption"""
        try:
            # Prepare credential data
            credential_data = {
                'token': credentials.token,
                'refresh_token': credentials.refresh_token,
                'token_uri': credentials.token_uri,
                'client_id': credentials.client_id,
                'client_secret': credentials.client_secret,
                'scopes': list(credentials.scopes) if credentials.scopes else [],
                'provider': self.provider,
                'user_info': user_info,
                'created_at': datetime.now().isoformat(),
                'expires_at': credentials.expiry.isoformat() if credentials.expiry else None
            }
            
            # üîê Encrypt credentials
            encrypted_credentials = self.encrypt_credentials(credential_data)
            if not encrypted_credentials:
                return {
                    'success': False,
                    'message': 'Failed to encrypt credentials',
                    'error': 'encryption_failed'
                }
            
            # Generate secure filename
            user_email = user_info.get('email', 'unknown')
            email_hash = hashlib.sha256(user_email.encode()).hexdigest()[:16]
            token_filename = f"{self.provider}_{email_hash}.json"
            token_filepath = os.path.join(self.tokens_dir, token_filename)
            
            # Store encrypted credentials
            encrypted_storage = {
                'encrypted_data': encrypted_credentials,
                'user_email': user_email,  # Keep email unencrypted for lookup
                'created_at': datetime.now().isoformat(),
                'encryption_version': '1.0',
                'provider': self.provider
            }
            
            with open(token_filepath, 'w') as f:
                json.dump(encrypted_storage, f, indent=2)
            
            # Set restrictive permissions (owner read/write only)
            os.chmod(token_filepath, 0o600)
            
            # Cache credentials for immediate use
            with self.auth_lock:
                self.credentials_cache[user_email] = {
                    'credentials': credentials,
                    'user_info': user_info,
                    'filepath': token_filepath,
                    'cached_at': datetime.now()
                }
            
            logger.info(f"‚úÖ Encrypted credentials stored for: {user_email}")
            
            return {
                'success': True,
                'message': 'Credentials stored securely',
                'filepath': token_filepath,
                'encryption_used': 'AES-256'
            }
            
        except Exception as e:
            logger.error(f"‚ùå Secure credential storage error: {e}")
            return {
                'success': False,
                'message': f'Credential storage failed: {str(e)}',
                'error': str(e)
            }
        
    def load_stored_credentials(self, user_email: Optional[str] = None) -> Optional[Credentials]:
        """
        Load stored credentials for user
        
        Args:
            user_email (str): User email (if None, loads first available)
            
        Returns:
            Credentials: OAuth2 credentials if found
        """
        try:
            # Check cache first
            if user_email and user_email in self.credentials_cache:
                cached = self.credentials_cache[user_email]
                if (datetime.now() - cached['cached_at']).seconds < 3600:  # 1 hour cache
                    logger.info(f"‚úÖ Using cached credentials for: {user_email}")
                    return cached['credentials']
            
            # Load from file
            token_files = []
            for filename in os.listdir(self.tokens_dir):
                if filename.startswith(f"{self.provider}_") and filename.endswith('.json'):
                    token_files.append(filename)
            
            if not token_files:
                logger.info("üì≠ No stored credentials found")
                return None
            
            # If specific user requested, find their file
            if user_email:
                email_hash = hashlib.sha256(user_email.encode()).hexdigest()[:16]
                target_filename = f"{self.provider}_{email_hash}.json"
                if target_filename in token_files:
                    token_files = [target_filename]
                else:
                    logger.warning(f"‚ö†Ô∏è No credentials found for: {user_email}")
                    return None
            
            # Load the first (or specified) credential file
            token_filepath = os.path.join(self.tokens_dir, token_files[0])
            
            with open(token_filepath, 'r') as f:
                credential_data = json.load(f)
            
            # Check if this is encrypted storage
            if 'encrypted_data' in credential_data:
                # New encrypted format
                decrypted_data = self.decrypt_credentials(credential_data['encrypted_data'])
                if not decrypted_data:
                    logger.error(f"‚ùå Failed to decrypt credentials")
                    return None
                credential_data = decrypted_data
            
            # Reconstruct credentials
            credentials = Credentials(
                token=credential_data['token'],
                refresh_token=credential_data['refresh_token'],
                token_uri=credential_data['token_uri'],
                client_id=credential_data['client_id'],
                client_secret=credential_data['client_secret'],
                scopes=credential_data['scopes']
            )
            
            # Refresh if expired
            if credentials.expired and credentials.refresh_token:
                logger.info("üîÑ Refreshing expired credentials...")
                credentials.refresh(Request())
                
                # Update stored credentials
                credential_data['token'] = credentials.token
                credential_data['expires_at'] = credentials.expiry.isoformat() if credentials.expiry else None
                
                # Re-encrypt and save
                encrypted_credentials = self.encrypt_credentials(credential_data)
                if encrypted_credentials:
                    encrypted_storage = {
                        'encrypted_data': encrypted_credentials,
                        'user_email': credential_data['user_info']['email'],
                        'created_at': datetime.now().isoformat(),
                        'encryption_version': '1.0',
                        'provider': self.provider
                    }
                    with open(token_filepath, 'w') as f:
                        json.dump(encrypted_storage, f, indent=2)
                    os.chmod(token_filepath, 0o600)
            
            # Cache credentials
            stored_user_email = credential_data['user_info']['email']
            with self.auth_lock:
                self.credentials_cache[stored_user_email] = {
                    'credentials': credentials,
                    'user_info': credential_data['user_info'],
                    'filepath': token_filepath,
                    'cached_at': datetime.now()
                }
            
            logger.info(f"‚úÖ Loaded credentials for: {stored_user_email}")
            return credentials
            
        except Exception as e:
            logger.error(f"‚ùå Error loading credentials: {e}")
            return None
    
    def get_authentication_status(self, session_token: Optional[str] = None) -> Dict[str, Any]:
        """üîê PHASE 1 SECURITY: Get authentication status using session tokens"""
        try:
            if not session_token:
                return {
                    'authenticated': False,
                    'provider': self.provider,
                    'message': 'No session token provided',
                    'security_mode': 'session_based',
                    'last_check': datetime.now().isoformat()
                }
            
            # Verify session token
            token_payload = self.verify_session_token(session_token)
            if not token_payload:
                return {
                    'authenticated': False,
                    'provider': self.provider,
                    'message': 'Invalid or expired session token',
                    'requires_reauth': True,
                    'security_mode': 'session_based',
                    'last_check': datetime.now().isoformat()
                }
            
            user_email = token_payload.get('user_email')
            if not user_email:
                return {
                    'authenticated': False,
                    'provider': self.provider,
                    'message': 'Invalid token payload',
                    'security_mode': 'session_based',
                    'last_check': datetime.now().isoformat()
                }
            
            # Check if encrypted credentials exist
            encrypted_credentials_exist = self._check_encrypted_credentials_exist(user_email)
            
            if encrypted_credentials_exist:
                return {
                    'authenticated': True,
                    'provider': self.provider,
                    'user_email': user_email,
                    'user_name': token_payload.get('user_name', 'Unknown'),
                    'photo_url': token_payload.get('photo_url'),
                    'expires_at': token_payload.get('exp'),
                    'security_mode': 'session_based',
                    'credentials_encrypted': True,
                    'last_check': datetime.now().isoformat()
                }
            else:
                return {
                    'authenticated': False,
                    'provider': self.provider,
                    'message': 'Stored credentials not found',
                    'requires_reauth': True,
                    'security_mode': 'session_based',
                    'last_check': datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"‚ùå Authentication status check error: {e}")
            return {
                'authenticated': False,
                'provider': self.provider,
                'error': str(e),
                'security_mode': 'session_based',
                'last_check': datetime.now().isoformat()
            }

    def _check_encrypted_credentials_exist(self, user_email: str) -> bool:
        """Check if encrypted credentials exist for user"""
        try:
            email_hash = hashlib.sha256(user_email.encode()).hexdigest()[:16]
            token_filename = f"{self.provider}_{email_hash}.json"
            token_filepath = os.path.join(self.tokens_dir, token_filename)
            return os.path.exists(token_filepath)
        except Exception:
            return False

    def load_credentials_from_session(self, session_token: str) -> Optional[Credentials]:
        """üîê PHASE 1: Load encrypted credentials using session token"""
        try:
            # Verify session token
            token_payload = self.verify_session_token(session_token)
            if not token_payload:
                logger.warning("‚ö†Ô∏è Invalid session token for credential loading")
                return None
            
            user_email = token_payload.get('user_email')
            if not user_email:
                logger.warning("‚ö†Ô∏è No user email in session token")
                return None
            
            return self._load_encrypted_credentials(user_email)
            
        except Exception as e:
            logger.error(f"‚ùå Session-based credential loading error: {e}")
            return None

    def _load_encrypted_credentials(self, user_email: str) -> Optional[Credentials]:
        """Load and decrypt stored credentials for user"""
        try:
            # Check cache first
            if user_email in self.credentials_cache:
                cached = self.credentials_cache[user_email]
                if (datetime.now() - cached['cached_at']).seconds < 3600:  # 1 hour cache
                    logger.info(f"‚úÖ Using cached credentials for: {user_email}")
                    return cached['credentials']
            
            # Load from encrypted storage
            email_hash = hashlib.sha256(user_email.encode()).hexdigest()[:16]
            token_filename = f"{self.provider}_{email_hash}.json"
            token_filepath = os.path.join(self.tokens_dir, token_filename)
            
            if not os.path.exists(token_filepath):
                logger.warning(f"‚ö†Ô∏è No encrypted credentials found for: {user_email}")
                return None
            
            # Load and decrypt
            with open(token_filepath, 'r') as f:
                encrypted_storage = json.load(f)
            
            credential_data = self.decrypt_credentials(encrypted_storage['encrypted_data'])
            if not credential_data:
                logger.error(f"‚ùå Failed to decrypt credentials for: {user_email}")
                return None
            
            # Reconstruct credentials
            credentials = Credentials(
                token=credential_data['token'],
                refresh_token=credential_data['refresh_token'],
                token_uri=credential_data['token_uri'],
                client_id=credential_data['client_id'],
                client_secret=credential_data['client_secret'],
                scopes=credential_data['scopes']
            )
            
            # Refresh if expired
            if credentials.expired and credentials.refresh_token:
                logger.info("üîÑ Refreshing expired credentials...")
                credentials.refresh(Request())
                
                # Update stored credentials
                credential_data['token'] = credentials.token
                credential_data['expires_at'] = credentials.expiry.isoformat() if credentials.expiry else None
                
                encrypted_updated = self.encrypt_credentials(credential_data)
                if encrypted_updated:
                    encrypted_storage['encrypted_data'] = encrypted_updated
                    with open(token_filepath, 'w') as f:
                        json.dump(encrypted_storage, f, indent=2)
                    os.chmod(token_filepath, 0o600)
            
            # Update cache
            with self.auth_lock:
                self.credentials_cache[user_email] = {
                    'credentials': credentials,
                    'user_info': credential_data['user_info'],
                    'filepath': token_filepath,
                    'cached_at': datetime.now()
                }
            
            logger.info(f"‚úÖ Loaded encrypted credentials for: {user_email}")
            return credentials
            
        except Exception as e:
            logger.error(f"‚ùå Error loading encrypted credentials: {e}")
            return None
    
    def _log_authentication_event(self, event_type: str, user_email: str, session_id: str = None, 
                                 success: bool = True, error: str = None):
        """üîê PHASE 1: Audit logging for authentication events"""
        try:
            # Get IP address safely
            try:
                ip_address = g.remote_addr if hasattr(g, 'remote_addr') else 'unknown'
            except RuntimeError:
                ip_address = 'unknown'  # Outside Flask request context
            
            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'event_type': event_type,
                'user_email': user_email,
                'session_id': session_id,
                'provider': self.provider,
                'success': success,
                'error': error,
                'ip_address': ip_address
            }
            
            # Write to audit log file
            audit_log_path = os.path.join(self.base_dir, 'audit_logs')
            os.makedirs(audit_log_path, exist_ok=True)
            
            log_file = os.path.join(audit_log_path, f'auth_audit_{datetime.now().strftime("%Y%m")}.log')
            with open(log_file, 'a') as f:
                f.write(json.dumps(log_entry) + '\n')
            
            # Set restrictive permissions on audit log
            os.chmod(log_file, 0o600)
            
            logger.info(f"üìù Audit log: {event_type} for {user_email} - {'Success' if success else 'Failed'}")
            
        except Exception as e:
            logger.error(f"‚ùå Audit logging error: {e}")
            
    def revoke_credentials(self, session_token: str = None, user_email: str = None) -> Dict[str, Any]:
        """üîê PHASE 1 SECURITY: Revoke stored credentials with audit logging"""
        try:
            revoked_count = 0
            revoked_users = []
            
            if session_token:
                # Revoke based on session token
                token_payload = self.verify_session_token(session_token)
                if token_payload:
                    user_email = token_payload.get('user_email')
                else:
                    return {
                        'success': False,
                        'message': 'Invalid session token for revocation',
                        'security_mode': 'session_based'
                    }
            
            if user_email:
                # Revoke specific user credentials
                email_hash = hashlib.sha256(user_email.encode()).hexdigest()[:16]
                token_filename = f"{self.provider}_{email_hash}.json"
                token_filepath = os.path.join(self.tokens_dir, token_filename)
                
                if os.path.exists(token_filepath):
                    os.remove(token_filepath)
                    revoked_count = 1
                    revoked_users.append(user_email)
                    
                    # Remove from cache
                    with self.auth_lock:
                        if user_email in self.credentials_cache:
                            del self.credentials_cache[user_email]
                    
                    # Audit logging
                    self._log_authentication_event(
                        event_type='credentials_revoked',
                        user_email=user_email,
                        success=True
                    )
            else:
                # Revoke all credentials for provider
                for filename in os.listdir(self.tokens_dir):
                    if filename.startswith(f"{self.provider}_") and filename.endswith('.json'):
                        filepath = os.path.join(self.tokens_dir, filename)
                        
                        # Try to get user email for audit log
                        try:
                            with open(filepath, 'r') as f:
                                storage = json.load(f)
                                revoked_user_email = storage.get('user_email', 'unknown')
                                revoked_users.append(revoked_user_email)
                        except:
                            revoked_users.append('unknown')
                        
                        os.remove(filepath)
                        revoked_count += 1
                
                # Clear cache
                with self.auth_lock:
                    self.credentials_cache.clear()
                
                # Audit logging for bulk revocation
                for revoked_user in revoked_users:
                    self._log_authentication_event(
                        event_type='credentials_revoked_bulk',
                        user_email=revoked_user,
                        success=True
                    )
            
            logger.info(f"üîå Revoked {revoked_count} credential(s) for {self.provider}")
            
            return {
                'success': True,
                'message': f"Revoked {revoked_count} credential(s)",
                'revoked_count': revoked_count,
                'revoked_users': revoked_users,
                'provider': self.provider,
                'security_mode': 'session_based'
            }
            
        except Exception as e:
            logger.error(f"‚ùå Credential revocation error: {e}")
            return {
                'success': False,
                'message': f"Credential revocation failed: {str(e)}",
                'error': str(e),
                'security_mode': 'session_based'
            }
    
    def cleanup_expired_sessions(self, max_age_hours: int = 1):
        """
        Clean up expired OAuth sessions
        
        Args:
            max_age_hours (int): Maximum session age in hours
        """
        try:
            cutoff_time = datetime.now() - timedelta(hours=max_age_hours)
            expired_sessions = []
            
            with self.auth_lock:
                for session_id, session_data in list(self.auth_sessions.items()):
                    created_at = datetime.fromisoformat(session_data['created_at'])
                    if created_at < cutoff_time:
                        expired_sessions.append(session_id)
                
                for session_id in expired_sessions:
                    del self.auth_sessions[session_id]
            
            if expired_sessions:
                logger.info(f"üßπ Cleaned up {len(expired_sessions)} expired OAuth sessions")
                
        except Exception as e:
            logger.error(f"‚ùå Session cleanup error: {e}")


def test_cloud_auth():
    """Test function for CloudAuthManager functionality"""
    print("üîß Testing CloudAuthManager...")
    
    try:
        # Initialize CloudAuthManager
        print("\n1. Initializing CloudAuthManager...")
        auth_manager = CloudAuthManager('google_drive')
        print("‚úÖ CloudAuthManager initialized")
        
        # Check authentication status
        print("\n2. Checking authentication status...")
        auth_status = auth_manager.get_authentication_status()
        print(f"‚úÖ Authenticated: {auth_status['authenticated']}")
        
        if auth_status['authenticated']:
            print(f"   User: {auth_status.get('user_email', 'Unknown')}")
        else:
            print("   No existing credentials found")
        
        # Load stored credentials (if any)
        print("\n3. Loading stored credentials...")
        credentials = auth_manager.load_stored_credentials()
        
        if credentials:
            print(f"‚úÖ Credentials loaded successfully")
            print(f"   Valid: {credentials.valid}")
            print(f"   Scopes: {list(credentials.scopes) if credentials.scopes else 'None'}")
        else:
            print("üì≠ No stored credentials found")
        
        print("\nüéâ CloudAuthManager test completed!")
        
    except Exception as e:
        print(f"‚ùå CloudAuthManager test failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_cloud_auth()
```
## üìÑ File: `nvr_client.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/nvr_client.py`

```python
# nvr_client.py - Fixed Authentication for ZM v1.34.26
import requests
import logging
import socket
import random
import json
import os
from typing import Dict, List, Tuple
from datetime import datetime, timedelta
from .onvif_client import onvif_client

class NVRClient:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.session = requests.Session()
        self.base_url = None
        # üÜï JWT Token storage
        self.access_token = None
        self.auth_credentials = None
    
    def _authenticate_zoneminder(self, username: str, password: str) -> bool:
        """üîß DEBUG: ZoneMinder authentication with detailed logging"""
        try:
            auth_data = {
                'user': username,
                'pass': password
            }
            
            login_url = f"{self.base_url}/host/login.json"
            self.logger.info(f"üîê Attempting login to: {login_url}")
            self.logger.info(f"üîê Login data: user={username}, pass=***")
            
            response = self.session.post(login_url, data=auth_data, timeout=10)
            
            self.logger.info(f"üîê Login response status: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                self.logger.info(f"üîê Login response data: {json.dumps(data, indent=2)}")
                
                # üéØ PRIORITY: Use auth credentials (works with current ZM config)
                if 'credentials' in data and data.get('credentials'):
                    self.auth_credentials = data['credentials']  # "auth=abc123"
                    self.logger.info(f"‚úÖ ZoneMinder auth hash successful: {self.auth_credentials}")
                    return True
                
                # Fallback: JWT token (if ZM configured differently)
                elif 'access_token' in data and data.get('access_token'):
                    self.access_token = data['access_token']
                    self.session.headers.update({
                        'Authorization': f'Bearer {self.access_token}'
                    })
                    self.logger.info("‚úÖ ZoneMinder JWT authentication successful")
                    return True
                
                # Simple success
                elif data.get('success', False):
                    self.logger.info("‚úÖ ZoneMinder basic auth successful")
                    return True
                
                else:
                    self.logger.error("‚ùå ZoneMinder authentication failed - no valid response")
                    return False
            else:
                self.logger.error(f"‚ùå ZoneMinder auth request failed: {response.status_code}")
                self.logger.error(f"‚ùå Response text: {response.text}")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå ZoneMinder authentication error: {e}")
            return False
    
    def _make_authenticated_request(self, url: str, **kwargs):
        """üîß DEBUG: Make request with detailed logging"""
        
        self.logger.info(f"üåê Making request to: {url}")
        
        # Priority: Auth credentials (current working method)
        if self.auth_credentials:
            separator = '&' if '?' in url else '?'
            auth_url = f"{url}{separator}{self.auth_credentials}"
            self.logger.info(f"üîë Using auth hash URL: {auth_url}")
            response = self.session.get(auth_url, **kwargs)
            self.logger.info(f"üåê Auth hash response status: {response.status_code}")
            if response.status_code != 200:
                self.logger.error(f"‚ùå Auth hash response error: {response.text[:200]}")
            return response
        
        # Fallback: JWT token
        elif self.access_token:
            self.logger.info(f"üîë Using JWT token in headers")
            response = self.session.get(url, **kwargs)
            self.logger.info(f"üåê JWT response status: {response.status_code}")
            return response
        
        # No auth - try direct request
        else:
            self.logger.info(f"üîì No auth - direct request")
            response = self.session.get(url, **kwargs)
            self.logger.info(f"üåê Direct response status: {response.status_code}")
            return response

    def _discover_zoneminder_real(self, url: str, config: dict) -> dict:
        """üîß FIXED: Auth first, then version check"""
        self.base_url = f"http://{url}/zm/api"
        username = config.get('username', '')
        password = config.get('password', '')
        
        self.logger.info(f"üéØ === ZONEMINDER DISCOVERY START ===")
        self.logger.info(f"üéØ Base URL: {self.base_url}")
        self.logger.info(f"üéØ Username: {username}")
        self.logger.info(f"üéØ Password provided: {'Yes' if password else 'No'}")
        
        try:
            # Step 1: Authentication FIRST (if credentials provided)
            if username and password:
                self.logger.info(f"üîê Step 1: Authenticating with credentials")
                auth_success = self._authenticate_zoneminder(username, password)
                if not auth_success:
                    error_msg = "ZoneMinder authentication failed"
                    self.logger.error(f"‚ùå {error_msg}")
                    return self._error_response(error_msg)
            elif username or password:
                error_msg = "Both username and password required for authentication"
                self.logger.error(f"‚ùå {error_msg}")
                return self._error_response(error_msg)
            else:
                self.logger.info(f"üîì Step 1: No credentials provided - skipping auth")
            
            # Step 2: Test connectivity & get version (AFTER auth)
            self.logger.info(f"üì° Step 2: Testing version endpoint")
            version_response = self._make_authenticated_request(f"{self.base_url}/host/getVersion.json", timeout=10)
            if version_response.status_code != 200:
                error_msg = f"ZoneMinder API not accessible. Status: {version_response.status_code}"
                self.logger.error(f"‚ùå {error_msg}")
                return self._error_response(error_msg)
            
            version_data = version_response.json()
            zm_version = version_data.get('version', 'Unknown')
            api_version = version_data.get('apiversion', 'Unknown')
            
            self.logger.info(f"‚úÖ ZoneMinder version: {zm_version}, API: {api_version}")
            
            # Step 3: Get real monitors with authentication
            self.logger.info(f"üìπ Step 3: Getting monitors")
            monitors_response = self._make_authenticated_request(f"{self.base_url}/monitors.json", timeout=10)
            if monitors_response.status_code != 200:
                if monitors_response.status_code == 401:
                    error_msg = "Authentication required but failed. Check username/password."
                    self.logger.error(f"‚ùå {error_msg}")
                    return self._error_response(error_msg)
                else:
                    error_msg = f"Failed to get monitors. Status: {monitors_response.status_code}"
                    self.logger.error(f"‚ùå {error_msg}")
                    return self._error_response(error_msg)
            
            monitors_data = monitors_response.json()
            monitors = monitors_data.get('monitors', [])
            
            self.logger.info(f"‚úÖ Found {len(monitors)} monitors")
            
            if not monitors:
                error_msg = "No monitors found in ZoneMinder"
                self.logger.error(f"‚ùå {error_msg}")
                return self._error_response(error_msg)
            
            # Step 4: Process real monitor data
            cameras = []
            for monitor_data in monitors:
                monitor = monitor_data.get('Monitor', {})
                monitor_status = monitor_data.get('Monitor_Status', {})
                
                monitor_id = monitor.get('Id')
                monitor_name = monitor.get('Name', f"Monitor_{monitor_id}")
                
                camera_info = {
                    "id": f"zm_monitor_{monitor_id}",
                    "name": monitor_name,
                    "description": f"ZM {monitor_name} ({monitor.get('Function', 'Record')})",
                    "zm_id": monitor_id,
                    "status": monitor_status.get('Status', 'Unknown'),
                    "resolution": f"{monitor.get('Width', 'Unknown')}x{monitor.get('Height', 'Unknown')}",
                    "function": monitor.get('Function', 'Record'),
                    "enabled": monitor.get('Enabled') == '1',
                    "type": monitor.get('Type', 'Unknown'),
                    "fps": {
                        "capture": float(monitor_status.get('CaptureFPS', '0.00')),
                        "analysis": float(monitor_status.get('AnalysisFPS', '0.00'))
                    },
                    "capabilities": self._get_zm_capabilities(monitor)
                }
                
                # Add path information
                monitor_path = monitor.get('Path', '')
                if monitor.get('Type') == 'File' and monitor_path:
                    camera_info['file_path'] = monitor_path
                elif monitor_path.startswith('rtsp://') or monitor_path.startswith('http://'):
                    camera_info['stream_url'] = monitor_path
                
                cameras.append(camera_info)
                
                self.logger.info(f"üìπ Found ZM monitor: {monitor_name} (ID: {monitor_id}, Status: {camera_info['status']})")
            
            # Step 5: Get system information
            self.logger.info(f"üîß Step 5: Getting system info")
            system_info = self._get_zoneminder_system_info()
            
            success_result = {
                "accessible": True,
                "message": f"ZoneMinder connection successful - Discovered {len(cameras)} camera(s)",
                "source_type": "nvr",
                "protocol": "zoneminder",
                "cameras": cameras,
                "device_info": {
                    "manufacturer": "ZoneMinder",
                    "model": "Open Source NVR",
                    "firmware": zm_version,
                    "api_version": api_version,
                    "disk_usage": system_info.get('disk_usage', 'Unknown'),
                    "total_cameras": len(cameras),
                    "api_url": self.base_url,
                    "auth_method": "Auth Hash" if self.auth_credentials else ("JWT Token" if self.access_token else "No Auth")
                }
            }
            
            self.logger.info(f"üéØ === ZONEMINDER DISCOVERY SUCCESS ===")
            return success_result
            
        except requests.exceptions.RequestException as e:
            error_msg = f"ZoneMinder API connection failed: {str(e)}"
            self.logger.error(f"‚ùå Request error: {error_msg}")
            return self._error_response(error_msg)
        except Exception as e:
            error_msg = f"ZoneMinder discovery failed: {str(e)}"
            self.logger.error(f"‚ùå Unexpected error: {error_msg}")
            return self._error_response(error_msg)
    
    def _get_zoneminder_system_info(self) -> dict:
        """üîß FIXED: Get ZoneMinder system information with auth"""
        system_info = {}
        
        try:
            # Get disk usage with authentication
            disk_response = self._make_authenticated_request(f"{self.base_url}/host/getDiskPercent.json", timeout=5)
            if disk_response.status_code == 200:
                disk_data = disk_response.json()
                usage = disk_data.get('usage', {})
                total_usage = usage.get('Total', {}).get('space', 'Unknown')
                system_info['disk_usage'] = f"{total_usage}% used" if total_usage != 'Unknown' else 'Unknown'
                
                self.logger.info(f"ZoneMinder disk usage: {system_info['disk_usage']}")
            
        except Exception as e:
            self.logger.warning(f"Failed to get ZoneMinder system info: {e}")
            system_info['disk_usage'] = 'Unknown'
        
        return system_info
    
    def test_connection_and_discover_cameras(self, source_data: dict) -> dict:
        """
        Universal NVR connection test and camera discovery
        Supports: ZoneMinder (Real), ONVIF (Mock), RTSP, etc.
        """
        url = source_data.get('path', '')
        config = source_data.get('config', {})
        protocol = config.get('protocol', 'onvif').lower()
        username = config.get('username', '')
        password = config.get('password', '')
        
        self.logger.info(f"Testing NVR connection to {url} using {protocol}")
        
        try:
            # Basic validation
            if not url:
                return self._error_response("NVR URL is required")
            
            # Extract host for network check
            host = self._extract_host(url)
            
            # Network connectivity check
            if not self._check_network_connectivity(host):
                return self._error_response(f"Cannot reach NVR at {host}. Check network connectivity.")
            
            # Protocol-specific discovery
            if protocol == 'zoneminder':
                return self._discover_zoneminder_real(url, config)
            elif protocol == 'onvif':
                return self._discover_onvif_real(url, config)
            elif protocol == 'rtsp':
                return self._discover_rtsp_mock(url, config)
            elif protocol == 'hikvision':
                return self._discover_vendor_mock(url, config, 'hikvision')
            elif protocol == 'dahua':
                return self._discover_vendor_mock(url, config, 'dahua')
            else:
                return self._discover_generic_mock(url, config)
                
        except Exception as e:
            self.logger.error(f"NVR connection test failed: {e}")
            return self._error_response(f"Connection test failed: {str(e)}")
    
    # ... (rest of the methods remain the same)
    
    def _get_zm_capabilities(self, monitor: dict) -> List[str]:
        """Get ZoneMinder monitor capabilities"""
        capabilities = []
        
        function = monitor.get('Function', '')
        if function in ['Record', 'Mocord']:
            capabilities.append('recording')
        if function in ['Monitor', 'Modect', 'Mocord']:
            capabilities.append('monitoring')
        if function in ['Modect', 'Mocord']:
            capabilities.append('motion_detection')
        
        if monitor.get('Controllable') == '1':
            capabilities.append('ptz')
        
        if monitor.get('RecordAudio') == '1':
            capabilities.append('audio')
            
        return capabilities
    
    def _extract_host(self, url: str) -> str:
        """Extract hostname/IP from URL"""
        url = url.replace('http://', '').replace('https://', '').replace('rtsp://', '')
        host = url.split(':')[0].split('/')[0]
        return host
    
    def _check_network_connectivity(self, host: str) -> bool:
        """Check network connectivity"""
        try:
            socket.gethostbyname(host)
            return True
        except socket.gaierror:
            # For local IPs and localhost, assume reachable
            if host.startswith('192.168.') or host.startswith('10.') or host.startswith('172.') or host in ['localhost', '127.0.0.1']:
                return True
            return False
    
    def _error_response(self, message: str) -> dict:
        """Standard error response"""
        return {
            "accessible": False,
            "message": message,
            "source_type": "nvr",
            "cameras": [],
            "device_info": {}
        }
    
    # Mock implementations for other protocols
    def _discover_onvif_mock(self, url: str, config: dict) -> dict:
        """Mock ONVIF discovery (for other NVR types)"""
        if not config.get('username') or not config.get('password'):
            return self._error_response("Username and password are required for ONVIF")
        
        host = self._extract_host(url)
        num_cameras = random.randint(2, 6)
        cameras = []
        
        camera_names = ["Front Door", "Parking", "Warehouse", "Office", "Storage", "Loading"]
        
        for i in range(num_cameras):
            cameras.append({
                "id": f"onvif_profile_{i+1}",
                "name": camera_names[i] if i < len(camera_names) else f"Camera {i+1}",
                "description": f"ONVIF Camera {i+1}",
                "stream_url": f"rtsp://{host}:554/stream{i+1}",
                "resolution": random.choice(["1920x1080", "1280x720", "2560x1440"]),
                "codec": random.choice(["H264", "H265"]),
                "capabilities": ["recording"] + (["ptz"] if i < 2 else [])
            })
        
        return {
            "accessible": True,
            "message": f"ONVIF connection successful - Discovered {num_cameras} cameras",
            "source_type": "nvr",
            "protocol": "onvif",
            "cameras": cameras,
            "device_info": {
                "manufacturer": "Generic ONVIF",
                "model": f"NVR-{num_cameras}CH",
                "firmware": f"V{random.randint(2,5)}.{random.randint(0,9)}.0"
            }
        }
    
    def _discover_rtsp_mock(self, url: str, config: dict) -> dict:
        """Mock RTSP discovery"""
        if not config.get('username') or not config.get('password'):
            return self._error_response("Username and password are required for RTSP")
        
        return {
            "accessible": True,
            "message": "RTSP connection successful - Found 2 streams",
            "source_type": "nvr",
            "protocol": "rtsp",
            "cameras": [
                {"id": "rtsp_1", "name": "Main Stream", "stream_url": f"rtsp://{url}/stream1"},
                {"id": "rtsp_2", "name": "Sub Stream", "stream_url": f"rtsp://{url}/stream2"}
            ],
            "device_info": {"manufacturer": "Generic RTSP", "model": "RTSP Server"}
        }
    
    def _discover_vendor_mock(self, url: str, config: dict, vendor: str) -> dict:
        """Mock vendor-specific discovery"""
        return {
            "accessible": False,
            "message": f"{vendor.title()} API integration coming soon. Use ONVIF protocol for now.",
            "source_type": "nvr",
            "protocol": vendor,
            "cameras": [],
            "device_info": {}
        }
    
    def _discover_generic_mock(self, url: str, config: dict) -> dict:
        """Mock generic discovery"""
        return {
            "accessible": False,
            "message": "Generic HTTP API not implemented. Try ONVIF, RTSP, or ZoneMinder protocols.",
            "source_type": "nvr",
            "protocol": "generic",
            "cameras": [],
            "device_info": {}
        }
    def _discover_onvif_real(self, url: str, config: dict) -> dict:
        """Real ONVIF discovery"""
        try:
            host = self._extract_host(url)
            port = int(config.get('port', 80))
            username = config.get('username', '')
            password = config.get('password', '')
            
            response = onvif_client.test_device_connection(host, port, username, password)
            
            # Validate cameras is array
            if 'cameras' in response and not isinstance(response['cameras'], list):
                response['cameras'] = [response['cameras']]
            
            # Handle multiple cameras and generate IDs
            if 'cameras' in response:
                for i, camera in enumerate(response['cameras']):
                    camera['id'] = f"onvif_{host}_channel_{i+1}"
            
            return response
            
        except Exception as e:
            return self._error_response(f"ONVIF l·ªói: {str(e)}")
```
## üìÑ File: `cloud_endpoints.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/cloud_endpoints.py`

```python
# modules/sources/cloud_endpoints.py - FIXED: API Routes & Session Handling
#!/usr/bin/env python3

from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import Flow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import hashlib
import json
import os
import logging
from datetime import datetime, timedelta
from flask import Blueprint, request, jsonify, session
from google.oauth2.credentials import Credentials
from flask_cors import CORS, cross_origin
from functools import wraps
from flask import g
import time
from collections import defaultdict
import jwt
import secrets
from cryptography.fernet import Fernet
import base64

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Secret key for JWT - In production, use environment variable
JWT_SECRET_KEY = os.getenv('JWT_SECRET_KEY', 'your-secret-key-change-in-production')

# Encryption key for credentials - In production, use environment variable  
ENCRYPTION_KEY = os.getenv('ENCRYPTION_KEY', Fernet.generate_key())
if isinstance(ENCRYPTION_KEY, str):
    ENCRYPTION_KEY = ENCRYPTION_KEY.encode()

def generate_session_token(user_email, user_info, expires_minutes=129600):  # 90 days = 90*24*60 = 129600 minutes
    """Generate JWT session token for V_track background service (90 days duration)"""
    try:
        payload = {
            'user_email': user_email,
            'user_name': user_info.get('name', 'Unknown'),
            'photo_url': user_info.get('photo_url'),
            'exp': datetime.utcnow() + timedelta(minutes=expires_minutes),
            'iat': datetime.utcnow(),
            'iss': 'vtrack-background-service',  # Match with config.py issuer
            'type': 'session'
        }
        
        token = jwt.encode(payload, JWT_SECRET_KEY, algorithm='HS256')
        logger.info(f"‚úÖ Generated background service session token for: {user_email} (expires in {expires_minutes}min = {expires_minutes//1440} days)")
        return token
        
    except Exception as e:
        logger.error(f"‚ùå JWT generation error: {e}")
        return None

def verify_session_token(token):
    """Verify and decode JWT session token"""
    try:
        payload = jwt.decode(token, JWT_SECRET_KEY, algorithms=['HS256'])
        return payload
    except jwt.ExpiredSignatureError:
        logger.warning("‚ö†Ô∏è Session token expired")
        return None
    except jwt.InvalidTokenError as e:
        logger.warning(f"‚ö†Ô∏è Invalid session token: {e}")
        return None

def encrypt_credentials(credentials_dict):
    """Encrypt credentials before storage"""
    try:
        fernet = Fernet(ENCRYPTION_KEY)
        credentials_json = json.dumps(credentials_dict).encode()
        encrypted_data = fernet.encrypt(credentials_json)
        return base64.b64encode(encrypted_data).decode()
    except Exception as e:
        logger.error(f"‚ùå Credential encryption error: {e}")
        return None

def decrypt_credentials(encrypted_data):
    """Decrypt stored credentials"""
    try:
        fernet = Fernet(ENCRYPTION_KEY)
        encrypted_bytes = base64.b64decode(encrypted_data.encode())
        decrypted_data = fernet.decrypt(encrypted_bytes)
        return json.loads(decrypted_data.decode())
    except Exception as e:
        logger.error(f"‚ùå Credential decryption error: {e}")
        return None

# üÜï Rate limiting storage (in-memory for simplicity)
rate_limit_storage = defaultdict(list)

# üÜï Caching storage for performance optimization
cache_storage = {}
CACHE_DURATIONS = {
    'auth_status': 300,      # 5 minutes
    'subfolders': 180,       # 3 minutes
    'user_info': 600,        # 10 minutes
}

def get_cache_key(endpoint, *args):
    """Generate cache key"""
    key_parts = [endpoint] + [str(arg) for arg in args]
    return hashlib.sha256(':'.join(key_parts).encode()).hexdigest()[:16]

def get_cached_data(cache_key):
    """Get cached data if valid"""
    if cache_key in cache_storage:
        cached_item = cache_storage[cache_key]
        if time.time() < cached_item['expires_at']:
            return cached_item['data']
        else:
            # Remove expired cache
            del cache_storage[cache_key]
    return None

def set_cached_data(cache_key, data, cache_type='default'):
    """Cache data with expiration"""
    duration = CACHE_DURATIONS.get(cache_type, 300)
    cache_storage[cache_key] = {
        'data': data,
        'expires_at': time.time() + duration,
        'created_at': time.time()
    }

RATE_LIMITS = {
    'auth_status': {'calls': 30, 'window': 60},   # 30 calls per minute  
    'default': {'calls': 60, 'window': 60}        # 60 calls per minute default
}

def rate_limit(endpoint_type='default'):
    """Rate limiting decorator"""
    def decorator(f):
        @wraps(f)
        def decorated_function(*args, **kwargs):
            client_ip = request.remote_addr
            current_time = time.time()
            
            # Get rate limit config
            limit_config = RATE_LIMITS.get(endpoint_type, RATE_LIMITS['default'])
            max_calls = limit_config['calls']
            time_window = limit_config['window']
            
            # Clean old entries
            cutoff_time = current_time - time_window
            rate_limit_storage[client_ip] = [
                call_time for call_time in rate_limit_storage[client_ip] 
                if call_time > cutoff_time
            ]
            
            # Check rate limit
            if len(rate_limit_storage[client_ip]) >= max_calls:
                logger.warning(f"üö´ Rate limit exceeded for {client_ip} on {endpoint_type}")
                return jsonify({
                    'success': False,
                    'message': f'Rate limit exceeded. Max {max_calls} calls per {time_window} seconds.',
                    'retry_after': int(time_window - (current_time - rate_limit_storage[client_ip][0]))
                }), 429
            
            # Record this call
            rate_limit_storage[client_ip].append(current_time)
            
            return f(*args, **kwargs)
        return decorated_function
    return decorator

# Create Blueprint for cloud endpoints
cloud_bp = Blueprint('cloud', __name__, url_prefix='/api/cloud')
# üîß FIX: Add CORS to cloud blueprint
CORS(cloud_bp, 
     origins=['http://localhost:3000', 'http://127.0.0.1:3000'],
     supports_credentials=True,
     allow_headers=['Content-Type', 'Authorization', 'X-Requested-With'],
     methods=['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'])

@cloud_bp.route('/authenticate', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
def cloud_authenticate():
    """Google OAuth2 authentication - FIXED for multiple environments"""
    try:
        data = request.get_json()
        provider = data.get('provider', 'google_drive')
        action = data.get('action', 'initiate_auth')
        
        # üÜï NEW: Get redirect URI from request or determine automatically
        custom_redirect = data.get('redirect_uri')
        
        logger.info(f"üîê Cloud authentication request: {provider}, action: {action}")
        
        if action == 'initiate_auth':
            CLIENT_SECRETS_FILE = os.path.join(
                os.path.dirname(__file__), 
                'credentials/google_drive_credentials_web.json'
            )
            
            if not os.path.exists(CLIENT_SECRETS_FILE):
                return jsonify({
                    'success': False,
                    'message': f'Credentials file not found: {CLIENT_SECRETS_FILE}',
                    'setup_required': True
                }), 400
            
            SCOPES = [
                'https://www.googleapis.com/auth/drive.file',
                'https://www.googleapis.com/auth/drive.readonly',
                'https://www.googleapis.com/auth/drive.metadata.readonly'
            ]
            
            # Create flow
            flow = Flow.from_client_secrets_file(
                CLIENT_SECRETS_FILE, 
                scopes=SCOPES
            )
            
            # üîß FIX: Determine redirect URI dynamically
            if custom_redirect:
                redirect_uri = custom_redirect
            else:
                # Auto-detect based on request headers
                host = request.headers.get('Host', 'localhost:8080')
                if '127.0.0.1' in host:
                    redirect_uri = 'http://127.0.0.1:8080/api/cloud/oauth/callback'
                else:
                    redirect_uri = 'http://localhost:8080/api/cloud/oauth/callback'
            
            flow.redirect_uri = redirect_uri
            
            # üîê PHASE 1 SECURITY: Enhanced CSRF protection with cryptographically secure random state
            state = secrets.token_urlsafe(32)  # More secure than default
            
            # Generate authorization URL
            authorization_url, _ = flow.authorization_url(
                access_type='offline',
                include_granted_scopes='true',
                prompt='consent',
                state=state  # Use secure state
            )
            
            # üîê Store secure state in session with timestamp
            session['oauth2_state'] = state
            session['oauth2_state_created'] = datetime.now().isoformat()
            session['oauth2_flow_data'] = {
                'scopes': SCOPES,
                'redirect_uri': flow.redirect_uri,
                'client_secrets_file': CLIENT_SECRETS_FILE,
                'csrf_token': secrets.token_hex(16)  # Additional CSRF protection
            }
            session.permanent = True
            
            logger.info(f"‚úÖ OAuth flow initiated")
            logger.info(f"   Auth URL: {authorization_url}")
            logger.info(f"   Redirect URI: {flow.redirect_uri}")
            
            return jsonify({
                'success': True,
                'auth_url': authorization_url,
                'state': state,
                'redirect_uri': flow.redirect_uri,
                'message': 'OAuth flow initiated - open popup window'
            }), 200
            
        else:
            return jsonify({
                'success': False,
                'message': f'Unknown action: {action}'
            }), 400
            
    except Exception as e:
        logger.error(f"‚ùå Cloud authentication error: {e}")
        return jsonify({
            'success': False,
            'message': f'Cloud authentication failed: {str(e)}'
        }), 500

@cloud_bp.route('/oauth/callback', methods=['GET', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
def cloud_oauth_callback():
    """OAuth2 callback handler - FIXED for session management and CORS"""
    try:
        logger.info("üîÑ Processing OAuth callback...")
        logger.info(f"   Request URL: {request.url}")
        logger.info(f"   Session ID: {session.get('_id', 'no-session')}")
        
        # Get OAuth parameters
        code = request.args.get('code')
        state = request.args.get('state')
        error = request.args.get('error')
        error_description = request.args.get('error_description')
        
        # Handle errors
        if error:
            logger.error(f"‚ùå OAuth error: {error}")
            return _create_error_page(f"OAuth error: {error}", error_description)
        
        if not code or not state:
            logger.error("‚ùå Missing required OAuth parameters")
            return _create_error_page("Missing OAuth parameters", "Authorization code or state missing")
        
        # üîß FIX: More flexible state verification
        stored_state = session.get('oauth2_state')
        if not stored_state:
            logger.warning("‚ö†Ô∏è No stored state found, but proceeding (session might have expired)")
            # Don't fail immediately - try to continue with OAuth
        elif state != stored_state:
            logger.error(f"‚ùå State mismatch: got {state}, expected {stored_state}")
            return _create_error_page("Security verification failed", "Please try authenticating again")
        
        # Get flow data (with fallback)
        flow_data = session.get('oauth2_flow_data')
        if not flow_data:
            logger.warning("‚ö†Ô∏è No flow data, using defaults")
            flow_data = {
                'scopes': [
                    'https://www.googleapis.com/auth/drive.file',
                    'https://www.googleapis.com/auth/drive.readonly',
                    'https://www.googleapis.com/auth/drive.metadata.readonly'
                ],
                'redirect_uri': 'http://localhost:8080/api/cloud/oauth/callback',
                'client_secrets_file': os.path.join(
                    os.path.dirname(__file__), 
                    'credentials/google_drive_credentials_web.json'
                )
            }
        
        # Recreate flow
        flow = Flow.from_client_secrets_file(
            flow_data['client_secrets_file'], 
            scopes=flow_data['scopes']
        )
        flow.redirect_uri = flow_data['redirect_uri']
        
        logger.info(f"   Using redirect URI: {flow.redirect_uri}")
        
        # Exchange code for tokens
        try:
            logger.info(f"üîÑ Attempting token exchange...")
            flow.fetch_token(code=code)
            credentials = flow.credentials
            logger.info("‚úÖ Token exchange successful")
        except Exception as token_error:
            logger.error(f"‚ùå Token exchange failed: {token_error}")
            return _create_error_page("Token exchange failed", str(token_error))
        
        # Get user info
        try:
            service = build('drive', 'v3', credentials=credentials)
            about = service.about().get(fields='user,storageQuota').execute()
            
            user_info = {
                'email': about.get('user', {}).get('emailAddress', 'unknown'),
                'name': about.get('user', {}).get('displayName', 'Unknown User'),
                'photo_url': about.get('user', {}).get('photoLink'),
            }
            logger.info(f"‚úÖ User info retrieved: {user_info['email']}")
            
        except Exception as user_error:
            logger.error(f"‚ùå Failed to get user info: {user_error}")
            user_info = {'email': 'unknown', 'name': 'Unknown User'}
        
        # üîê PHASE 1 SECURITY: Store credentials safely and return session token only
        session_credentials = None
        try:
            session_credentials = _store_credentials_safely(credentials, user_info)
            logger.info("‚úÖ Credentials encrypted and session token generated")
        except Exception as storage_error:
            logger.error(f"‚ùå Failed to store credentials securely: {storage_error}")
            return _create_error_page("Secure storage failed", str(storage_error))

        # üîê SECURE SESSION RESULT - NO RAW CREDENTIALS
        session_result = {
            'success': True,
            'authenticated': True,
            'user_info': user_info,
            'user_email': user_info['email'],
            'session_token': session_credentials['session_token'],  # JWT token only
            'folders': [],  # Use loaded folders
            'folder_loading_required': True,
            'lazy_loading_enabled': True,
            'existing_auth': False,
            'message': f'Authentication completed for {user_info["email"]}',
            'backend_port': 8080,
            'timestamp': datetime.now().isoformat(),
            'security_mode': 'encrypted_storage'  # Indicate security enhancement
            # ‚ùå REMOVED: 'credentials' field - no longer exposed to frontend
        }    
            # Store in session with longer lifetime
        session['auth_result'] = session_result
        session.permanent = True
        
        # üîß FIX: Clear OAuth session data
        session.pop('oauth2_state', None)
        session.pop('oauth2_flow_data', None)
        
        logger.info(f"‚úÖ OAuth completed successfully for: {user_info['email']}")
        
        # Return success page with postMessage
        return _create_success_page_with_postmessage(session_result)
        
    except Exception as e:
        logger.error(f"‚ùå OAuth callback error: {e}")
        import traceback
        traceback.print_exc()
        return _create_error_page("Authentication error", str(e))

def _store_credentials_safely(credentials, user_info):
    """Store credentials safely with AES-256 encryption"""
    try:
        tokens_dir = os.path.join(os.path.dirname(__file__), 'tokens')
        os.makedirs(tokens_dir, exist_ok=True)
        
        email_hash = hashlib.sha256(user_info['email'].encode()).hexdigest()[:16]
        token_filename = f"google_drive_{email_hash}.json"
        token_filepath = os.path.join(tokens_dir, token_filename)
        
        # Prepare credential data
        credential_data = {
            'token': credentials.token,
            'refresh_token': credentials.refresh_token,
            'token_uri': credentials.token_uri,
            'client_id': credentials.client_id,
            'client_secret': credentials.client_secret,
            'scopes': list(credentials.scopes) if credentials.scopes else [],
            'user_info': user_info,
            'created_at': datetime.now().isoformat(),
            'expires_at': credentials.expiry.isoformat() if credentials.expiry else None
        }
        
        # üîê PHASE 1 SECURITY: Encrypt credentials before storage
        encrypted_credentials = encrypt_credentials(credential_data)
        if not encrypted_credentials:
            raise Exception("Failed to encrypt credentials")
        
        # Store encrypted credentials
        encrypted_storage = {
            'encrypted_data': encrypted_credentials,
            'user_email': user_info['email'],  # Keep email unencrypted for lookup
            'created_at': datetime.now().isoformat(),
            'encryption_version': '1.0'
        }
        
        with open(token_filepath, 'w') as f:
            json.dump(encrypted_storage, f, indent=2)
        
        # Set restrictive permissions (600 = owner read/write only)
        os.chmod(token_filepath, 0o600)
        logger.info(f"‚úÖ Encrypted credentials stored to: {token_filepath}")
        
        # üîê PHASE 1: Return session data instead of raw credentials
        return {
            'session_token': generate_session_token(user_info['email'], user_info),
            'user_email': user_info['email'],
            'user_name': user_info.get('name', 'Unknown'),
            'photo_url': user_info.get('photo_url'),
            'encrypted_storage_path': token_filepath
        }
        
    except Exception as e:
        logger.error(f"‚ùå Secure credential storage error: {e}")
        raise
    
def _create_success_page_with_postmessage(session_result):
    """Success page with postMessage for COOP compatibility"""
    return f"""
    <!DOCTYPE html>
    <html>
        <head>
            <title>VTrack - Authentication Successful</title>
            <meta charset="utf-8">
            <style>
                body {{
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                    text-align: center;
                    padding: 50px;
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    margin: 0;
                    min-height: 100vh;
                }}
                .container {{
                    background: white;
                    color: #333;
                    padding: 40px;
                    border-radius: 15px;
                    display: inline-block;
                    box-shadow: 0 20px 40px rgba(0,0,0,0.1);
                    max-width: 500px;
                }}
                .success-icon {{ font-size: 4em; margin-bottom: 20px; }}
                .user-info {{
                    background: #d4edda;
                    border: 1px solid #c3e6cb;
                    padding: 20px;
                    margin: 20px 0;
                    border-radius: 8px;
                    text-align: left;
                }}
                .status {{
                    background: #cce5ff;
                    border: 1px solid #99ccff;
                    padding: 15px;
                    margin: 15px 0;
                    border-radius: 8px;
                    text-align: left;
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="success-icon">‚úÖ</div>
                <h1 style="color: #28a745;">Authentication Successful!</h1>
                
                <div class="user-info">
                    <h3>Google Drive Connected (Secure Mode)</h3>
                    <p><strong>Account:</strong> {session_result['user_email']}</p>
                    <p><strong>Name:</strong> {session_result['user_info']['name']}</p>
                    <p><strong>Security:</strong> Encrypted storage, Session tokens</p>
                    <p><strong>Backend:</strong> localhost:8080</p>
                </div>
                
                <div class="status" id="status">
                    <strong>Status:</strong> <span id="statusText">Notifying VTrack...</span>
                </div>
                
                <p style="color: #28a745; font-weight: bold;">
                    üéâ You can now close this window and return to VTrack!
                </p>
                
                <p style="font-size: 0.9em; color: #666;">
                    This window will close automatically in <span id="countdown">10</span> seconds.
                </p>
                
                <button onclick="window.close()" style="padding: 10px 20px; background: #007bff; color: white; border: none; border-radius: 5px; cursor: pointer; margin: 10px;">
                    Close Window
                </button>
            </div>
            
            <script>
                console.log('‚úÖ OAuth success page loaded');
                
                const statusEl = document.getElementById('statusText');
                const countdownEl = document.getElementById('countdown');
                
                // üîê PHASE 1 SECURITY: Prepare SECURE session data (NO CREDENTIALS)
                const authData = {{
                    success: {str(session_result.get('success', True)).lower()},
                    authenticated: {str(session_result.get('authenticated', True)).lower()},
                    user_email: "{session_result.get('user_email', '')}",
                    user_info: {json.dumps(session_result.get('user_info', {}))},
                    session_token: "{session_result.get('session_token', '')}",
                    folders: {json.dumps(session_result.get('folders', []))},
                    folder_loading_required: {str(session_result.get('folder_loading_required', True)).lower()},
                    lazy_loading_enabled: {str(session_result.get('lazy_loading_enabled', True)).lower()},
                    existing_auth: {str(session_result.get('existing_auth', False)).lower()},
                    message: "{session_result.get('message', '')}",
                    backend_port: {session_result.get('backend_port', 8080)},
                    timestamp: "{session_result.get('timestamp', '')}",
                    security_mode: "{session_result.get('security_mode', 'encrypted_storage')}"
                    // ‚ùå REMOVED: credentials field - no longer exposed to frontend
                }};
                
                // Function to notify parent window
                function notifyParent() {{
                    try {{
                        if (window.opener && !window.opener.closed) {{
                            console.log('üì¨ Sending SECURE success message to parent window');
                            
                            // üîß FIX: Send to both localhost:3000 and 127.0.0.1:3000
                            const origins = [
                                'http://localhost:3000',
                                'http://127.0.0.1:3000'
                            ];
                            
                            origins.forEach(origin => {{
                                window.opener.postMessage({{
                                    type: 'OAUTH_SUCCESS',
                                    ...authData
                                }}, origin);
                            }});
                            
                            statusEl.textContent = 'VTrack notified successfully! (Secure mode)';
                            statusEl.style.color = '#28a745';
                        }} else {{
                            console.log('‚ö†Ô∏è Parent window not available');
                            statusEl.textContent = 'Parent window not found';
                            statusEl.style.color = '#856404';
                        }}
                    }} catch (error) {{
                        console.error('‚ùå Error notifying parent:', error);
                        statusEl.textContent = 'Error notifying VTrack';
                        statusEl.style.color = '#dc3545';
                    }}
                }}
                
                // Auto-close countdown
                let countdown = 10;
                const countdownInterval = setInterval(() => {{
                    countdown--;
                    countdownEl.textContent = countdown;
                    if (countdown <= 0) {{
                        clearInterval(countdownInterval);
                        window.close();
                    }}
                }}, 1000);
                
                // Immediate notification
                notifyParent();
                
                // Auto-close after delay
                setTimeout(() => {{
                    try {{
                        window.close();
                    }} catch (e) {{
                        console.log('üìù Note: Could not auto-close window');
                    }}
                }}, 10000);
            </script>
        </body>
    </html>
    """

def _create_error_page(error_message, error_details=None):
    """Error page for port 8080"""
    return f"""
    <!DOCTYPE html>
    <html>
        <head>
            <title>VTrack - Authentication Failed</title>
            <meta charset="utf-8">
            <style>
                body {{
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                    text-align: center;
                    padding: 50px;
                    background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%);
                    color: white;
                    margin: 0;
                    min-height: 100vh;
                }}
                .container {{
                    background: white;
                    color: #333;
                    padding: 40px;
                    border-radius: 15px;
                    display: inline-block;
                    box-shadow: 0 20px 40px rgba(0,0,0,0.1);
                }}
                .error-icon {{ font-size: 4em; margin-bottom: 20px; }}
                .error-details {{
                    background: #f8d7da;
                    border: 1px solid #f5c6cb;
                    padding: 15px;
                    margin: 20px 0;
                    border-radius: 8px;
                    text-align: left;
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="error-icon">‚ùå</div>
                <h1 style="color: #dc3545;">Authentication Failed</h1>
                
                <div class="error-details">
                    <h4>Error Details:</h4>
                    <p><strong>Message:</strong> {error_message}</p>
                    {f"<p><strong>Details:</strong> {error_details}</p>" if error_details else ""}
                    <p><strong>Backend:</strong> localhost:8080</p>
                </div>
                
                <p>Please try again or contact support.</p>
                <button onclick="window.close()" style="padding: 10px 20px; background: #007bff; color: white; border: none; border-radius: 5px; cursor: pointer;">
                    Close Window
                </button>
            </div>
            
            <script>
                // Notify parent window of error
                if (window.opener) {{
                    const origins = ['http://localhost:3000', 'http://127.0.0.1:3000'];
                    origins.forEach(origin => {{
                        window.opener.postMessage({{
                            type: 'OAUTH_ERROR',
                            error: '{error_message}',
                            details: '{error_details or ""}',
                            backend_port: 8080
                        }}, origin);
                    }});
                }}
                
                setTimeout(() => window.close(), 10000);
            </script>
        </body>
    </html>
    """, 400

# üÜï NEW: Auth status check with lazy loading support
@cloud_bp.route('/auth-status', methods=['GET', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@rate_limit('auth_status')
def auth_status():
    """üîê PHASE 1 SECURITY: Check authentication status using session tokens"""
    
    if request.method == 'OPTIONS':
        response = jsonify({'status': 'ok'})
        response.headers.add('Access-Control-Allow-Origin', 'http://localhost:3000')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')
        response.headers.add('Access-Control-Allow-Methods', 'GET,OPTIONS')
        response.headers.add('Access-Control-Allow-Credentials', 'true')
        return response
    
    try:
        cache_key = get_cache_key('auth_status', session.get('_id', 'anonymous'))
        cached_result = get_cached_data(cache_key)
        
        if cached_result:
            logger.debug("üìã Auth status cache hit")
            return jsonify(cached_result), 200
        
        # üîê PHASE 1: Get session token from request headers or session
        session_token = request.headers.get('Authorization', '').replace('Bearer ', '') or session.get('session_token')
        
        if session_token:
            # Verify session token
            token_payload = verify_session_token(session_token)
            if token_payload:
                # Valid session token
                result = {
                    'success': True,
                    'authenticated': True,
                    'user_email': token_payload.get('user_email'),
                    'user_info': {
                        'email': token_payload.get('user_email'),
                        'name': token_payload.get('user_name'),
                        'photo_url': token_payload.get('photo_url')
                    },
                    'session_token': session_token,  # Return current token
                    'folders': [],  # Empty - use separate endpoint
                    'folder_loading_required': True,
                    'lazy_loading_enabled': True,
                    'message': f"Authenticated as {token_payload.get('user_email')}",
                    'existing_auth': True,
                    'backend_port': 8080,
                    'security_mode': 'session_based',
                    'token_expires': token_payload.get('exp')
                    # ‚ùå REMOVED: 'credentials' field - no longer exposed
                }
                
                set_cached_data(cache_key, result, 'auth_status')
                return jsonify(result), 200
            else:
                # Invalid or expired token
                result = {
                    'success': False,
                    'authenticated': False,
                    'message': 'Session token invalid or expired',
                    'lazy_loading_enabled': False,
                    'requires_reauth': True,
                    'security_mode': 'session_based'
                }
        else:
            # No session token found
            result = {
                'success': False,
                'authenticated': False,
                'message': 'No authentication session found',
                'lazy_loading_enabled': False,
                'security_mode': 'session_based'
            }
        
        set_cached_data(cache_key, result, 'auth_status')
        return jsonify(result), 200
        
    except Exception as e:
        logger.error(f"‚ùå Auth status error: {e}")
        return jsonify({
            'success': False,
            'authenticated': False,
            'message': f'Auth status check failed: {str(e)}',
            'security_mode': 'session_based'
        }), 500

# üÜï NEW: Folder initialization endpoint
@cloud_bp.route('/folders/initialize', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@rate_limit('folder_discovery')
def initialize_folder_tree():
    """
    Initialize folder tree after authentication
    Called separately from auth flow for better performance
    """
    try:
        # üîí SECURITY: Verify session token
        session_token = request.headers.get('Authorization', '').replace('Bearer ', '')
        if not session_token:
            return jsonify({
                'success': False,
                'message': 'Session token required',
                'requires_auth': True
            }), 401

        # Verify session token
        token_payload = verify_session_token(session_token)
        if not token_payload:
            return jsonify({
                'success': False,
                'message': 'Invalid or expired session token',
                'requires_reauth': True
            }), 401

        user_email = token_payload.get('user_email')
        
        # Load encrypted credentials for this user
        credentials = load_encrypted_credentials_for_user(user_email)
        if not credentials:
            return jsonify({
                'success': False,
                'message': 'No valid credentials found',
                'requires_auth': True
            }), 401
        
        # Get request parameters
        data = request.get_json() or {}
        max_folders = min(data.get('max_folders', 30), 50)  # Default 30, max 50
        
        logger.info(f"üìÇ Loading initial folder tree (max: {max_folders}) for: {user_email}")
        
        # Initialize Google Drive service
        service = build('drive', 'v3', credentials=credentials)
        
        # Get root level folders
        query = "mimeType='application/vnd.google-apps.folder' and 'root' in parents and trashed=false"
        results = service.files().list(
            q=query,
            fields="files(id, name, parents, createdTime)",
            pageSize=max_folders,
            orderBy='name'
        ).execute()
        
        folders = results.get('files', [])
        
        # Format for tree component
        tree_folders = []
        for folder in folders:
            tree_folder = {
                'id': folder['id'],
                'name': folder['name'],
                'type': 'folder',
                'parent_id': 'root',
                'depth': 1,
                'selectable': False,  # Only depth 4 is selectable
                'has_subfolders': True,  # Assume true, check on expand
                'created': folder.get('createdTime'),
                'path': f"/My Drive/{folder['name']}",
                'loaded': False  # Mark as not fully loaded
            }
            tree_folders.append(tree_folder)
        
        response_data = {
            'success': True,
            'folders': tree_folders,
            'total_count': len(tree_folders),
            'user_email': user_email,
            'tree_initialized': True,
            'lazy_loading': True,
            'max_depth': 4,
            'selectable_depth': 4,
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"‚úÖ Folder tree initialized: {len(tree_folders)} root folders")
        return jsonify(response_data), 200
        
    except Exception as e:
        logger.error(f"‚ùå Folder tree initialization error: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to initialize folder tree: {str(e)}',
            'error_type': type(e).__name__
        }), 500

# üÜï NEW: List subfolders endpoint
@cloud_bp.route('/folders/list_subfolders', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@rate_limit('folder_discovery')
def list_subfolders():
    """List subfolders for a given parent folder"""
    try:
        # üîí SECURITY: Verify session token
        session_token = request.headers.get('Authorization', '').replace('Bearer ', '')
        if not session_token:
            return jsonify({
                'success': False,
                'message': 'Session token required',
                'requires_auth': True
            }), 401

        # Verify session token
        token_payload = verify_session_token(session_token)
        if not token_payload:
            return jsonify({
                'success': False,
                'message': 'Invalid or expired session token',
                'requires_reauth': True
            }), 401

        user_email = token_payload.get('user_email')
        
        # Load encrypted credentials for this user
        credentials = load_encrypted_credentials_for_user(user_email)
        if not credentials:
            return jsonify({
                'success': False,
                'message': 'No valid credentials found',
                'requires_auth': True
            }), 401
        
        # Get request parameters
        data = request.get_json() or {}
        parent_id = data.get('parent_id', 'root')
        max_results = min(data.get('max_results', 50), 100)
        
        logger.info(f"üìÇ Loading subfolders for: {parent_id} (user: {user_email})")
        
        # Initialize Google Drive service
        service = build('drive', 'v3', credentials=credentials)
        
        # Get subfolders
        query = f"mimeType='application/vnd.google-apps.folder' and '{parent_id}' in parents and trashed=false"
        results = service.files().list(
            q=query,
            fields="files(id, name, parents, createdTime)",
            pageSize=max_results,
            orderBy='name'
        ).execute()
        
        folders = results.get('files', [])
        
        # Format for tree component (determine depth from parent)
        parent_depth = 0  # TODO: Could track depth in request if needed
        tree_folders = []
        for folder in folders:
            tree_folder = {
                'id': folder['id'],
                'name': folder['name'],
                'type': 'folder',
                'parent_id': parent_id,
                'depth': parent_depth + 1,
                'selectable': (parent_depth + 1) >= 3,  # Selectable at depth 3+
                'has_subfolders': True,  # Assume true for now
                'created': folder.get('createdTime'),
                'path': f"/{folder['name']}",  # Simplified path
                'loaded': False
            }
            tree_folders.append(tree_folder)
        
        response_data = {
            'success': True,
            'folders': tree_folders,
            'total_count': len(tree_folders),
            'parent_id': parent_id,
            'user_email': user_email,
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"‚úÖ Loaded {len(tree_folders)} subfolders for {parent_id}")
        return jsonify(response_data), 200
        
    except Exception as e:
        logger.error(f"‚ùå List subfolders error: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to list subfolders: {str(e)}',
            'error_type': type(e).__name__
        }), 500

# üÜï NEW: Disconnect endpoint
@cloud_bp.route('/disconnect', methods=['POST'])
def cloud_disconnect():
    """Disconnect from cloud provider"""
    try:
        data = request.get_json()
        provider = data.get('provider', 'google_drive')
        user_email = data.get('user_email')
        
        logger.info(f"üîå Disconnecting {provider} for {user_email}")
        
        # Clear session
        session.pop('auth_result', None)
        session.pop('oauth2_state', None)
        session.pop('oauth2_flow_data', None)
        
        # Clear cache
        cache_storage.clear()
        
        # Optionally remove stored token file
        if user_email:
            try:
                tokens_dir = os.path.join(os.path.dirname(__file__), 'tokens')
                email_hash = hashlib.sha256(user_email.encode()).hexdigest()[:16]
                token_filename = f"google_drive_{email_hash}.json"
                token_filepath = os.path.join(tokens_dir, token_filename)
                
                if os.path.exists(token_filepath):
                    os.remove(token_filepath)
                    logger.info(f"üóëÔ∏è Removed token file: {token_filename}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Could not remove token file: {e}")
        
        return jsonify({
            'success': True,
            'message': f'Successfully disconnected from {provider}',
            'provider': provider
        }), 200
        
    except Exception as e:
        logger.error(f"‚ùå Disconnect error: {e}")
        return jsonify({
            'success': False,
            'message': f'Disconnect failed: {str(e)}'
        }), 500

def load_encrypted_credentials_for_user(user_email):
    """Load and decrypt credentials for backend operations"""
    try:
        tokens_dir = os.path.join(os.path.dirname(__file__), 'tokens')
        email_hash = hashlib.sha256(user_email.encode()).hexdigest()[:16]
        token_filename = f"google_drive_{email_hash}.json"
        token_filepath = os.path.join(tokens_dir, token_filename)
        
        if not os.path.exists(token_filepath):
            logger.warning(f"‚ö†Ô∏è No encrypted credentials found for: {user_email}")
            return None
        
        # Load encrypted storage
        with open(token_filepath, 'r') as f:
            encrypted_storage = json.load(f)
        
        # Decrypt credentials
        credential_data = decrypt_credentials(encrypted_storage['encrypted_data'])
        if not credential_data:
            logger.error(f"‚ùå Failed to decrypt credentials for: {user_email}")
            return None
        
        # Reconstruct credentials object
        credentials = Credentials(
            token=credential_data['token'],
            refresh_token=credential_data['refresh_token'],
            token_uri=credential_data['token_uri'],
            client_id=credential_data['client_id'],
            client_secret=credential_data['client_secret'],
            scopes=credential_data['scopes']
        )
        
        # Refresh if expired
        if credentials.expired and credentials.refresh_token:
            logger.info("üîÑ Refreshing expired credentials...")
            credentials.refresh(Request())
            
            # Update stored credentials with new token
            credential_data['token'] = credentials.token
            credential_data['expires_at'] = credentials.expiry.isoformat() if credentials.expiry else None
            
            encrypted_updated = encrypt_credentials(credential_data)
            if encrypted_updated:
                encrypted_storage['encrypted_data'] = encrypted_updated
                with open(token_filepath, 'w') as f:
                    json.dump(encrypted_storage, f, indent=2)
                os.chmod(token_filepath, 0o600)
        
        logger.info(f"‚úÖ Loaded encrypted credentials for: {user_email}")
        return credentials
        
    except Exception as e:
        logger.error(f"‚ùå Error loading encrypted credentials: {e}")
        return None

def get_credentials_from_session():
    """Get credentials using session token (for API endpoints)"""
    try:
        # Get session token from request headers or session
        session_token = request.headers.get('Authorization', '').replace('Bearer ', '') or session.get('session_token')
        
        if not session_token:
            return None
        
        # Verify session token
        token_payload = verify_session_token(session_token)
        if not token_payload:
            return None
        
        user_email = token_payload.get('user_email')
        if not user_email:
            return None
        
        # Load encrypted credentials for this user
        return load_encrypted_credentials_for_user(user_email)
        
    except Exception as e:
        logger.error(f"‚ùå Error getting credentials from session: {e}")
        return None
```
## üìÑ File: `sync_endpoints.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/sync_endpoints.py`

```python
#!/usr/bin/env python3
"""
Sync Endpoints for VTrack - Missing API endpoints for auto-sync functionality
Exposes PyDriveDownloader methods as REST APIs
"""

import logging
from flask import Blueprint, request, jsonify
from typing import Dict, Any

# Import the PyDrive downloader
from .pydrive_downloader import pydrive_downloader, start_source_sync, stop_source_sync, force_sync_source
from database import get_sync_status

logger = logging.getLogger(__name__)

# Create blueprint for sync endpoints
sync_bp = Blueprint('sync', __name__)

@sync_bp.route('/start-auto-sync', methods=['POST'])
def start_auto_sync():
    """Start auto-sync for a cloud source"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'message': 'No data provided'
            }), 400
        
        source_id = data.get('source_id')
        if not source_id:
            return jsonify({
                'success': False,
                'message': 'source_id is required'
            }), 400
        
        # Start auto-sync using PyDriveDownloader
        success = start_source_sync(source_id)
        
        if success:
            # Get initial status
            status = get_sync_status(source_id)
            return jsonify({
                'success': True,
                'message': f'Auto-sync started for source {source_id}',
                'source_id': source_id,
                'sync_status': status
            }), 200
        else:
            return jsonify({
                'success': False,
                'message': f'Failed to start auto-sync for source {source_id}'
            }), 500
            
    except Exception as e:
        logger.error(f"‚ùå Error starting auto-sync: {e}")
        return jsonify({
            'success': False,
            'message': f'Error starting auto-sync: {str(e)}'
        }), 500

@sync_bp.route('/stop-auto-sync', methods=['POST'])
def stop_auto_sync():
    """Stop auto-sync for a source"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'message': 'No data provided'
            }), 400
        
        source_id = data.get('source_id')
        if not source_id:
            return jsonify({
                'success': False,
                'message': 'source_id is required'
            }), 400
        
        # Stop auto-sync using PyDriveDownloader
        success = stop_source_sync(source_id)
        
        if success:
            return jsonify({
                'success': True,
                'message': f'Auto-sync stopped for source {source_id}',
                'source_id': source_id
            }), 200
        else:
            return jsonify({
                'success': False,
                'message': f'Failed to stop auto-sync for source {source_id}'
            }), 500
            
    except Exception as e:
        logger.error(f"‚ùå Error stopping auto-sync: {e}")
        return jsonify({
            'success': False,
            'message': f'Error stopping auto-sync: {str(e)}'
        }), 500

@sync_bp.route('/sync-status/<int:source_id>', methods=['GET'])
def get_sync_status_api(source_id: int):
    """Get sync status for a source"""
    try:
        # Get sync status from database
        status = get_sync_status(source_id)
        
        if status:
            # Add runtime information
            is_running = source_id in pydrive_downloader.sync_timers
            
            response_data = {
                'success': True,
                'source_id': source_id,
                'sync_status': status,
                'runtime': {
                    'is_running': is_running,
                    'timer_active': is_running
                }
            }
            
            return jsonify(response_data), 200
        else:
            return jsonify({
                'success': False,
                'message': f'No sync status found for source {source_id}'
            }), 404
            
    except Exception as e:
        logger.error(f"‚ùå Error getting sync status: {e}")
        return jsonify({
            'success': False,
            'message': f'Error getting sync status: {str(e)}'
        }), 500

@sync_bp.route('/force-sync/<int:source_id>', methods=['POST'])
def force_sync_api(source_id: int):
    """Force immediate sync for a source"""
    try:
        logger.info(f"üöÄ Force sync requested for source {source_id}")
        
        # Trigger immediate sync using PyDriveDownloader
        result = force_sync_source(source_id)
        
        if result['success']:
            return jsonify({
                'success': True,
                'message': f'Sync completed for source {source_id}',
                'source_id': source_id,
                'sync_result': result
            }), 200
        else:
            return jsonify({
                'success': False,
                'message': f'Sync failed for source {source_id}',
                'source_id': source_id,
                'error': result.get('message', 'Unknown error')
            }), 500
            
    except Exception as e:
        logger.error(f"‚ùå Error forcing sync: {e}")
        return jsonify({
            'success': False,
            'message': f'Error forcing sync: {str(e)}'
        }), 500

@sync_bp.route('/sync-dashboard', methods=['GET'])
def get_sync_dashboard():
    """Get comprehensive sync dashboard data"""
    try:
        from modules.db_utils import get_db_connection
        
        # Get all cloud sources with sync status
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT 
                vs.id as source_id,
                vs.name as source_name,
                vs.source_type,
                vs.path as source_path,
                ss.sync_enabled,
                ss.last_sync_timestamp,
                ss.next_sync_timestamp,
                ss.sync_interval_minutes,
                ss.last_sync_status,
                ss.last_sync_message,
                ss.files_downloaded_count,
                ss.total_download_size_mb
            FROM video_sources vs
            LEFT JOIN sync_status ss ON vs.id = ss.source_id
            WHERE vs.active = 1 AND vs.source_type = 'cloud'
            ORDER BY vs.name
        """)
        
        results = cursor.fetchall()
        conn.close()
        
        # Format dashboard data
        dashboard_data = []
        for row in results:
            source_id = row[0]
            is_running = source_id in pydrive_downloader.sync_timers
            
            source_data = {
                'source_id': source_id,
                'source_name': row[1],
                'source_type': row[2],
                'source_path': row[3],
                'sync_enabled': bool(row[4]) if row[4] is not None else False,
                'last_sync_timestamp': row[5],
                'next_sync_timestamp': row[6],
                'sync_interval_minutes': row[7] or 15,
                'last_sync_status': row[8] or 'not_started',
                'last_sync_message': row[9] or 'No sync performed yet',
                'files_downloaded_count': row[10] or 0,
                'total_download_size_mb': row[11] or 0.0,
                'runtime': {
                    'is_running': is_running,
                    'timer_active': is_running
                }
            }
            dashboard_data.append(source_data)
        
        return jsonify({
            'success': True,
            'dashboard': dashboard_data,
            'total_sources': len(dashboard_data),
            'active_syncs': len([s for s in dashboard_data if s['runtime']['is_running']])
        }), 200
        
    except Exception as e:
        logger.error(f"‚ùå Error getting sync dashboard: {e}")
        return jsonify({
            'success': False,
            'message': f'Error getting sync dashboard: {str(e)}'
        }), 500

@sync_bp.route('/test-pydrive', methods=['GET'])
def test_pydrive_connection():
    """Test PyDrive2 installation and basic functionality"""
    try:
        # Test PyDrive2 import
        from pydrive2.auth import GoogleAuth
        from pydrive2.drive import GoogleDrive
        
        # Test basic functionality
        auth_test = GoogleAuth()
        
        return jsonify({
            'success': True,
            'message': 'PyDrive2 is installed and working',
            'pydrive_version': '1.15.4',
            'test_result': 'PyDrive2 GoogleAuth class imported successfully'
        }), 200
        
    except ImportError as e:
        return jsonify({
            'success': False,
            'message': 'PyDrive2 not installed or import failed',
            'error': str(e),
            'suggestion': 'Run: pip install PyDrive2==1.15.4'
        }), 500
    except Exception as e:
        logger.error(f"‚ùå PyDrive test error: {e}")
        return jsonify({
            'success': False,
            'message': f'PyDrive test failed: {str(e)}'
        }), 500

@sync_bp.route('/debug-credentials/<int:source_id>', methods=['GET'])
def debug_credentials(source_id: int):
    """Debug credentials loading step by step"""
    try:
        from modules.db_utils import get_db_connection
        import json
        import hashlib
        import os
        
        debug_info = {}
        
        # Step 1: Get source config
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            SELECT config FROM video_sources 
            WHERE id = ? AND source_type = 'cloud' AND active = 1
        """, (source_id,))
        result = cursor.fetchone()
        conn.close()
        
        if not result or not result[0]:
            return jsonify({
                'success': False,
                'error': f'No source config found for source {source_id}',
                'debug_info': debug_info
            }), 404
        
        debug_info['step1_config'] = 'Found source config'
        
        # Step 2: Parse config
        config_data = json.loads(result[0])
        user_email = config_data.get('user_email')
        debug_info['step2_user_email'] = user_email
        
        if not user_email:
            return jsonify({
                'success': False,
                'error': 'No user_email in source config',
                'debug_info': debug_info
            }), 400
        
        # Step 3: Calculate file path
        tokens_dir = os.path.join(os.path.dirname(__file__), 'tokens')
        email_hash = hashlib.sha256(user_email.encode()).hexdigest()[:16]
        token_filename = f"google_drive_{email_hash}.json"
        token_filepath = os.path.join(tokens_dir, token_filename)
        
        debug_info['step3_filepath'] = token_filepath
        debug_info['step3_file_exists'] = os.path.exists(token_filepath)
        
        if not os.path.exists(token_filepath):
            return jsonify({
                'success': False,
                'error': f'Credentials file not found: {token_filepath}',
                'debug_info': debug_info
            }), 404
        
        # Step 4: Load encrypted storage
        with open(token_filepath, 'r') as f:
            encrypted_storage = json.load(f)
        
        debug_info['step4_storage_keys'] = list(encrypted_storage.keys())
        debug_info['step4_has_encrypted_data'] = 'encrypted_data' in encrypted_storage
        
        # Step 5: Test decryption
        try:
            from modules.sources.cloud_endpoints import decrypt_credentials
            credential_data = decrypt_credentials(encrypted_storage['encrypted_data'])
            debug_info['step5_decrypt_success'] = credential_data is not None
            if credential_data:
                debug_info['step5_credential_keys'] = list(credential_data.keys())
            else:
                debug_info['step5_decrypt_error'] = 'Decryption returned None'
        except Exception as e:
            debug_info['step5_decrypt_error'] = str(e)
            return jsonify({
                'success': False,
                'error': f'Decryption failed: {str(e)}',
                'debug_info': debug_info
            }), 500
        
        # Step 6: Test credentials object creation
        try:
            from google.oauth2.credentials import Credentials
            credentials = Credentials(
                token=credential_data['token'],
                refresh_token=credential_data['refresh_token'],
                token_uri=credential_data['token_uri'],
                client_id=credential_data['client_id'],
                client_secret=credential_data['client_secret'],
                scopes=credential_data['scopes']
            )
            debug_info['step6_credentials_created'] = True
            debug_info['step6_credentials_expired'] = credentials.expired
            debug_info['step6_has_refresh_token'] = credentials.refresh_token is not None
        except Exception as e:
            debug_info['step6_credentials_error'] = str(e)
            return jsonify({
                'success': False,
                'error': f'Credentials object creation failed: {str(e)}',
                'debug_info': debug_info
            }), 500
        
        # Step 7: Test PyDrive auth
        try:
            from pydrive2.auth import GoogleAuth
            from pydrive2.drive import GoogleDrive
            
            gauth = GoogleAuth()
            gauth.credentials = credentials
            drive = GoogleDrive(gauth)
            
            # Test connection
            about = drive.GetAbout()
            debug_info['step7_pydrive_success'] = True
            debug_info['step7_user_info'] = {
                'name': about.get('name', 'Unknown'),
                'email': about.get('user', {}).get('emailAddress', 'Unknown')
            }
            
        except Exception as e:
            debug_info['step7_pydrive_error'] = str(e)
            return jsonify({
                'success': False,
                'error': f'PyDrive authentication failed: {str(e)}',
                'debug_info': debug_info
            }), 500
        
        return jsonify({
            'success': True,
            'message': 'All authentication steps successful',
            'debug_info': debug_info
        }), 200
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Debug failed: {str(e)}',
            'debug_info': debug_info
        }), 500

@sync_bp.route('/auto-start-cloud-sync', methods=['POST'])
def auto_start_cloud_sync():
    """Auto-start sync for all active cloud sources (called when backend starts)"""
    try:
        from modules.db_utils import get_db_connection
        
        # Get all active cloud sources
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            SELECT id, name FROM video_sources 
            WHERE active = 1 AND source_type = 'cloud'
        """)
        sources = cursor.fetchall()
        conn.close()
        
        started_sources = []
        failed_sources = []
        
        for source_id, source_name in sources:
            try:
                success = start_source_sync(source_id)
                if success:
                    started_sources.append({'id': source_id, 'name': source_name})
                    logger.info(f"‚úÖ Auto-started sync for {source_name} (ID: {source_id})")
                else:
                    failed_sources.append({'id': source_id, 'name': source_name})
                    logger.warning(f"‚ö†Ô∏è Failed to auto-start sync for {source_name} (ID: {source_id})")
            except Exception as e:
                failed_sources.append({'id': source_id, 'name': source_name, 'error': str(e)})
                logger.error(f"‚ùå Error auto-starting sync for {source_name}: {e}")
        
        return jsonify({
            'success': True,
            'message': f'Auto-start completed: {len(started_sources)} started, {len(failed_sources)} failed',
            'started_sources': started_sources,
            'failed_sources': failed_sources,
            'total_sources': len(sources)
        }), 200
        
    except Exception as e:
        logger.error(f"‚ùå Error in auto-start: {e}")
        return jsonify({
            'success': False,
            'message': f'Auto-start failed: {str(e)}'
        }), 500

# Health check endpoint
@sync_bp.route('/sync-health', methods=['GET'])
def sync_health_check():
    """Health check for sync service"""
    try:
        active_timers = len(pydrive_downloader.sync_timers)
        active_locks = len(pydrive_downloader.sync_locks)
        cached_clients = len(pydrive_downloader.drive_clients)
        
        return jsonify({
            'success': True,
            'health': 'healthy',
            'service': 'PyDriveDownloader',
            'stats': {
                'active_sync_timers': active_timers,
                'active_sync_locks': active_locks,
                'cached_drive_clients': cached_clients
            }
        }), 200
        
    except Exception as e:
        logger.error(f"‚ùå Health check failed: {e}")
        return jsonify({
            'success': False,
            'health': 'unhealthy',
            'error': str(e)
        }), 500

```
## üìÑ File: `auto_sync_service.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/auto_sync_service.py`

```python
import threading
import logging
import time
from datetime import datetime, timedelta
import json
from modules.db_utils import get_db_connection
from database import get_sync_status, initialize_sync_status

logger = logging.getLogger(__name__)

class AutoSyncService:
    def __init__(self):
        self.sync_timers = {}  # Store timers for each source
        self.sync_locks = {}   # Locks to prevent concurrent syncs
        self.downloaders = {}  # Cache for different downloader types
        
    def _get_downloader_for_source(self, source_type: str):
        """Only create downloader when needed"""
        if source_type not in self.downloaders:
            if source_type == 'nvr':
                # from modules.sources.nvr_downloader import NVRDownloader  # DISABLED: Import causes issues
                # self.downloaders[source_type] = NVRDownloader()  # DISABLED: Not using NVR downloader
                self.downloaders[source_type] = None  # Placeholder
            elif source_type == 'cloud':
                from modules.sources.cloud_manager import CloudManager
                self.downloaders[source_type] = CloudManager()
            elif source_type == 'local':
                # Local sources don't need special downloader
                self.downloaders[source_type] = None
        return self.downloaders.get(source_type)
        
    def start_auto_sync(self, source_config: dict) -> bool:
        """Start auto-sync for a source"""
        source_id = source_config.get('id')
        if not source_id:
            logger.error("Source ID required to start sync")
            return False
            
        if source_id in self.sync_timers:
            logger.warning(f"Sync already running for source {source_id}")
            return True
            
        # Initialize status if not exists
        current_status = get_sync_status(source_id)
        if not current_status:
            initialize_sync_status(source_id, sync_enabled=True, interval_minutes=10)
            
        self.sync_locks[source_id] = threading.Lock()
        self._schedule_next_sync(source_id)
        
        logger.info(f"Auto-sync started for source {source_id}")
        return True
        
    def stop_auto_sync(self, source_id: int) -> bool:
        """Stop auto-sync for a source"""
        try:
            if source_id not in self.sync_timers:
                logger.warning(f"No active sync for source {source_id}")
                return True
                
            # Cancel timer
            self.sync_timers[source_id].cancel()
            del self.sync_timers[source_id]
            del self.sync_locks[source_id]
            
            # Update status
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE sync_status 
                SET sync_enabled = 0, 
                    last_sync_status = 'stopped',
                    last_sync_message = 'Auto-sync stopped by user'
                WHERE source_id = ?
            """, (source_id,))
            conn.commit()
            conn.close()
            
            logger.info(f"Auto-sync stopped for source {source_id}")
            return True
            
        except Exception as e:
            logger.error(f"Error stopping sync for {source_id}: {e}")
            return False
            
    def get_sync_status(self, source_id: int) -> dict:
        """Get current sync status"""
        return get_sync_status(source_id) or {}
        
    def _sync_latest_recordings(self, source_id: int) -> dict:
        """Perform sync of latest recordings"""
        with self.sync_locks.get(source_id, threading.Lock()):
            try:
                # Get source config and type
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT config, source_type FROM video_sources 
                    WHERE id = ?
                """, (source_id,))
                result = cursor.fetchone()
                conn.close()
                
                if not result:
                    return {'success': False, 'message': 'Source not found'}
                    
                config = json.loads(result[0])
                source_type = result[1]
                
                # Update status to in_progress
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE sync_status 
                    SET last_sync_status = 'in_progress',
                        last_sync_message = 'Sync started'
                    WHERE source_id = ?
                """, (source_id,))
                conn.commit()
                conn.close()
                
                # Get appropriate downloader
                downloader = self._get_downloader_for_source(source_type)
                
                if not downloader:
                    return {'success': False, 'message': f'No downloader available for source type: {source_type}'}
                
                # Download last 24 hours
                time_range = {
                    'from': datetime.now() - timedelta(hours=24),
                    'to': datetime.now()
                }
                
                download_result = downloader.download_latest_recordings(config, time_range)
                
                # Update status
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE sync_status 
                    SET last_sync_timestamp = ?,
                        last_sync_status = ?,
                        last_sync_message = ?,
                        files_downloaded_count = files_downloaded_count + ?,
                        total_download_size_mb = total_download_size_mb + ?
                    WHERE source_id = ?
                """, (
                    datetime.now().isoformat(),
                    'success' if download_result['success'] else 'failed',
                    download_result['message'],
                    download_result.get('files_downloaded', 0),
                    download_result.get('total_size_mb', 0.0),
                    source_id
                ))
                conn.commit()
                conn.close()
                
                return download_result
                
            except Exception as e:
                logger.error(f"Sync error for {source_id}: {e}")
                
                # Update error status
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE sync_status 
                    SET last_sync_status = 'failed',
                        last_sync_message = ?
                    WHERE source_id = ?
                """, (str(e), source_id))
                conn.commit()
                conn.close()
                
                return {'success': False, 'message': str(e)}
    
    def _schedule_next_sync(self, source_id: int):
        """Schedule next sync run"""
        status = self.get_sync_status(source_id)
        if not status.get('sync_enabled', True):
            return
            
        interval = status.get('sync_interval_minutes', 10)
        next_sync = datetime.now() + timedelta(minutes=interval)
        
        # Update next timestamp
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE sync_status 
            SET next_sync_timestamp = ?
            WHERE source_id = ?
        """, (next_sync.isoformat(), source_id))
        conn.commit()
        conn.close()
        
        # Schedule timer
        timer = threading.Timer(interval * 60, self._perform_sync, args=(source_id,))
        timer.daemon = True
        self.sync_timers[source_id] = timer
        timer.start()
        
        logger.info(f"Next sync scheduled for {source_id} at {next_sync}")
    
    def _perform_sync(self, source_id: int):
        """Perform sync and schedule next"""
        self._sync_latest_recordings(source_id)
        self._schedule_next_sync(source_id)
```
## üìÑ File: `path_manager.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/path_manager.py`

```python
import sqlite3
import os
import json
import logging
import uuid
from datetime import datetime
import pytz
from modules.db_utils import get_db_connection
from modules.scheduler.db_sync import db_rwlock

# C·∫•u h√¨nh m√∫i gi·ªù Vi·ªát Nam - ƒê·ªíNG NH·∫§T V·ªöI FILE_LISTER
VIETNAM_TZ = pytz.timezone('Asia/Ho_Chi_Minh')

class PathManager:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
    def get_all_active_sources(self):
        """Get all active video sources from database"""
        try:
            with db_rwlock.gen_rlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT id, source_type, name, path, config, active, created_at 
                    FROM video_sources 
                    WHERE active = 1 
                    ORDER BY source_type, name
                """)
                sources = []
                for row in cursor.fetchall():
                    source = {
                        'id': row[0],
                        'source_type': row[1],
                        'name': row[2],
                        'path': row[3],
                        'config': json.loads(row[4]) if row[4] else {},
                        'active': row[5],
                        'created_at': row[6]
                    }
                    sources.append(source)
                conn.close()
                return sources
        except Exception as e:
            self.logger.error(f"Error getting active sources: {e}")
            return []

    def get_current_active_source(self):
        """Get current active source (Single Active Source)"""
        sources = self.get_all_active_sources()
        return sources[0] if sources else None
    
    def get_source_by_id(self, source_id):
        """Get specific video source by ID"""
        try:
            with db_rwlock.gen_rlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT id, source_type, name, path, config, active, created_at 
                    FROM video_sources 
                    WHERE id = ?
                """, (source_id,))
                row = cursor.fetchone()
                conn.close()
                if row:
                    return {
                        'id': row[0],
                        'source_type': row[1],
                        'name': row[2],
                        'path': row[3],
                        'config': json.loads(row[4]) if row[4] else {},
                        'active': row[5],
                        'created_at': row[6]
                    }
                return None
        except Exception as e:
            self.logger.error(f"Error getting source by id {source_id}: {e}")
            return None
    
    def get_source_id_by_name(self, source_name):
        """Get source ID by name"""
        try:
            with db_rwlock.gen_rlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("SELECT id FROM video_sources WHERE name = ?", (source_name,))
                result = cursor.fetchone()
                conn.close()
                return result[0] if result else None
        except Exception as e:
            self.logger.error(f"Error getting source id by name {source_name}: {e}")
            return None
    
    def set_active_source(self, source_id):
        """Set single active source (disable all others)"""
        try:
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                
                # Disable all sources first
                cursor.execute("UPDATE video_sources SET active = 0")
                
                # Enable the specified source
                cursor.execute("UPDATE video_sources SET active = 1 WHERE id = ?", (source_id,))
                
                if cursor.rowcount == 0:
                    conn.rollback()
                    conn.close()
                    return False, f"No source found with id {source_id}"
                
                conn.commit()
                conn.close()
                
                self.logger.info(f"Set source id {source_id} as active")
                return True, "Active source updated successfully"
                
        except Exception as e:
            self.logger.error(f"Error setting active source: {e}")
            return False, str(e)
    
    def validate_source_accessibility(self, source_config):
        """Check if source is accessible based on type"""
        source_type = source_config.get('source_type')
        path = source_config.get('path')
        config = source_config.get('config', {})
        
        try:
            if source_type == 'local':
                return self._validate_local_path(path)
            elif source_type == 'camera':
                return self._validate_camera_source(path, config)
            elif source_type == 'cloud':
                return self._validate_cloud_source(path, config)
            else:
                return False, f"Unknown source type: {source_type}"
        except Exception as e:
            self.logger.error(f"Error validating source accessibility: {e}")
            return False, str(e)
    
    def _validate_local_path(self, path):
        """Validate local file system path"""
        if not path:
            return False, "Path is required"
        if not os.path.exists(path):
            return False, f"Path does not exist: {path}"
        if not os.access(path, os.R_OK):
            return False, f"No read permission for path: {path}"
        return True, "Local path is accessible"
    
    
    def _validate_camera_source(self, path, config):
        """Validate camera/NVR source"""
        if config.get('type') == 'directory':
            return self._validate_local_path(path)
        elif config.get('type') == 'api':
            api_url = config.get('api_url')
            if not api_url:
                return False, "API URL is required for camera source"
            
            try:
                import requests
                response = requests.get(api_url, timeout=10)
                if response.status_code == 200:
                    return True, "Camera API accessible"
                else:
                    return False, f"Camera API returned status {response.status_code}"
            except ImportError:
                return False, "requests library not installed"
            except Exception as e:
                return False, f"Camera API validation failed: {e}"
        else:
            return False, "Invalid camera source type"
    
    def _validate_cloud_source(self, path, config):
        """Validate cloud storage source"""
        provider = config.get('provider', '').lower()
        
        if provider == 'google_drive':
            return self._validate_google_drive(config)
        elif provider == 'dropbox':
            return self._validate_dropbox(config)
        elif provider == 'onedrive':
            return self._validate_onedrive(config)
        else:
            return False, f"Unsupported cloud provider: {provider}"
    
    def _validate_google_drive(self, config):
        """Validate Google Drive access"""
        try:
            from google.oauth2.credentials import Credentials
            from googleapiclient.discovery import build
            
            credentials_data = config.get('credentials')
            if not credentials_data:
                return False, "Google Drive credentials not found"
            
            credentials = Credentials.from_authorized_user_info(credentials_data)
            service = build('drive', 'v3', credentials=credentials)
            service.files().list(pageSize=1).execute()
            
            return True, "Google Drive connection successful"
            
        except ImportError:
            return False, "Google API library not installed"
        except Exception as e:
            return False, f"Google Drive validation failed: {e}"
    
    def _validate_dropbox(self, config):
        """Validate Dropbox access"""
        try:
            import dropbox
            
            access_token = config.get('access_token')
            if not access_token:
                return False, "Dropbox access token not found"
            
            dbx = dropbox.Dropbox(access_token)
            dbx.users_get_current_account()
            
            return True, "Dropbox connection successful"
            
        except ImportError:
            return False, "Dropbox library not installed"
        except Exception as e:
            return False, f"Dropbox validation failed: {e}"
    
    def _validate_onedrive(self, config):
        """Validate OneDrive access"""
        return False, "OneDrive validation not implemented yet"
    
    def add_source(self, source_type, name, path, config=None):
        """Add new video source"""
        try:
            if not all([source_type, name, path]):
                return False, "source_type, name, and path are required"
            
            config_json = json.dumps(config) if config else None
            
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                
                # Check if name already exists
                cursor.execute("SELECT COUNT(*) FROM video_sources WHERE name = ?", (name,))
                if cursor.fetchone()[0] > 0:
                    conn.close()
                    return False, f"Source name '{name}' already exists"
                
                # ‚úÖ FIXED: Use VIETNAM_TZ for created_at - ƒê·ªíNG NH·∫§T V·ªöI FILE_LISTER
                cursor.execute("""
                    INSERT INTO video_sources (source_type, name, path, config, active, created_at)
                    VALUES (?, ?, ?, ?, 1, ?)
                """, (source_type, name, path, config_json, datetime.now(VIETNAM_TZ)))
                
                source_id = cursor.lastrowid
                conn.commit()
                conn.close()
                
                self.logger.info(f"Added new video source: {name} (id: {source_id})")
                return True, f"Source '{name}' added successfully"
                
        except Exception as e:
            self.logger.error(f"Error adding source: {e}")
            return False, str(e)
    
    def update_source(self, source_id, **kwargs):
        """Update existing video source"""
        try:
            if not source_id:
                return False, "source_id is required"
            
            # Build update query
            update_fields = []
            update_values = []
            
            for field, value in kwargs.items():
                if field in ['source_type', 'name', 'path', 'active']:
                    update_fields.append(f"{field} = ?")
                    update_values.append(value)
                elif field == 'config':
                    update_fields.append("config = ?")
                    update_values.append(json.dumps(value) if value else None)
            
            if not update_fields:
                return False, "No valid fields to update"
            
            update_values.append(source_id)
            
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                
                query = f"UPDATE video_sources SET {', '.join(update_fields)} WHERE id = ?"
                cursor.execute(query, update_values)
                
                if cursor.rowcount == 0:
                    conn.close()
                    return False, f"No source found with id {source_id}"
                
                conn.commit()
                conn.close()
                
                self.logger.info(f"Updated video source id: {source_id}")
                return True, "Source updated successfully"
                
        except Exception as e:
            self.logger.error(f"Error updating source: {e}")
            return False, str(e)
    
    def delete_source(self, source_id):
        """Delete video source"""
        try:
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                
                cursor.execute("DELETE FROM video_sources WHERE id = ?", (source_id,))
                
                if cursor.rowcount == 0:
                    conn.close()
                    return False, f"No source found with id {source_id}"
                
                conn.commit()
                conn.close()
                
                self.logger.info(f"Deleted video source id: {source_id}")
                return True, "Source deleted successfully"
                
        except Exception as e:
            self.logger.error(f"Error deleting source: {e}")
            return False, str(e)
    
    def toggle_source_status(self, source_id, active):
        """Toggle source active status"""
        try:
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                
                cursor.execute("UPDATE video_sources SET active = ? WHERE id = ?", (active, source_id,))
                
                if cursor.rowcount == 0:
                    conn.close()
                    return False, f"No source found with id {source_id}"
                
                conn.commit()
                conn.close()
                
                status = "activated" if active else "deactivated"
                self.logger.info(f"Source id {source_id} {status}")
                return True, f"Source {status} successfully"
                
        except Exception as e:
            self.logger.error(f"Error toggling source status: {e}")
            return False, str(e)
```
## üìÑ File: `cloud_lazy_folder_routes.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/cloud_lazy_folder_routes.py`

```python
#!/usr/bin/env python3

"""
Lazy Loading Folder Tree Routes for Google Drive Integration
Separated from main cloud_endpoints.py for better organization
"""

from flask import Blueprint, request, jsonify, session
from flask_cors import cross_origin
from google.oauth2.credentials import Credentials
from modules.sources.google_drive_service import GoogleDriveFolderService
from datetime import datetime
import logging
from functools import wraps
import time
from collections import defaultdict

logger = logging.getLogger(__name__)

# Rate limiting storage
lazy_folder_rate_limit_storage = defaultdict(list)

LAZY_FOLDER_RATE_LIMITS = {
    'folder_discovery': {'calls': 15, 'window': 60},
    'folder_search': {'calls': 10, 'window': 60},
    'folder_info': {'calls': 20, 'window': 60}
}

def lazy_folder_rate_limit(endpoint_type='folder_discovery'):
    """Rate limiting decorator for lazy folder operations"""
    def decorator(f):
        @wraps(f)
        def decorated_function(*args, **kwargs):
            client_ip = request.remote_addr
            current_time = time.time()
            
            limit_config = LAZY_FOLDER_RATE_LIMITS.get(endpoint_type, {'calls': 15, 'window': 60})
            max_calls = limit_config['calls']
            time_window = limit_config['window']
            
            # Clean old entries
            cutoff_time = current_time - time_window
            lazy_folder_rate_limit_storage[client_ip] = [
                call_time for call_time in lazy_folder_rate_limit_storage[client_ip] 
                if call_time > cutoff_time
            ]
            
            # Check rate limit
            if len(lazy_folder_rate_limit_storage[client_ip]) >= max_calls:
                logger.warning(f"üö´ Lazy folder rate limit exceeded for {client_ip} on {endpoint_type}")
                return jsonify({
                    'success': False,
                    'message': f'Rate limit exceeded. Max {max_calls} calls per {time_window} seconds.',
                    'retry_after': int(time_window - (current_time - lazy_folder_rate_limit_storage[client_ip][0]))
                }), 429
            
            # Record this call
            lazy_folder_rate_limit_storage[client_ip].append(current_time)
            
            return f(*args, **kwargs)
        return decorated_function
    return decorator

def get_credentials_from_session():
    """Get Google Drive credentials from session data"""
    try:
        auth_result = session.get('auth_result')
        if not auth_result or not auth_result.get('credentials'):
            return None
        
        cred_data = auth_result['credentials']
        credentials = Credentials(
            token=cred_data['token'],
            refresh_token=cred_data.get('refresh_token'),
            token_uri=cred_data.get('token_uri'),
            client_id=cred_data.get('client_id'),
            client_secret=cred_data.get('client_secret'),
            scopes=cred_data.get('scopes', [])
        )
        
        return credentials
    except Exception as e:
        logger.error(f"‚ùå Error getting credentials from session: {e}")
        return None

# Create Blueprint for lazy folder routes
lazy_folder_bp = Blueprint('lazy_folders', __name__, url_prefix='/folders')

@lazy_folder_bp.route('/list_subfolders', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@lazy_folder_rate_limit('folder_discovery')
def list_subfolders():
    """
    Get subfolders of a specific parent folder with lazy loading
    
    Request JSON:
    {
        "parent_id": "folder_id_or_root",
        "max_results": 50,
        "include_stats": false
    }
    
    Response:
    {
        "success": true,
        "folders": [...],
        "parent_info": {...},
        "total_count": 25,
        "has_more": false
    }
    """
    try:
        data = request.get_json()
        parent_id = data.get('parent_id', 'root')
        max_results = min(data.get('max_results', 50), 100)  # Cap at 100
        include_stats = data.get('include_stats', False)
        
        logger.info(f"üìÇ Listing subfolders for parent: {parent_id}")
        
        # Get credentials
        credentials = get_credentials_from_session()
        if not credentials:
            return jsonify({
                'success': False,
                'message': 'No valid Google Drive credentials found. Please authenticate first.',
                'requires_auth': True
            }), 401
        
        # Initialize folder service
        folder_service = GoogleDriveFolderService(credentials)
        
        # Get subfolders
        subfolders = folder_service.get_subfolders(parent_id, max_results)
        
        # Enrich folder data with depth and selection info
        enriched_folders = []
        for folder in subfolders:
            # Calculate depth for this folder
            depth = folder_service.calculate_folder_depth(folder['id'])
            
            # Check if folder has subfolders (for expand indicator)
            has_subfolders = folder_service.has_subfolders(folder['id'])
            
            enriched_folder = {
                'id': folder['id'],
                'name': folder['name'],
                'type': 'folder',
                'parent_id': parent_id,
                'depth': depth,
                'selectable': folder_service.is_selectable_folder(depth),
                'has_subfolders': has_subfolders,
                'created': folder.get('created'),
                'modified': folder.get('modified'),
                'path': folder_service.build_folder_path(folder['id'])
            }
            
            # Add statistics if requested
            if include_stats:
                stats = folder_service.get_folder_statistics(folder['id'])
                enriched_folder['stats'] = stats
            
            enriched_folders.append(enriched_folder)
        
        # Get parent folder info
        parent_info = {}
        if parent_id != 'root':
            parent_info = folder_service.get_folder_info(parent_id)
        else:
            parent_info = {
                'id': 'root',
                'name': 'My Drive',
                'depth': 0,
                'path': '/My Drive',
                'selectable': False
            }
        
        response_data = {
            'success': True,
            'folders': enriched_folders,
            'parent_info': parent_info,
            'total_count': len(enriched_folders),
            'has_more': len(enriched_folders) == max_results,  # Might have more if we hit the limit
            'cache_info': folder_service.get_cache_info(),
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"‚úÖ Found {len(enriched_folders)} subfolders in {parent_id}")
        return jsonify(response_data), 200
        
    except Exception as e:
        logger.error(f"‚ùå Error listing subfolders: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to list subfolders: {str(e)}',
            'error_type': type(e).__name__
        }), 500

@lazy_folder_bp.route('/get_depth', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@lazy_folder_rate_limit('folder_info')
def get_folder_depth():
    """
    Get the depth level of a specific folder
    
    Request JSON:
    {
        "folder_id": "folder_id"
    }
    
    Response:
    {
        "success": true,
        "folder_id": "folder_id",
        "depth": 2,
        "selectable": false,
        "path": "/My Drive/Project/Area"
    }
    """
    try:
        data = request.get_json()
        folder_id = data.get('folder_id')
        
        if not folder_id:
            return jsonify({
                'success': False,
                'message': 'folder_id is required'
            }), 400
        
        # Get credentials
        credentials = get_credentials_from_session()
        if not credentials:
            return jsonify({
                'success': False,
                'message': 'No valid Google Drive credentials found',
                'requires_auth': True
            }), 401
        
        # Initialize folder service
        folder_service = GoogleDriveFolderService(credentials)
        
        # Calculate depth
        depth = folder_service.calculate_folder_depth(folder_id)
        path = folder_service.build_folder_path(folder_id)
        selectable = folder_service.is_selectable_folder(depth)
        
        return jsonify({
            'success': True,
            'folder_id': folder_id,
            'depth': depth,
            'selectable': selectable,
            'path': path,
            'timestamp': datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"‚ùå Error getting folder depth: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to get folder depth: {str(e)}'
        }), 500

@lazy_folder_bp.route('/search', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@lazy_folder_rate_limit('folder_search')
def search_folders():
    """
    Search for folders by name
    
    Request JSON:
    {
        "query": "camera",
        "max_results": 20
    }
    
    Response:
    {
        "success": true,
        "folders": [...],
        "query": "camera",
        "total_found": 15
    }
    """
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        max_results = min(data.get('max_results', 20), 50)
        
        if not query:
            return jsonify({
                'success': False,
                'message': 'Search query is required'
            }), 400
        
        # Get credentials
        credentials = get_credentials_from_session()
        if not credentials:
            return jsonify({
                'success': False,
                'message': 'No valid Google Drive credentials found',
                'requires_auth': True
            }), 401
        
        # Initialize folder service
        folder_service = GoogleDriveFolderService(credentials)
        
        # Search folders
        search_results = folder_service.search_folders(query, max_results)
        
        return jsonify({
            'success': True,
            'folders': search_results,
            'query': query,
            'total_found': len(search_results),
            'timestamp': datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"‚ùå Error searching folders: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to search folders: {str(e)}'
        }), 500

@lazy_folder_bp.route('/get_info', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@lazy_folder_rate_limit('folder_info')
def get_folder_info():
    """
    Get comprehensive information about a folder
    
    Request JSON:
    {
        "folder_id": "folder_id",
        "include_stats": true
    }
    """
    try:
        data = request.get_json()
        folder_id = data.get('folder_id')
        include_stats = data.get('include_stats', False)
        
        if not folder_id:
            return jsonify({
                'success': False,
                'message': 'folder_id is required'
            }), 400
        
        # Get credentials
        credentials = get_credentials_from_session()
        if not credentials:
            return jsonify({
                'success': False,
                'message': 'No valid Google Drive credentials found',
                'requires_auth': True
            }), 401
        
        # Initialize folder service
        folder_service = GoogleDriveFolderService(credentials)
        
        # Get folder info
        folder_info = folder_service.get_folder_info(folder_id)
        
        if include_stats:
            stats = folder_service.get_folder_statistics(folder_id)
            folder_info['stats'] = stats
        
        return jsonify({
            'success': True,
            'folder_info': folder_info,
            'timestamp': datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"‚ùå Error getting folder info: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to get folder info: {str(e)}'
        }), 500

@lazy_folder_bp.route('/clear_cache', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@lazy_folder_rate_limit('folder_info')
def clear_folder_cache():
    """Clear Google Drive folder service cache"""
    try:
        # If we have active credentials, clear service cache
        credentials = get_credentials_from_session()
        if credentials:
            folder_service = GoogleDriveFolderService(credentials)
            folder_service.clear_cache()
        
        return jsonify({
            'success': True,
            'message': 'Folder service cache cleared successfully',
            'timestamp': datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"‚ùå Error clearing folder cache: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to clear folder cache: {str(e)}'
        }), 500

@lazy_folder_bp.route('/breadcrumb', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@lazy_folder_rate_limit('folder_info')
def get_folder_breadcrumb():
    """
    Get breadcrumb navigation for a folder
    
    Request JSON:
    {
        "folder_id": "folder_id"
    }
    
    Response:
    {
        "success": true,
        "breadcrumb": [
            {"id": "root", "name": "My Drive", "depth": 0},
            {"id": "123", "name": "Project", "depth": 1},
            {"id": "456", "name": "Area", "depth": 2}
        ]
    }
    """
    try:
        data = request.get_json()
        folder_id = data.get('folder_id')
        
        if not folder_id:
            return jsonify({
                'success': False,
                'message': 'folder_id is required'
            }), 400
        
        # Get credentials
        credentials = get_credentials_from_session()
        if not credentials:
            return jsonify({
                'success': False,
                'message': 'No valid Google Drive credentials found',
                'requires_auth': True
            }), 401
        
        # Initialize folder service
        folder_service = GoogleDriveFolderService(credentials)
        
        # Build breadcrumb
        breadcrumb = []
        current_id = folder_id
        
        # Traverse up the hierarchy
        while current_id and current_id != 'root' and len(breadcrumb) < 10:
            try:
                folder_info = folder_service.get_folder_info(current_id)
                breadcrumb.insert(0, {
                    'id': current_id,
                    'name': folder_info.get('name', 'Unknown'),
                    'depth': folder_info.get('depth', 0)
                })
                
                # Get parent
                parents = folder_info.get('parents', [])
                current_id = parents[0] if parents else 'root'
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error getting folder info for breadcrumb {current_id}: {e}")
                break
        
        # Add root if not already there
        if not breadcrumb or breadcrumb[0]['id'] != 'root':
            breadcrumb.insert(0, {
                'id': 'root',
                'name': 'My Drive',
                'depth': 0
            })
        
        return jsonify({
            'success': True,
            'breadcrumb': breadcrumb,
            'total_levels': len(breadcrumb),
            'timestamp': datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"‚ùå Error getting breadcrumb: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to get breadcrumb: {str(e)}'
        }), 500

@lazy_folder_bp.route('/validate_selection', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@lazy_folder_rate_limit('folder_info')
def validate_folder_selection():
    """
    Validate if selected folders meet the depth requirements
    
    Request JSON:
    {
        "folder_ids": ["id1", "id2", "id3"]
    }
    
    Response:
    {
        "success": true,
        "valid_selections": [...],
        "invalid_selections": [...],
        "total_valid": 2
    }
    """
    try:
        data = request.get_json()
        folder_ids = data.get('folder_ids', [])
        
        if not folder_ids:
            return jsonify({
                'success': False,
                'message': 'folder_ids array is required'
            }), 400
        
        # Get credentials
        credentials = get_credentials_from_session()
        if not credentials:
            return jsonify({
                'success': False,
                'message': 'No valid Google Drive credentials found',
                'requires_auth': True
            }), 401
        
        # Initialize folder service
        folder_service = GoogleDriveFolderService(credentials)
        
        valid_selections = []
        invalid_selections = []
        
        for folder_id in folder_ids:
            try:
                depth = folder_service.calculate_folder_depth(folder_id)
                selectable = folder_service.is_selectable_folder(depth)
                folder_info = folder_service.get_folder_info(folder_id)
                
                selection_info = {
                    'id': folder_id,
                    'name': folder_info.get('name', 'Unknown'),
                    'depth': depth,
                    'selectable': selectable,
                    'path': folder_info.get('path', ''),
                    'reason': 'Valid camera folder' if selectable else f'Wrong depth (level {depth}, need level 4)'
                }
                
                if selectable:
                    valid_selections.append(selection_info)
                else:
                    invalid_selections.append(selection_info)
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error validating folder {folder_id}: {e}")
                invalid_selections.append({
                    'id': folder_id,
                    'name': 'Unknown',
                    'depth': -1,
                    'selectable': False,
                    'path': '',
                    'reason': f'Validation error: {str(e)}'
                })
        
        return jsonify({
            'success': True,
            'valid_selections': valid_selections,
            'invalid_selections': invalid_selections,
            'total_valid': len(valid_selections),
            'total_invalid': len(invalid_selections),
            'timestamp': datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"‚ùå Error validating folder selection: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to validate selection: {str(e)}'
        }), 500
```
## üìÑ File: `onvif_client.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/onvif_client.py`

```python
# onvif_client.py - ONVIF cho VTrack
import logging
import socket
import requests
from typing import Dict

logger = logging.getLogger(__name__)

class VTrackOnvifClient:
    def __init__(self):
        self.connected_cameras = {}
        
    def test_device_connection(self, ip: str, port: int, username: str = '', password: str = '') -> Dict:
        """Test ONVIF connection - discover multiple cameras from multiple ports"""
        logger.info(f"üéØ Testing ONVIF multiple camera discovery on {ip}")
        
        try:
            # Multiple ports for docker-compose setup
            ports_to_test = [1000, 1001, 1002] if port in [80, 1000] else [port]
            
            discovered_cameras = []
            accessible_ports = []
            
            for test_port in ports_to_test:
                try:
                    # Test socket connection
                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                    sock.settimeout(3)
                    result = sock.connect_ex((ip, test_port))
                    sock.close()
                    
                    if result == 0:
                        logger.info(f"‚úÖ Port {test_port} accessible")
                        
                        # Test ONVIF service
                        camera = self._test_single_port(ip, test_port, username, password)
                        if camera:
                            discovered_cameras.append(camera)
                            accessible_ports.append(test_port)
                            logger.info(f"‚úÖ Camera discovered on port {test_port}: {camera['name']}")
                        else:
                            logger.warning(f"‚ö†Ô∏è Port {test_port} accessible but no ONVIF camera")
                    else:
                        logger.info(f"‚ùå Port {test_port} not accessible")
                        
                except Exception as port_error:
                    logger.warning(f"‚ùå Error testing port {test_port}: {port_error}")
                    continue
            
            if discovered_cameras:
                return {
                    'accessible': True,
                    'message': f'ONVIF Multi-Camera Discovery - Found {len(discovered_cameras)} camera(s) on ports: {accessible_ports}',
                    'source_type': 'nvr',
                    'protocol': 'onvif',
                    'cameras': discovered_cameras,
                    'device_info': {
                        'manufacturer': 'Multiple ONVIF Devices',
                        'model': 'Multi-Camera System',
                        'firmware': 'Various',
                        'total_cameras': len(discovered_cameras),
                        'discovered_ports': accessible_ports
                    }
                }
            else:
                return {
                    'accessible': False,
                    'message': f'No ONVIF cameras found on {ip} ports: {ports_to_test}',
                    'source_type': 'nvr',
                    'protocol': 'onvif',
                    'cameras': []
                }
                
        except Exception as e:
            logger.error(f"‚ùå Multiple camera discovery failed: {e}")
            return {
                'accessible': False,
                'message': f'Discovery error: {str(e)}',
                'source_type': 'nvr',
                'protocol': 'onvif',
                'cameras': []
            }

    def _test_single_port(self, ip: str, port: int, username: str = '', password: str = '') -> Dict:
        """Test single ONVIF camera on specific port"""
        try:
            soap_request = '''<?xml version="1.0" encoding="UTF-8"?>
<soap:Envelope xmlns:soap="http://www.w3.org/2003/05/soap-envelope" xmlns:tds="http://www.onvif.org/ver10/device/wsdl">
<soap:Header/>
<soap:Body>
<tds:GetDeviceInformation/>
</soap:Body>
</soap:Envelope>'''
            
            headers = {
                'Content-Type': 'application/soap+xml; charset=utf-8',
                'Content-Length': str(len(soap_request))
            }
            
            response = requests.post(
                f'http://{ip}:{port}/onvif/device_service',
                data=soap_request,
                headers=headers,
                timeout=5
            )
            
            if response.status_code == 200 and 'GetDeviceInformationResponse' in response.text:
                # Parse device info
                manufacturer = self._extract_xml_value(response.text, 'tds:Manufacturer', 'ACME Security')
                model = self._extract_xml_value(response.text, 'tds:Model', f'Camera-{port}')
                firmware = self._extract_xml_value(response.text, 'tds:FirmwareVersion', '2.0')
                
                # Port-based camera mapping
                camera_names = {
                    1000: "Front Door Camera",
                    1001: "Parking Lot Camera", 
                    1002: "Warehouse Camera"
                }
                
                rtsp_ports = {
                    1000: 8554,
                    1001: 8555,
                    1002: 8556
                }
                
                resolutions = {
                    1000: "1920x1080",
                    1001: "1280x720",
                    1002: "800x600"
                }
                
                codecs = {
                    1000: "H264",
                    1001: "H265", 
                    1002: "MPEG4"
                }
                
                camera_name = camera_names.get(port, f"Camera Port {port}")
                rtsp_port = rtsp_ports.get(port, 8554)
                resolution = resolutions.get(port, "640x480")
                codec = codecs.get(port, "H264")
                
                return {
                    'id': f"onvif_{ip}_{port}",
                    'name': camera_name,
                    'description': f"ONVIF {model} ({firmware})",
                    'stream_url': f"rtsp://{ip}:{rtsp_port}/stream",
                    'resolution': resolution,
                    'codec': codec,
                    'capabilities': ['recording'] + (['ptz'] if port == 1000 else []),
                    'onvif_port': port,
                    'rtsp_port': rtsp_port,
                    'manufacturer': manufacturer,
                    'model': model,
                    'firmware': firmware
                }
            else:
                return None
                
        except Exception as e:
            logger.warning(f"Failed to test port {port}: {e}")
            return None

    def _extract_xml_value(self, xml_text: str, tag: str, default: str = '') -> str:
        """Extract value from XML tag"""
        try:
            start_tag = f'<{tag}>'
            end_tag = f'</{tag}>'
            if start_tag in xml_text:
                start = xml_text.find(start_tag) + len(start_tag)
                end = xml_text.find(end_tag)
                return xml_text[start:end].strip() or default
            return default
        except:
            return default

# Global instance
onvif_client = VTrackOnvifClient()
```
## üìÑ File: `google_drive_service.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/google_drive_service.py`

```python
#!/usr/bin/env python3

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from google.oauth2.credentials import Credentials
import time
import logging
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

class GoogleDriveFolderService:
    """Service class for Google Drive folder operations with lazy loading support"""
    
    def __init__(self, credentials: Credentials):
        """Initialize service with Google Drive credentials"""
        self.credentials = credentials
        self.service = build('drive', 'v3', credentials=credentials)
        self.cache = {}
        self.cache_duration = 180  # 3 minutes cache
    
    def _get_cache_key(self, operation: str, *args) -> str:
        """Generate cache key for operations"""
        return f"{operation}:{'_'.join(str(arg) for arg in args)}"
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """Check if cached data is still valid"""
        if cache_key not in self.cache:
            return False
        
        cached_time = self.cache[cache_key].get('timestamp', 0)
        return time.time() - cached_time < self.cache_duration
    
    def _set_cache(self, cache_key: str, data: any):
        """Cache data with timestamp"""
        self.cache[cache_key] = {
            'data': data,
            'timestamp': time.time()
        }
    
    def _get_cache(self, cache_key: str) -> any:
        """Get cached data if valid"""
        if self._is_cache_valid(cache_key):
            return self.cache[cache_key]['data']
        return None
    
    def get_subfolders(self, parent_id: str = 'root', max_results: int = 50) -> List[Dict]:
        """
        Get subfolders of a parent folder with caching
        
        Args:
            parent_id: Parent folder ID ('root' for root folders)
            max_results: Maximum number of folders to return
            
        Returns:
            List of folder dictionaries with id, name, parents, createdTime
        """
        cache_key = self._get_cache_key('subfolders', parent_id, max_results)
        cached_result = self._get_cache(cache_key)
        
        if cached_result is not None:
            logger.debug(f"üìã Cache hit for subfolders: {parent_id}")
            return cached_result
        
        try:
            logger.info(f"üìÇ Fetching subfolders for parent: {parent_id}")
            
            # Build query for folders only
            if parent_id == 'root':
                query = "mimeType='application/vnd.google-apps.folder' and 'root' in parents and trashed=false"
            else:
                query = f"mimeType='application/vnd.google-apps.folder' and '{parent_id}' in parents and trashed=false"
            
            results = self.service.files().list(
                q=query,
                pageSize=min(max_results, 100),  # Google Drive max is 100
                fields="files(id, name, parents, createdTime, modifiedTime)",
                orderBy="name"
            ).execute()
            
            folders = results.get('files', [])
            
            # Format folder data
            formatted_folders = []
            for folder in folders:
                formatted_folder = {
                    'id': folder['id'],
                    'name': folder['name'],
                    'type': 'folder',
                    'parent_id': parent_id,
                    'parents': folder.get('parents', []),
                    'created': folder.get('createdTime'),
                    'modified': folder.get('modifiedTime')
                }
                formatted_folders.append(formatted_folder)
            
            # Cache the result
            self._set_cache(cache_key, formatted_folders)
            
            logger.info(f"‚úÖ Found {len(formatted_folders)} subfolders in {parent_id}")
            return formatted_folders
            
        except HttpError as e:
            logger.error(f"‚ùå HTTP error getting subfolders for {parent_id}: {e}")
            return []
        except Exception as e:
            logger.error(f"‚ùå Error getting subfolders for {parent_id}: {e}")
            return []
    
    def calculate_folder_depth(self, folder_id: str) -> int:
        """
        Calculate the depth level of a folder (0=root, 1=level1, etc.)
        
        Args:
            folder_id: Folder ID to calculate depth for
            
        Returns:
            Integer depth level (0-based)
        """
        if folder_id == 'root':
            return 0
        
        cache_key = self._get_cache_key('depth', folder_id)
        cached_result = self._get_cache(cache_key)
        
        if cached_result is not None:
            return cached_result
        
        try:
            depth = 0
            current_id = folder_id
            
            # Traverse up the folder hierarchy
            while current_id != 'root' and depth < 10:  # Prevent infinite loops
                try:
                    folder_info = self.service.files().get(
                        fileId=current_id,
                        fields="parents"
                    ).execute()
                    
                    parents = folder_info.get('parents', [])
                    if not parents:
                        break
                    
                    current_id = parents[0]  # Use first parent
                    depth += 1
                    
                    # Root check
                    if current_id == 'root':
                        break
                        
                except HttpError as e:
                    logger.warning(f"‚ö†Ô∏è Cannot get parent for {current_id}: {e}")
                    break
            
            # Cache the result
            self._set_cache(cache_key, depth)
            
            logger.debug(f"üìè Folder {folder_id} is at depth {depth}")
            return depth
            
        except Exception as e:
            logger.error(f"‚ùå Error calculating depth for {folder_id}: {e}")
            return 0
    
    def build_folder_path(self, folder_id: str) -> str:
        """
        Build the full path string for a folder
        
        Args:
            folder_id: Folder ID to build path for
            
        Returns:
            Full folder path string (e.g., "/Project/Area/Date/Camera")
        """
        if folder_id == 'root':
            return "/My Drive"
        
        cache_key = self._get_cache_key('path', folder_id)
        cached_result = self._get_cache(cache_key)
        
        if cached_result is not None:
            return cached_result
        
        try:
            path_parts = []
            current_id = folder_id
            
            # Traverse up the hierarchy collecting names
            while current_id != 'root' and len(path_parts) < 10:
                try:
                    folder_info = self.service.files().get(
                        fileId=current_id,
                        fields="name, parents"
                    ).execute()
                    
                    folder_name = folder_info.get('name', 'Unknown')
                    path_parts.insert(0, folder_name)
                    
                    parents = folder_info.get('parents', [])
                    if not parents:
                        break
                    
                    current_id = parents[0]
                    
                except HttpError as e:
                    logger.warning(f"‚ö†Ô∏è Cannot get folder info for {current_id}: {e}")
                    break
            
            # Build full path
            if path_parts:
                full_path = "/My Drive/" + "/".join(path_parts)
            else:
                full_path = "/My Drive"
            
            # Cache the result
            self._set_cache(cache_key, full_path)
            
            logger.debug(f"üìÅ Folder path for {folder_id}: {full_path}")
            return full_path
            
        except Exception as e:
            logger.error(f"‚ùå Error building path for {folder_id}: {e}")
            return "/My Drive/Unknown"
    
    def is_selectable_folder(self, folder_depth: int) -> bool:
        """
        Check if a folder at given depth can be selected
        
        Args:
            folder_depth: Depth level of the folder
            
        Returns:
            True if folder can be selected (depth == 4), False otherwise
        """
        return folder_depth == 4
    
    def get_folder_info(self, folder_id: str) -> Dict:
        """
        Get comprehensive information about a folder
        
        Args:
            folder_id: Folder ID to get info for
            
        Returns:
            Dictionary with folder information
        """
        cache_key = self._get_cache_key('info', folder_id)
        cached_result = self._get_cache(cache_key)
        
        if cached_result is not None:
            return cached_result
        
        try:
            folder_info = self.service.files().get(
                fileId=folder_id,
                fields="id, name, parents, createdTime, modifiedTime, size, mimeType"
            ).execute()
            
            depth = self.calculate_folder_depth(folder_id)
            path = self.build_folder_path(folder_id)
            
            info = {
                'id': folder_info['id'],
                'name': folder_info['name'],
                'parents': folder_info.get('parents', []),
                'created': folder_info.get('createdTime'),
                'modified': folder_info.get('modifiedTime'),
                'depth': depth,
                'path': path,
                'selectable': self.is_selectable_folder(depth),
                'mime_type': folder_info.get('mimeType')
            }
            
            # Cache the result
            self._set_cache(cache_key, info)
            
            return info
            
        except HttpError as e:
            logger.error(f"‚ùå HTTP error getting folder info for {folder_id}: {e}")
            return {}
        except Exception as e:
            logger.error(f"‚ùå Error getting folder info for {folder_id}: {e}")
            return {}
    
    def search_folders(self, query: str, max_results: int = 20) -> List[Dict]:
        """
        Search for folders by name
        
        Args:
            query: Search query string
            max_results: Maximum number of results
            
        Returns:
            List of matching folders
        """
        try:
            logger.info(f"üîç Searching folders: {query}")
            
            # Escape query for Google Drive search
            escaped_query = query.replace("'", "\\'").replace("\\", "\\\\")
            
            search_query = f"mimeType='application/vnd.google-apps.folder' and name contains '{escaped_query}' and trashed=false"
            
            results = self.service.files().list(
                q=search_query,
                pageSize=min(max_results, 100),
                fields="files(id, name, parents, createdTime)",
                orderBy="name"
            ).execute()
            
            folders = results.get('files', [])
            
            # Add depth and path information
            enriched_folders = []
            for folder in folders:
                depth = self.calculate_folder_depth(folder['id'])
                path = self.build_folder_path(folder['id'])
                
                enriched_folder = {
                    'id': folder['id'],
                    'name': folder['name'],
                    'parents': folder.get('parents', []),
                    'created': folder.get('createdTime'),
                    'depth': depth,
                    'path': path,
                    'selectable': self.is_selectable_folder(depth)
                }
                enriched_folders.append(enriched_folder)
            
            logger.info(f"‚úÖ Found {len(enriched_folders)} folders matching '{query}'")
            return enriched_folders
            
        except Exception as e:
            logger.error(f"‚ùå Error searching folders: {e}")
            return []
    
    def has_subfolders(self, folder_id: str) -> bool:
        """
        Check if a folder has any subfolders (for UI expand indicators)
        
        Args:
            folder_id: Folder ID to check
            
        Returns:
            True if folder has subfolders, False otherwise
        """
        cache_key = self._get_cache_key('has_subfolders', folder_id)
        cached_result = self._get_cache(cache_key)
        
        if cached_result is not None:
            return cached_result
        
        try:
            query = f"mimeType='application/vnd.google-apps.folder' and '{folder_id}' in parents and trashed=false"
            
            results = self.service.files().list(
                q=query,
                pageSize=1,  # Only need to know if any exist
                fields="files(id)"
            ).execute()
            
            has_folders = len(results.get('files', [])) > 0
            
            # Cache the result
            self._set_cache(cache_key, has_folders)
            
            return has_folders
            
        except Exception as e:
            logger.error(f"‚ùå Error checking subfolders for {folder_id}: {e}")
            return False
    
    def get_folder_statistics(self, folder_id: str) -> Dict:
        """
        Get statistics about a folder (file count, total size, etc.)
        
        Args:
            folder_id: Folder ID to get stats for
            
        Returns:
            Dictionary with folder statistics
        """
        try:
            # Get all files in folder
            query = f"'{folder_id}' in parents and trashed=false"
            
            results = self.service.files().list(
                q=query,
                pageSize=1000,  # Get more files for accurate count
                fields="files(id, name, size, mimeType)"
            ).execute()
            
            files = results.get('files', [])
            
            # Calculate statistics
            total_files = len(files)
            total_size = 0
            video_count = 0
            folder_count = 0
            
            video_mimes = [
                'video/mp4', 'video/avi', 'video/mov', 'video/mkv',
                'video/m4v', 'video/wmv', 'video/flv', 'video/webm'
            ]
            
            for file in files:
                mime_type = file.get('mimeType', '')
                size = int(file.get('size', 0))
                
                total_size += size
                
                if mime_type == 'application/vnd.google-apps.folder':
                    folder_count += 1
                elif any(vm in mime_type for vm in video_mimes):
                    video_count += 1
            
            stats = {
                'total_files': total_files,
                'total_size_bytes': total_size,
                'total_size_mb': round(total_size / (1024 * 1024), 2),
                'video_count': video_count,
                'folder_count': folder_count,
                'other_files': total_files - video_count - folder_count
            }
            
            logger.debug(f"üìä Stats for {folder_id}: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"‚ùå Error getting folder statistics for {folder_id}: {e}")
            return {
                'total_files': 0,
                'total_size_bytes': 0,
                'total_size_mb': 0,
                'video_count': 0,
                'folder_count': 0,
                'other_files': 0
            }
    
    def clear_cache(self):
        """Clear all cached data"""
        self.cache.clear()
        logger.info("üßπ Cleared Google Drive folder service cache")
    
    def get_cache_info(self) -> Dict:
        """Get information about current cache state"""
        valid_entries = 0
        expired_entries = 0
        
        current_time = time.time()
        
        for key, entry in self.cache.items():
            if current_time - entry['timestamp'] < self.cache_duration:
                valid_entries += 1
            else:
                expired_entries += 1
        
        return {
            'total_entries': len(self.cache),
            'valid_entries': valid_entries,
            'expired_entries': expired_entries,
            'cache_duration_seconds': self.cache_duration
        }
```
## üìÑ File: `TG.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/db_utils/TG.py`

```python

```
## üìÑ File: `__init__.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/db_utils/__init__.py`

```python
from .db_utils import find_project_root, get_db_connection

```
## üìÑ File: `db_utils.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/db_utils/db_utils.py`

```python
import sqlite3
import os

# H√†m t√¨m th∆∞ m·ª•c g·ªëc d·ª± √°n d·ª±a tr√™n t√™n th∆∞ m·ª•c
def find_project_root(start_path):
    current_path = os.path.abspath(start_path)
    while os.path.basename(current_path) != "V_Track":
        parent_path = os.path.dirname(current_path)
        if parent_path == current_path:  # ƒê√£ ƒë·∫øn th∆∞ m·ª•c g·ªëc c·ªßa h·ªá th·ªëng (/)
            raise ValueError("Could not find project root (V_Track directory)")
        current_path = parent_path
    return current_path

# X√°c ƒë·ªãnh th∆∞ m·ª•c g·ªëc c·ªßa d·ª± √°n
BASE_DIR = find_project_root(os.path.abspath(__file__))

# ƒê·ªãnh nghƒ©a DB_PATH m·∫∑c ƒë·ªãnh d·ª±a tr√™n BASE_DIR
DEFAULT_DB_PATH = os.path.join(BASE_DIR, "backend/database", "events.db")
os.makedirs(os.path.dirname(DEFAULT_DB_PATH), exist_ok=True)  # T·∫°o th∆∞ m·ª•c database n·∫øu ch∆∞a c√≥

# H√†m l·∫•y DB_PATH t·ª´ processing_config
def get_db_path():
    try:
        conn = sqlite3.connect(DEFAULT_DB_PATH)  # K·∫øt n·ªëi t·∫°m th·ªùi ƒë·ªÉ truy v·∫•n
        cursor = conn.cursor()
        cursor.execute("SELECT db_path FROM processing_config WHERE id = 1")
        result = cursor.fetchone()
        conn.close()
        return result[0] if result else DEFAULT_DB_PATH
    except Exception as e:
        print(f"Error getting DB_PATH from database: {e}")
        return DEFAULT_DB_PATH

DB_PATH = get_db_path()

def get_db_connection():
    if not os.path.exists(DB_PATH):
        raise FileNotFoundError(f"Database file not found: {DB_PATH}")
    if not os.access(DB_PATH, os.R_OK):
        raise PermissionError(f"No read permission for database: {DB_PATH}")
    if not os.access(DB_PATH, os.W_OK):
        raise PermissionError(f"No write permission for database: {DB_PATH}")
    return sqlite3.connect(DB_PATH, check_same_thread=False)

```
## üìÑ File: `__init__.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/account/__init__.py`

```python

```
## üìÑ File: `account.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/account/account.py`

```python

```
## üìÑ File: `query.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/query/query.py`

```python
from flask import Blueprint, request, jsonify
from datetime import datetime
import csv
import io
import os
import base64
import pandas as pd
import json
from io import BytesIO
from modules.db_utils import find_project_root, get_db_connection
from ..utils.file_parser import parse_uploaded_file
from modules.scheduler.db_sync import db_rwlock  # Th√™m import db_rwlock

query_bp = Blueprint('query', __name__)

# X√°c ƒë·ªãnh th∆∞ m·ª•c g·ªëc c·ªßa d·ª± √°n
BASE_DIR = find_project_root(os.path.abspath(__file__))

# ƒê·ªãnh nghƒ©a DB_PATH d·ª±a tr√™n BASE_DIR
DB_PATH = os.path.join(BASE_DIR, "database", "events.db")
os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)

@query_bp.route('/get-csv-headers', methods=['POST'])
def get_csv_headers():
    data = request.get_json()
    file_content = data.get('file_content', '')
    file_path = data.get('file_path', '')
    is_excel = data.get('is_excel', False)

    if file_content:
        if not file_content.strip():
            return jsonify({"error": "File CSV is empty. Please provide a valid CSV file with content."}), 400

        try:
            df = parse_uploaded_file(file_content=file_content, is_excel=is_excel)
            rows = [df.columns.tolist()]
        except Exception as e:
            return jsonify({"error": f"Failed to read file content: {str(e)}. Ensure the content is properly formatted."}), 400
    elif file_path:
        if not os.path.exists(file_path):
            return jsonify({"error": f"File not found at path: {file_path}. Please check the file path and try again."}), 404

        try:
            with open(file_path, "rb") as f:
                file_content = base64.b64encode(f.read()).decode("utf-8")
            df = parse_uploaded_file(file_content=file_content, is_excel=is_excel)
            rows = [df.columns.tolist()]
        except Exception as e:
            return jsonify({"error": f"Failed to read file from path {file_path}: {str(e)}. Ensure the file is accessible and properly formatted."}), 400
    else:
        return jsonify({"error": "No file content or path provided. Please provide either file content or a valid file path."}), 400

    if not rows or len(rows) < 1:
        return jsonify({"error": "CSV file has no header. Please ensure the CSV file contains at least one row with headers."}), 400

    header = rows[0]
    if not header:
        return jsonify({"error": "CSV file header is empty. Please ensure the first row contains valid headers."}), 400

    return jsonify({"headers": header}), 200

@query_bp.route('/parse-csv', methods=['POST'])
def parse_csv():
    data = request.get_json()
    file_content = data.get('file_content', '')
    file_path = data.get('file_path', '')
    column_name = data.get('column_name', 'tracking_codes')
    is_excel = data.get('is_excel', False)

    try:
        if file_content:
            df = parse_uploaded_file(file_content=file_content, is_excel=is_excel)
        elif file_path:
            with open(file_path, "rb") as f:
                file_content = base64.b64encode(f.read()).decode("utf-8")
            df = parse_uploaded_file(file_content=file_content, is_excel=is_excel)
        else:
            return jsonify({"error": "No file provided"}), 400

        if column_name not in df.columns:
            return jsonify({"error": f"C·ªôt '{column_name}' kh√¥ng t·ªìn t·∫°i trong file."}), 400

        values = df[column_name].dropna().astype(str).tolist()
        codes = []
        for val in values:
            # Th·ª≠ c·∫Øt chu·ªói theo d·∫•u ph·∫©y tr∆∞·ªõc
            split_vals = val.split(',')
            if len(split_vals) == 1:  # N·∫øu kh√¥ng c·∫Øt ƒë∆∞·ª£c, th·ª≠ d·∫•u ch·∫•m ph·∫©y
                split_vals = val.split(';')
            codes.extend(v.strip() for v in split_vals if v.strip())
        codes = list(set(codes))  # Lo·∫°i b·ªè tr√πng l·∫∑p

        return jsonify({"tracking_codes": codes}), 200

    except Exception as e:
        return jsonify({"error": f"Failed to parse CSV: {str(e)}. Ensure the file and column name are valid."}), 500

@query_bp.route('/query', methods=['POST'])
def query_events():
    data = request.get_json()
    print(f"Received data: {data}")  # Log d·ªØ li·ªáu nh·∫≠n ƒë∆∞·ª£c
    search_string = data.get('search_string', '')
    default_days = data.get('default_days', 7)  # Th√™m gi√° tr·ªã m·∫∑c ƒë·ªãnh 7 n·∫øu kh√¥ng c√≥ trong request
    from_time = data.get('from_time')
    to_time = data.get('to_time')
    selected_cameras = data.get('selected_cameras', [])  # L·∫•y selected_cameras

    # T√°ch search_string theo d√≤ng v√† lo·∫°i b·ªè s·ªë th·ª© t·ª±
    tracking_codes = []
    if search_string:
        lines = search_string.splitlines()
        for line in lines:
            line = line.strip()
            if line:
                # Lo·∫°i b·ªè s·ªë th·ª© t·ª± (e.g., "1. " ho·∫∑c "2. ")
                code = line.split('. ', 1)[-1].strip()
                if code:
                    tracking_codes.append(code)
    print(f"Parsed tracking_codes from search_string: {tracking_codes}")  # Log tracking_codes ƒë√£ t√°ch

    try:
        if from_time and to_time:
            try:
                from_timestamp = int(datetime.fromisoformat(from_time.replace('Z', '+00:00')).timestamp() * 1000)
                to_timestamp = int(datetime.fromisoformat(to_time.replace('Z', '+00:00')).timestamp() * 1000)
            except ValueError as e:
                return jsonify({"error": f"Invalid time format for from_time or to_time: {str(e)}. Use ISO format (e.g., 2023-10-01T00:00:00Z)."}), 400
        else:
            to_timestamp = int(datetime.now().timestamp() * 1000)
            from_timestamp = to_timestamp - (default_days * 24 * 60 * 60 * 1000)
        print(f"Time range: from_timestamp={from_timestamp}, to_timestamp={to_timestamp}")  # Log kho·∫£ng th·ªùi gian

        with db_rwlock.gen_rlock():  # Th√™m kh√≥a ƒë·ªçc
            conn = get_db_connection()
            cursor = conn.cursor()

            query = """
                SELECT event_id, ts, te, duration, tracking_codes, video_file, packing_time_start, packing_time_end
                FROM events
                WHERE is_processed = 0
            """
            params = []
            # Ch·ªâ th√™m ƒëi·ªÅu ki·ªán th·ªùi gian n·∫øu packing_time_start kh√¥ng null
            if from_timestamp and to_timestamp:
                query += " AND (packing_time_start IS NULL OR (packing_time_start >= ? AND packing_time_start <= ?))"
                params.extend([from_timestamp, to_timestamp])
            if selected_cameras:
                query += " AND camera_name IN ({})".format(','.join('?' * len(selected_cameras)))
                params.extend(selected_cameras)

            print(f"Executing query: {query} with params: {params}")  # Log truy v·∫•n
            cursor.execute(query, params)
            events = cursor.fetchall()
            print(f"Fetched events: {events}")  # Log k·∫øt qu·∫£ truy v·∫•n

            filtered_events = []
            for event in events:
                event_dict = {
                    'event_id': event[0],
                    'ts': event[1],
                    'te': event[2],
                    'duration': event[3],
                    'tracking_codes': event[4],
                    'video_file': event[5],
                    'packing_time_start': event[6],
                    'packing_time_end': event[7]
                }
                print(f"Raw tracking_codes for event {event[0]}: {event[4]}")  # Log gi√° tr·ªã th√¥ c·ªßa tracking_codes
                try:
                    tracking_codes_list = json.loads(event[4]) if event[4] else []
                    if not isinstance(tracking_codes_list, list):
                        raise ValueError("tracking_codes is not a list")
                except Exception:
                    print(f"[WARN] tracking_codes fallback for event {event[0]}")
                    tracking_codes_list = []
                    if event[4]:
                        raw = event[4].strip("[]").replace("'", "").replace('"', "")
                        tracking_codes_list = [code.strip() for code in raw.split(',')]
                print(f"Parsed tracking_codes for event {event[0]}: {tracking_codes_list}")  # Log tracking_codes ƒë√£ parse
                if not tracking_codes:
                    filtered_events.append(event_dict)
                else:
                    for code in tracking_codes:
                        if code in tracking_codes_list:
                            filtered_events.append(event_dict)
                            break
            print(f"Filtered events: {filtered_events}")  # Log k·∫øt qu·∫£ sau khi l·ªçc

            conn.close()
        return jsonify({'events': filtered_events}), 200
    except Exception as e:
        print(f"Error in query_events: {str(e)}")  # Log l·ªói chi ti·∫øt
        return jsonify({"error": f"Failed to query events: {str(e)}. Ensure the database is accessible and the events table exists."}), 500
```
## üìÑ File: `__init__.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/query/__init__.py`

```python

```
## üìÑ File: `trigger_processor.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/trigger_processor.py`

```python
import cv2
import logging
import time
import os
from datetime import timedelta
from modules.db_utils import get_db_connection
from modules.scheduler.db_sync import db_rwlock

def run_trigger_logic(
    core_sampler,
    video_capture,
    video_metadata,
    trigger_roi_coords_for_state_check,
    get_log_handle_callback
):
    core_sampler.logger.info(f"TRIGGER_PROCESSOR: Starting for {video_metadata['base_video_name']}")
    frame_idx_counter_tr = 0
    frame_states_buffer_list_tr = []
    mvd_buffer_list_tr = []
    last_recorded_state_tr = None
    last_recorded_mvd_tr = ""
    second = 0
    current_start_second = 0
    current_end_second = core_sampler.log_segment_duration
    log_file = os.path.join(core_sampler.log_dir_output_segments, f"log_{video_metadata['base_video_name']}_{current_start_second:04d}_{current_end_second:04d}.txt")
    log_file_handle = open(log_file, 'w')
    log_file_handle.write(f"# Start: {current_start_second}, End: {current_end_second}, Start_Time: {(video_metadata['start_time_obj'] + timedelta(seconds=current_start_second)).strftime('%Y-%m-%d %H:%M:%S')}, Camera_Name: {video_metadata['camera_name']}, Video_File: {video_metadata['absolute_video_path']}\n")
    log_file_handle.flush()
    with db_rwlock.gen_wlock():
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT 1 FROM processed_logs WHERE log_file = ?", (log_file,))
        log_exists = cursor.fetchone()
        if not log_exists:
            cursor.execute("INSERT INTO processed_logs (log_file, is_processed) VALUES (?, 0)", (log_file,))
        conn.commit()
        conn.close()
    while video_capture.isOpened():
        ret, bgr_frame = video_capture.read()
        if not ret: break
        frame_idx_counter_tr += 1
        if frame_idx_counter_tr % core_sampler.frame_interval != 0: continue
        state_val, mvd_val = core_sampler._internal_process_frame_qr(
            bgr_frame,
            frame_idx_counter_tr,
            trigger_roi_coords_for_state_check=trigger_roi_coords_for_state_check
        )
        frame_states_buffer_list_tr.append(state_val)
        mvd_buffer_list_tr.append(mvd_val)
        second_in_video = (frame_idx_counter_tr - 1) / core_sampler.fps
        second = round(second_in_video)
        if second >= current_end_second:
            log_file_handle.close()
            current_start_second = current_end_second
            current_end_second += core_sampler.log_segment_duration
            log_file = os.path.join(core_sampler.log_dir_output_segments, f"log_{video_metadata['base_video_name']}_{current_start_second:04d}_{current_end_second:04d}.txt")
            log_file_handle = open(log_file, 'w')
            log_file_handle.write(f"# Start: {current_start_second}, End: {current_end_second}, Start_Time: {(video_metadata['start_time_obj'] + timedelta(seconds=current_start_second)).strftime('%Y-%m-%d %H:%M:%S')}, Camera_Name: {video_metadata['camera_name']}, Video_File: {video_metadata['absolute_video_path']}\n")
            log_file_handle.flush()
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("SELECT 1 FROM processed_logs WHERE log_file = ?", (log_file,))
                log_exists = cursor.fetchone()
                if not log_exists:
                    cursor.execute("INSERT INTO processed_logs (log_file, is_processed) VALUES (?, 0)", (log_file,))
                conn.commit()
                conn.close()
        if len(frame_states_buffer_list_tr) == 5:
            on_count_tr = frame_states_buffer_list_tr.count("On")
            determined_final_state_tr = "On" if on_count_tr >= 3 else "Off"
            determined_final_mvd_tr = mvd_buffer_list_tr[-1] if mvd_buffer_list_tr else ""
            if determined_final_state_tr != last_recorded_state_tr or determined_final_mvd_tr != last_recorded_mvd_tr:
                log_handle_tr = get_log_handle_callback(second_in_video)
                log_handle_tr.write(f"{second},{determined_final_state_tr},{determined_final_mvd_tr}\n")
                log_handle_tr.flush()
                last_recorded_state_tr = determined_final_state_tr
                last_recorded_mvd_tr = determined_final_mvd_tr
            frame_states_buffer_list_tr.clear()
            mvd_buffer_list_tr.clear()
    log_file_handle.close()
    core_sampler.logger.info(f"TRIGGER_PROCESSOR: Finished for {video_metadata['base_video_name']}")
```
## üìÑ File: `IdleMonitor.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/IdleMonitor.py`

```python
import cv2
import mediapipe as mp
import queue
import os
import logging
from datetime import datetime
import uuid
from modules.config.logging_config import get_logger

class IdleMonitor:
    def __init__(self, processing_config=None):
        """Kh·ªüi t·∫°o IdleMonitor v·ªõi queue v√† processing_config."""
        self.video_file = None
        self.logger = get_logger("app", {"video_id": None})
        self.logger.setLevel(logging.INFO)
        self.work_block_queue = queue.Queue()  # Queue l∆∞u work block
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.6)
        self.IDLE_GAP = 120  # seconds
        self.HAND_SAMPLE_INTERVAL = 1  # seconds
        self.MIN_WORK_BLOCK = 10  # seconds
        self.MIN_PACKING_TIME = processing_config.get('min_packing_time', 5) if processing_config else 5  # seconds
        self.CHUNK_SIZE = int(self.MIN_PACKING_TIME * 0.8) # seconds
        self.video_id = str(uuid.uuid4())  # ƒê·ªãnh danh duy nh·∫•t cho video

    def process_video(self, video_file, camera_name, packing_area):
        """X·ª≠ l√Ω video, x√°c ƒë·ªãnh work block, l∆∞u v√†o queue, d√πng packing_area t·ª´ program_runner."""
        self.video_file = video_file
        self.logger = get_logger("app", {"video_id": os.path.basename(self.video_file)})
        # ƒê·ªçc video
        cap = cv2.VideoCapture(video_file)
        if not cap.isOpened():
            self.logger.error(f"Failed to open video: {video_file}")
            return

        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        video_duration = int(total_frames / fps)
        self.logger.info(f"Processing video: {video_file}, Duration: {video_duration}s, Video ID: {self.video_id}")

        # Ki·ªÉm tra packing_area
        roi = packing_area
        if not roi:
            self.logger.warning(f"No packing_area for {camera_name}, using full frame")
            roi = None

        hand_timeline = []
        event_id = 0

        # Qu√©t video theo chunk
        sec = 0
        while sec < video_duration:
            chunk_end = min(sec + self.CHUNK_SIZE, video_duration)
            chunk_has_hand = False
            check_time = sec
            # Qu√©t ƒë·ªÅu to√†n chunk
            while check_time < chunk_end:
                frame_id = int(check_time * fps)
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)
                ret, frame = cap.read()
                if not ret:
                    break

                # √Åp d·ª•ng ROI n·∫øu c√≥
                if roi:
                    x, y, w, h = roi
                    frame_height, frame_width = frame.shape[:2]
                    if w > 0 and h > 0 and y + h <= frame_height and x + w <= frame_width:
                        frame = frame[y:y+h, x:x+w]
                    else:
                        self.logger.warning(f"Invalid ROI for frame {frame_id}: {roi}")
                        frame = frame

                # Hand detection
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = self.hands.process(rgb_frame)
                if results.multi_hand_landmarks is not None:
                    chunk_has_hand = True
                    break
                check_time += self.HAND_SAMPLE_INTERVAL

            hand_timeline.append(chunk_has_hand)
            #self.logger.info(f"[Chunk {sec:04d}-{chunk_end:04d}s] Hand: {chunk_has_hand}, Video ID: {self.video_id}")
            sec += self.CHUNK_SIZE

        # Ph√°t hi·ªán idle block
        idle_gap_list = []
        idle_candidate_start = None
        for tick_time, hand_detected in enumerate(hand_timeline):
            if hand_detected:
                if idle_candidate_start is not None:
                    idle_duration = tick_time - idle_candidate_start
                    if idle_duration * self.CHUNK_SIZE >= self.IDLE_GAP:
                        idle_gap_list.append({'start': idle_candidate_start * self.CHUNK_SIZE, 'end': tick_time * self.CHUNK_SIZE})
                    idle_candidate_start = None
            else:
                if idle_candidate_start is None:
                    idle_candidate_start = tick_time

        if idle_candidate_start is not None:
            idle_duration = len(hand_timeline) - idle_candidate_start
            if idle_duration * self.CHUNK_SIZE >= self.IDLE_GAP:
                idle_gap_list.append({'start': idle_candidate_start * self.CHUNK_SIZE, 'end': len(hand_timeline) * self.CHUNK_SIZE})

        # Ph√°t hi·ªán work block
        work_blocks = []
        prev_end = 0
        for idle in idle_gap_list:
            if idle['start'] > prev_end:
                work_blocks.append({'start': prev_end, 'end': idle['start']})
            prev_end = idle['end']
        if prev_end < len(hand_timeline) * self.CHUNK_SIZE:
            work_blocks.append({'start': prev_end, 'end': len(hand_timeline) * self.CHUNK_SIZE})

        # L∆∞u work block v√†o queue
        for idx, block in enumerate(work_blocks):
            duration = block['end'] - block['start']
            event_id += 1
            if duration < self.MIN_WORK_BLOCK:
                self.logger.info(f"Skipping work block {idx+1}: duration {duration}s < {self.MIN_WORK_BLOCK}s")
                continue
            self.work_block_queue.put({
                'video_id': self.video_id,
                'event_id': f"evt_{event_id:03d}",
                'file_path': video_file,
                'start_time': block['start'],
                'end_time': block['end']
            })
            self.logger.info(f"Work block {idx+1}: {block['start']}s --> {block['end']}s (duration: {duration}s), Video ID: {self.video_id}")
            if duration < self.MIN_WORK_BLOCK:
                self.logger.warning(f"Block shorter than {self.MIN_WORK_BLOCK}s")

        cap.release()
        self.hands.close()

    def get_work_block_queue(self):
        """Tr·∫£ v·ªÅ queue ch·ª©a work block."""
        return self.work_block_queue
```
## üìÑ File: `qr_detector.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/qr_detector.py`

```python
import cv2
import os
import json
import logging
import queue
import threading
import time
import glob
import traceback
from datetime import datetime

# ƒê·∫£m b·∫£o th∆∞ m·ª•c LOG t·ªìn t·∫°i
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
LOG_DIR = os.path.join(BASE_DIR, "resources", "output_clips", "LOG")
os.makedirs(LOG_DIR, exist_ok=True)

# ‚úÖ FIXED: Use direct logging.basicConfig instead of modules.config.logging_config
log_file_path = os.path.join(LOG_DIR, f"qr_detector_{datetime.now().strftime('%Y-%m-%d')}.log")
logging.basicConfig(
    filename=log_file_path,
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
logger.info("QR Detector logging initialized")

# ƒê∆∞·ªùng d·∫´n t·ªõi m√¥ h√¨nh WeChat QRCode (t∆∞∆°ng ƒë·ªëi)
MODEL_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "models", "wechat_qr")
DETECT_PROTO = os.path.join(MODEL_DIR, "detect.prototxt")
DETECT_MODEL = os.path.join(MODEL_DIR, "detect.caffemodel")
SR_PROTO = os.path.join(MODEL_DIR, "sr.prototxt")
SR_MODEL = os.path.join(MODEL_DIR, "sr.caffemodel")

# ƒê∆∞·ªùng d·∫´n l∆∞u ·∫£nh
CAMERA_ROI_DIR = os.path.join(BASE_DIR, "resources", "output_clips", "CameraROI")

def select_qr_roi(video_path, camera_id, roi_frame_path, step="mvd"):
    """
    Cho ph√©p ng∆∞·ªùi d√πng v·∫Ω ROI cho m√£ QR (1 ho·∫∑c 2 v√πng cho mvd), sau ƒë√≥ x·ª≠ l√Ω video.
    Args:
        video_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file video.
        camera_id (str): ID c·ªßa camera.
        roi_frame_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn ·∫£nh t·∫°m cu·ªëi c√πng t·ª´ b∆∞·ªõc tr∆∞·ªõc (ƒë√£ c√≥ ROI v·∫Ω s·∫µn).
        step (str): Giai ƒëo·∫°n hi·ªán t·∫°i (mvd).
    Returns:
        dict: {'success': bool, 'rois': [{'x': int, 'y': int, 'w': int, 'h': int, 'type': str}, ...], 
               'roi_frame': str, 'qr_detected': bool, 'qr_detected_roi1': bool, 'qr_detected_roi2': bool, 
               'qr_content': str, 'trigger_detected': bool, 'table_type': str}
              ho·∫∑c {'success': false, 'error': str}
    """
    try:
        logger.debug(f"[MVD] B·∫Øt ƒë·∫ßu select_qr_roi v·ªõi video_path: {video_path}, camera_id: {camera_id}, roi_frame_path: {roi_frame_path}, step: {step}")

        # Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa c√°c m√¥ h√¨nh
        for model_file in [DETECT_PROTO, DETECT_MODEL, SR_PROTO, SR_MODEL]:
            logger.debug(f"[MVD] Ki·ªÉm tra file m√¥ h√¨nh: {model_file}")
            if not os.path.exists(model_file):
                logger.error(f"[MVD] File m√¥ h√¨nh kh√¥ng t√¨m th·∫•y: {model_file}")
                cv2.destroyAllWindows()
                return {"success": False, "error": f"File m√¥ h√¨nh kh√¥ng t√¨m th·∫•y: {model_file}"}

        # Ki·ªÉm tra file ·∫£nh v√† video
        logger.debug(f"[MVD] Ki·ªÉm tra ·∫£nh t·∫°m: {roi_frame_path}")
        if not os.path.exists(roi_frame_path):
            logger.error(f"[MVD] ·∫¢nh t·∫°m kh√¥ng t·ªìn t·∫°i: {roi_frame_path}")
            cv2.destroyAllWindows()
            return {"success": False, "error": f"·∫¢nh t·∫°m kh√¥ng t·ªìn t·∫°i: {roi_frame_path}"}
        
        logger.debug(f"[MVD] Ki·ªÉm tra video: {video_path}")
        if not os.path.exists(video_path):
            logger.error(f"[MVD] Video kh√¥ng t·ªìn t·∫°i: {video_path}")
            cv2.destroyAllWindows()
            return {"success": False, "error": f"Video kh√¥ng t·ªìn t·∫°i: {video_path}"}

        # ƒê·ªçc frame t·ª´ ·∫£nh t·∫°m
        try:
            logger.debug(f"[MVD] ƒê·ªçc ·∫£nh t·∫°m: {roi_frame_path}")
            frame = cv2.imread(roi_frame_path)
            if frame is None:
                logger.error(f"[MVD] Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh t·∫°m: {roi_frame_path}")
                cv2.destroyAllWindows()
                return {"success": False, "error": f"Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh t·∫°m: {roi_frame_path}"}
            logger.debug(f"[MVD] K√≠ch th∆∞·ªõc ·∫£nh t·∫°m: {frame.shape[:2]}")
        except Exception as e:
            logger.error(f"[MVD] OpenCV imread error: {str(e)}\n{traceback.format_exc()}")
            cv2.destroyAllWindows()
            return {"success": False, "error": f"OpenCV imread error: {str(e)}"}

        # Ch·ªçn lo·∫°i b√†n ƒë√≥ng g√≥i
        table_type = None
        while table_type is None:
            current_frame = frame.copy()
            window_title = "**** Nhan 1 cho ban tieu chuan, 2 cho ban khong tieu chuan, q thoat ****"
            cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)
            cv2.imshow(window_title, current_frame)
            key = cv2.waitKey(0) & 0xFF
            cv2.destroyAllWindows()
            if key == ord('1'):
                table_type = "standard"
                logger.debug("[MVD] Ch·ªçn b√†n ti√™u chu·∫©n")
            elif key == ord('2'):
                table_type = "non_standard"
                logger.debug("[MVD] Ch·ªçn b√†n kh√¥ng ti√™u chu·∫©n")
            elif key == ord('q'):
                logger.debug("[MVD] Ng∆∞·ªùi d√πng tho√°t")
                cv2.destroyAllWindows()
                return {"success": False, "error": "Ng∆∞·ªùi d√πng tho√°t"}
            else:
                logger.debug("[MVD] Ph√≠m kh√¥ng h·ª£p l·ªá, hi·ªÉn th·ªã l·∫°i h∆∞·ªõng d·∫´n")
                continue

        while True:
            # T·∫°o b·∫£n sao m·ªõi c·ªßa frame m·ªói l·∫ßn v·∫Ω l·∫°i
            current_frame = frame.copy()
            rois = []

            # V·∫Ω ROI 1 (m√£ QR)
            window_title = "**** Keo chuot ve vung ma QR. Enter xac nhan, Esc huy ****"
            try:
                logger.debug("[MVD] G·ªçi cv2.selectROI cho MVD ROI 1")
                cv2.destroyAllWindows()
                cv2.startWindowThread()
                cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)
                roi1 = cv2.selectROI(window_title, current_frame, showCrosshair=True, fromCenter=False)
                cv2.destroyAllWindows()
                x1, y1, w1, h1 = map(int, roi1)
                if w1 > 0 and h1 > 0:
                    rois.append({"x": x1, "y": y1, "w": w1, "h": h1, "type": "mvd"})
                    cv2.rectangle(current_frame, (x1, y1), (x1 + w1, y1 + h1), (0, 0, 255), 2)  # M√†u ƒë·ªè cho MVD
                    cv2.putText(current_frame, "ShippingLabel", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                    logger.debug(f"[MVD] ƒê√£ ch·ªçn MVD ROI 1: x={x1}, y={y1}, w={w1}, h={h1}")
                    cv2.namedWindow("**** Da ve vung ma QR ****", cv2.WINDOW_NORMAL)
                    cv2.imshow("**** Da ve vung ma QR ****", current_frame)
                    cv2.waitKey(500)
                    cv2.destroyAllWindows()
                else:
                    logger.debug("[MVD] ROI 1 kh√¥ng h·ª£p l·ªá")
                    cv2.namedWindow("**** Loi: ROI khong hop le. Ve lai vung ma QR ****", cv2.WINDOW_NORMAL)
                    cv2.imshow("**** Loi: ROI khong hop le. Ve lai vung ma QR ****", current_frame)
                    cv2.waitKey(2000)
                    cv2.destroyAllWindows()
                    continue

                # Ch·ªâ v·∫Ω ROI 2 (trigger) cho b√†n ti√™u chu·∫©n
                if table_type == "standard":
                    window_title = "**** Ve vung ma trigger (QR: TimeGo). Enter xac nhan, Esc huy ****"
                    roi2_label = "Trigger"
                    logger.debug("[MVD] G·ªçi cv2.selectROI cho ROI 2")
                    cv2.destroyAllWindows()
                    cv2.startWindowThread()
                    cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)
                    roi2 = cv2.selectROI(window_title, current_frame, showCrosshair=True, fromCenter=False)
                    cv2.destroyAllWindows()
                    x2, y2, w2, h2 = map(int, roi2)
                    if w2 > 0 and h2 > 0:
                        roi_type = "trigger"
                        rois.append({"x": x2, "y": y2, "w": w2, "h": h2, "type": roi_type})
                        cv2.rectangle(current_frame, (x2, y2), (x2 + w2, y2 + h2), (0, 0, 255), 2)  # M√†u ƒë·ªè
                        cv2.putText(current_frame, roi2_label, (x2, y2 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                        logger.debug(f"[MVD] ƒê√£ ch·ªçn ROI 2: x={x2}, y={y2}, w={w2}, h={h2}, type={roi_type}")
            except Exception as e:
                logger.error(f"[MVD] OpenCV selectROI error: {str(e)}\n{traceback.format_exc()}")
                cv2.destroyAllWindows()
                return {"success": False, "error": f"OpenCV selectROI error: {str(e)}"}

            # L∆∞u ·∫£nh v√†o CameraROI n·∫øu c√≥ ROI h·ª£p l·ªá
            if rois:
                roi_frame_path_new = os.path.join(CAMERA_ROI_DIR, f"camera_{camera_id}_roi_MVD.jpg")
                try:
                    logger.debug(f"[MVD] L∆∞u ·∫£nh v·ªõi ROI v√†o: {roi_frame_path_new}")
                    ret = cv2.imwrite(roi_frame_path_new, current_frame)
                    if not ret:
                        logger.error(f"[MVD] Kh√¥ng th·ªÉ l∆∞u ·∫£nh t·∫°i: {roi_frame_path_new}")
                        cv2.destroyAllWindows()
                        return {"success": False, "error": f"Kh√¥ng th·ªÉ l∆∞u ·∫£nh t·∫°i {roi_frame_path_new}"}
                    logger.info(f"[MVD] ƒê√£ l∆∞u frame v·ªõi ROI v√†o: {roi_frame_path_new}")
                except Exception as e:
                    logger.error(f"[MVD] OpenCV imwrite error: {str(e)}\n{traceback.format_exc()}")
                    cv2.destroyAllWindows()
                    return {"success": False, "error": f"OpenCV imwrite error: {str(e)}"}
                break
            else:
                logger.debug("[MVD] Kh√¥ng ch·ªçn ƒë∆∞·ª£c ROI h·ª£p l·ªá, hi·ªÉn th·ªã l·∫°i ·∫£nh tr∆∞·ªõc ƒë√≥ ƒë·ªÉ v·∫Ω l·∫°i.")
                cv2.namedWindow("**** Loi: ROI khong hop le. Ve lai vung ma QR ****", cv2.WINDOW_NORMAL)
                cv2.imshow("**** Loi: ROI khong hop le. Ve lai vung ma QR ****", current_frame)
                cv2.waitKey(2000)
                cv2.destroyAllWindows()
                continue

        # Ki·ªÉm tra t√≠nh t∆∞∆°ng th√≠ch c·ªßa ·∫£nh packing v·ªõi MVD
        logger.debug(f"[MVD] Ki·ªÉm tra t√≠nh t∆∞∆°ng th√≠ch c·ªßa ·∫£nh packing v·ªõi MVD: {roi_frame_path}")

        # Kh·ªüi t·∫°o danh s√°ch h√†ng ƒë·ª£i v√† c·ªù tho√°t
        frame_queues = [queue.Queue(maxsize=50) for _ in range(len(rois))]
        exit_flag = threading.Event()
        qr_detected = False
        qr_detected_roi1 = False
        qr_detected_roi2 = False
        qr_content = ""
        trigger_detected = False

        def process_roi(video_file, roi_index, x, y, w, h, interval=5):
            nonlocal qr_detected, qr_detected_roi1, qr_detected_roi2, qr_content, trigger_detected
            try:
                logger.debug(f"[MVD] Kh·ªüi t·∫°o WeChatQRCode cho ROI {roi_index + 1}")
                local_detector = cv2.wechat_qrcode_WeChatQRCode(DETECT_PROTO, DETECT_MODEL, SR_PROTO, SR_MODEL)
                logger.debug(f"[MVD] WeChatQRCode kh·ªüi t·∫°o th√†nh c√¥ng cho ROI {roi_index + 1}")
            except Exception as e:
                logger.error(f"[MVD] OpenCV WeChatQRCode error in ROI {roi_index + 1}: {str(e)}\n{traceback.format_exc()}")
                return

            try:
                logger.debug(f"[MVD] M·ªü video cho ROI {roi_index + 1}: {video_file}")
                cap = cv2.VideoCapture(video_file)
                if not cap.isOpened():
                    logger.error(f"[MVD] Kh√¥ng th·ªÉ m·ªü video '{video_file}' cho ROI {roi_index + 1}")
                    return
                logger.debug(f"[MVD] Video m·ªü th√†nh c√¥ng cho ROI {roi_index + 1}")
            except Exception as e:
                logger.error(f"[MVD] OpenCV VideoCapture error in ROI {roi_index + 1}: {str(e)}\n{traceback.format_exc()}")
                return

            frame_count = 0
            start_time = time.time()

            while not exit_flag.is_set():
                try:
                    ret, frame = cap.read()
                    if not ret:
                        logger.debug(f"[MVD] K·∫øt th√∫c video '{video_file}' (ROI {roi_index + 1})")
                        break

                    frame_count += 1
                    if frame_count % interval != 0:
                        continue

                    logger.debug(f"[MVD] X·ª≠ l√Ω frame {frame_count} cho ROI {roi_index + 1}")
                    roi_frame = frame[y:y+h, x:x+w]
                    if roi_frame.size == 0 or roi_frame.shape[0] == 0 or roi_frame.shape[1] == 0:
                        logger.warning(f"[MVD] ROI {roi_index + 1} kh√¥ng h·ª£p l·ªá, b·ªè qua frame")
                        continue

                    if len(roi_frame.shape) == 2:
                        roi_frame = cv2.cvtColor(roi_frame, cv2.COLOR_GRAY2BGR)

                    logger.debug(f"[MVD] Ph√°t hi·ªán QR trong ROI {roi_index + 1}")
                    texts, points = local_detector.detectAndDecode(roi_frame)
                    if texts:
                        qr_detected = True
                        if roi_index == 0:
                            qr_detected_roi1 = True
                        elif roi_index == 1 and table_type == "standard":
                            qr_detected_roi2 = True
                        qr_content = texts[0]  # L∆∞u n·ªôi dung QR ƒë·∫ßu ti√™n
                        # Ki·ªÉm tra trigger cho ROI 2 (b√†n ti√™u chu·∫©n)
                        if table_type == "standard" and roi_index == 1 and texts[0].lower() == "timego":
                            trigger_detected = True
                            logger.info(f"[MVD] [ROI {roi_index + 1}] Ph√°t hi·ªán trigger: {texts[0]}")
                        for text, box in zip(texts, points):
                            logger.info(f"[MVD] [ROI {roi_index + 1}] N·ªôi dung m√£ QR: {text}")
                            # V·∫Ω khung vi·ªÅn QR
                            for i in range(4):
                                pt1 = tuple(map(int, box[i]))
                                pt2 = tuple(map(int, box[(i + 1) % 4]))
                                cv2.line(roi_frame, pt1, pt2, (0, 255, 0), 2)
                            # Hi·ªÉn th·ªã n·ªôi dung QR d∆∞·ªõi khung vi·ªÅn
                            bottom_left = tuple(map(int, box[2]))  # G√≥c d∆∞·ªõi tr√°i
                            cv2.putText(roi_frame, text[:20], (bottom_left[0], bottom_left[1] + 30), 
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

                    elapsed_time = time.time() - start_time
                    elapsed_time_text = f"Time: {elapsed_time:.1f}"
                    cv2.putText(roi_frame, elapsed_time_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
                    cv2.putText(roi_frame, "Dang phat hien ma QR. Noi dung hien thi neu tim thay", 
                                (10, roi_frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                    frame_queues[roi_index].put(roi_frame)
                    logger.debug(f"[MVD] ƒê√£ ƒë·∫©y frame cho ROI {roi_index + 1} v√†o h√†ng ƒë·ª£i")
                except Exception as e:
                    logger.error(f"[MVD] OpenCV processing error in process_roi (ROI {roi_index + 1}): {str(e)}\n{traceback.format_exc()}")
                    break

            logger.debug(f"[MVD] Gi·∫£i ph√≥ng video capture cho ROI {roi_index + 1}")
            cap.release()

        # Kh·ªüi ch·∫°y lu·ªìng x·ª≠ l√Ω video cho t·ª´ng ROI
        threads = []
        for i, roi in enumerate(rois):
            if roi["w"] > 0 and roi["h"] > 0:
                logger.debug(f"[MVD] Kh·ªüi ch·∫°y thread cho ROI {i + 1}")
                thread = threading.Thread(target=process_roi, args=(video_path, i, roi["x"], roi["y"], roi["w"], roi["h"], 5))
                thread.start()
                threads.append(thread)
                logger.info(f"[MVD] Thread cho ROI {i + 1} ƒë√£ kh·ªüi ch·∫°y")
            else:
                logger.warning(f"[MVD] ROI {i + 1} kh√¥ng h·ª£p l·ªá, b·ªè qua")

        # Kh·ªüi t·∫°o c·ª≠a s·ªï cho t·ª´ng ROI
        for i in range(len(frame_queues)):
            try:
                logger.debug(f"[MVD] Kh·ªüi t·∫°o c·ª≠a s·ªï cho ROI {i + 1}")
                window_title = f"**** Dang phat hien ma QR. Noi dung hien thi neu tim thay (ROI {i + 1}) ****"
                cv2.namedWindow(window_title, cv2.WINDOW_AUTOSIZE)
                logger.debug(f"[MVD] C·ª≠a s·ªï cho ROI {i + 1} ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o")
            except Exception as e:
                logger.error(f"[MVD] OpenCV namedWindow error: {str(e)}\n{traceback.format_exc()}")
                cv2.destroyAllWindows()
                return {"success": False, "error": f"OpenCV namedWindow error: {str(e)}"}

        # Hi·ªÉn th·ªã m·ªói ROI trong c·ª≠a s·ªï ri√™ng
        while any(thread.is_alive() for thread in threads) or any(not q.empty() for q in frame_queues):
            for i in range(len(frame_queues)):
                try:
                    frame = frame_queues[i].get(timeout=0.1)
                    window_name = f"**** Dang phat hien ma QR. Noi dung hien thi neu tim thay (ROI {i + 1}) ****"
                    cv2.imshow(window_name, frame)
                    logger.debug(f"[MVD] Hi·ªÉn th·ªã frame cho {window_name}")
                except queue.Empty:
                    pass
                except Exception as e:
                    logger.error(f"[MVD] OpenCV imshow error in loop: {str(e)}\n{traceback.format_exc()}")
                    cv2.destroyAllWindows()
                    return {"success": False, "error": f"OpenCV imshow error: {str(e)}"}

            try:
                if cv2.waitKey(10) & 0xFF == ord('q'):
                    logger.debug("[MVD] Nh·∫≠n l·ªánh tho√°t t·ª´ ng∆∞·ªùi d√πng")
                    exit_flag.set()
                    break
            except Exception as e:
                logger.error(f"[MVD] OpenCV waitKey error: {str(e)}\n{traceback.format_exc()}")
                cv2.destroyAllWindows()
                return {"success": False, "error": f"OpenCV waitKey error: {str(e)}"}

        logger.debug("[MVD] ƒê√≥ng t·∫•t c·∫£ c·ª≠a s·ªï OpenCV")
        cv2.destroyAllWindows()
        for thread in threads:
            logger.debug(f"[MVD] Ch·ªù thread ROI {threads.index(thread) + 1} k·∫øt th√∫c")
            thread.join()

        # L∆∞u k·∫øt qu·∫£ v√†o /tmp/qr_roi.json
        result = {
            "success": True,
            "rois": rois,
            "roi_frame": os.path.relpath(roi_frame_path_new, BASE_DIR),
            "qr_detected": qr_detected,
            "qr_detected_roi1": qr_detected_roi1,
            "qr_detected_roi2": qr_detected_roi2 if table_type == "standard" else False,
            "qr_content": qr_content,
            "trigger_detected": trigger_detected,
            "table_type": table_type
        }
        logger.debug(f"[MVD] L∆∞u k·∫øt qu·∫£ v√†o /tmp/qr_roi.json: {result}")
        try:
            with open("/tmp/qr_roi.json", "w") as f:
                json.dump(result, f)
            logger.info("[MVD] ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o /tmp/qr_roi.json")
        except Exception as e:
            logger.error(f"[MVD] L·ªói khi l∆∞u /tmp/qr_roi.json: {str(e)}\n{traceback.format_exc()}")
            cv2.destroyAllWindows()
            return {"success": False, "error": f"L·ªói khi l∆∞u /tmp/qr_roi.json: {str(e)}"}

        logger.info(f"[MVD] Ho√†n t·∫•t select_qr_roi cho camera_id: {camera_id}, step: {step}")
        cv2.destroyAllWindows()
        return result

    except Exception as e:
        logger.error(f"[MVD] L·ªói trong select_qr_roi: {str(e)}\n{traceback.format_exc()}")
        cv2.destroyAllWindows()
        return {"success": False, "error": f"L·ªói h·ªá th·ªëng: {str(e)}"}

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 4:
        logger.error("Usage: python3 qr_detector.py <video_path> <camera_id> <roi_frame_path>")
        sys.exit(1)

    video_path = sys.argv[1]
    camera_id = sys.argv[2]
    roi_frame_path = sys.argv[3]
    try:
        result = select_qr_roi(video_path, camera_id, roi_frame_path, step="mvd")
        if not result["success"]:
            logger.error(result["error"])
            sys.exit(1)
    except Exception as e:
        logger.error(f"[MVD] L·ªói khi ch·∫°y script: {str(e)}\n{traceback.format_exc()}")
        cv2.destroyAllWindows()
        sys.exit(1)
```
## üìÑ File: `roi_preview.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/roi_preview.py`

```python
import cv2
import argparse
import os
import logging

def setup_logging():
    log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "resources", "output_clips", "LOG")
    os.makedirs(log_dir, exist_ok=True)
    log_file_path = os.path.join(log_dir, f"roi_preview_{os.getpid()}.log")
    logging.basicConfig(
        filename=log_file_path,
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    logging.info(f"ROI Preview started with PID {os.getpid()}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--video', required=True, help='File video .mp4')
    parser.add_argument('--roi', nargs=4, type=int, required=True, help='T·ªça ƒë·ªô ROI: x y w h')
    args = parser.parse_args()

    setup_logging()
    x, y, w, h = args.roi
    cap = cv2.VideoCapture(args.video)
    if not cap.isOpened():
        logging.error(f"Cannot open {args.video}")
        return

    win = "ROI Preview"
    cv2.namedWindow(win, cv2.WINDOW_NORMAL)
    cv2.resizeWindow(win, w, h)

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        crop = frame[y:y+h, x:x+w]
        cv2.imshow(win, crop)
        if cv2.waitKey(30) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()
    logging.info("ROI Preview closed")

if __name__ == "__main__":
    main()
```
## üìÑ File: `event_detector.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/event_detector.py`

```python
from flask import Blueprint, jsonify
import os
import sqlite3
import logging
from datetime import datetime
from modules.db_utils import get_db_connection
from modules.scheduler.db_sync import db_rwlock
from modules.config.logging_config import get_logger


# ƒê·ªãnh nghƒ©a BASE_DIR
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

event_detector_bp = Blueprint('event_detector', __name__)

def calculate_duration(ts, te):
    if ts is None or te is None:
        return None
    if te < ts:
        return None
    return te - ts

def process_single_log(log_file_path):
    if not os.path.isfile(log_file_path):
        logging.warning(f"Log file not found: {log_file_path}, skipping.")
        return
    # Kh·ªüi t·∫°o logger v·ªõi context log_file
    logger = get_logger(__name__, {"log_file": log_file_path})
    logger.info("Logging initialized for process_single_log")

    try:
        with db_rwlock.gen_wlock():
            conn = get_db_connection()
            cursor = conn.cursor()

            # Ki·ªÉm tra tr·∫°ng th√°i is_processed c·ªßa file log
            cursor.execute("SELECT is_processed FROM processed_logs WHERE log_file = ?", (log_file_path,))
            result = cursor.fetchone()
            if result and result[0] == 1:
                logging.info(f"Log file {log_file_path} already processed, skipping")
                conn.close()
                return

            with open(log_file_path, "r") as f:
                header = f.readline().strip()
                logging.info(f"Header: {header}")
                start_time = int(header.split("Start: ")[1].split(",")[0])
                end_time = int(header.split("End: ")[1].split(",")[0])
                start_time_str = header.split("Start_Time: ")[1].split(",")[0].strip()
                camera_name = header.split("Camera_Name: ")[1].split(",")[0].strip()
                video_path = header.split("Video_File: ")[1].split(",")[0].strip()
                start_time_dt = datetime.strptime(start_time_str, "%Y-%m-%d %H:%M:%S")
                logging.info(f"Parsed header - Start: {start_time}, End: {end_time}, Start_Time: {start_time_str}, Camera_Name: {camera_name}, Video_File: {video_path}")

                # Ki·ªÉm tra n·∫øu file log r·ªóng
                first_data_line = f.readline().strip()
                if not first_data_line:
                    logging.info(f"Log file {log_file_path} is empty, skipping")
                    cursor.execute("UPDATE processed_logs SET is_processed = 1, processed_at = ? WHERE log_file = ?", (datetime.now(), log_file_path))
                    conn.commit()
                    conn.close()
                    return

                # N·∫øu c√≥ d·ªØ li·ªáu, quay l·∫°i v√† x·ª≠ l√Ω
                f.seek(0)
                next(f)
                frame_sampler_data = []
                for line in f:
                    parts = line.strip().split(",")
                    try:
                        second, state = parts[0], parts[1]
                        codes = [parts[2]] if len(parts) > 2 and parts[2] else []
                        frame_sampler_data.append({"second": float(second), "state": state, "tracking_codes": codes})
                    except Exception as e:
                        logging.info(f"Error parsing line '{line.strip()}': {str(e)}")

            # L·∫•y min_packing_time t·ª´ Processing_config
            cursor.execute("SELECT min_packing_time FROM Processing_config LIMIT 1")
            min_packing_time_row = cursor.fetchone()
            min_packing_time = min_packing_time_row[0] if min_packing_time_row else 5
            logging.info(f"Min packing time: {min_packing_time}")

            # L·∫•y pending_event m·ªõi nh·∫•t theo ts
            cursor.execute("SELECT event_id, ts, tracking_codes, video_file FROM events WHERE te IS NULL AND camera_name = ? ORDER BY event_id DESC LIMIT 1", (camera_name,))
            pending_event = cursor.fetchone()
            logging.info(f"Pending event: {pending_event}")
            ts = pending_event[1] if pending_event else None
            pending_tracking_codes = eval(pending_event[2]) if pending_event and pending_event[2] else []
            pending_video_file = pending_event[3] if pending_event else None
            event_id = pending_event[0] if pending_event else None
            segments = []
            prev_state = None
            has_pending = ts is not None and ts <= start_time

            for data in frame_sampler_data:
                current_state = data["state"]
                current_second = data["second"]
                current_tracking_codes = data["tracking_codes"]

                if has_pending and ts is not None:
                    if current_state == "On":
                        te = current_second
                        total_duration = calculate_duration(ts, te)
                        
                        # T√°ch tracking_codes th√†nh c√°c s·ª± ki·ªán li√™n ti·∫øp
                        all_tracking_codes = list(set(pending_tracking_codes + current_tracking_codes))
                        num_codes = len(all_tracking_codes) if all_tracking_codes else 1
                        duration_per_event = max(round(total_duration / num_codes), min_packing_time)  # L√†m tr√≤n v√† ƒë·∫£m b·∫£o >= min_packing_time
                        total_duration = duration_per_event * num_codes  # C·∫≠p nh·∫≠t total_duration
                        te = ts + total_duration if ts is not None else te
                        logging.info(f"ƒêi·ªÅu ch·ªânh pending event: Ts={ts}, Te={te}, Duration m·ªói s·ª± ki·ªán th√†nh {duration_per_event}")
                        
                        if pending_video_file == video_path:
                            current_ts = ts
                            for i, code in enumerate(all_tracking_codes):
                                current_te = current_ts + duration_per_event if current_ts is not None else te
                                if i == 0:
                                    # C·∫≠p nh·∫≠t pending event cho m√£ ƒë·∫ßu ti√™n
                                    segments.append((current_ts, current_te, duration_per_event, [code], video_path, event_id))
                                    logging.info(f"C·∫≠p nh·∫≠t pending event li√™n ti·∫øp {i+1}/{num_codes}: Ts={current_ts}, Te={current_te}, Duration={duration_per_event}, Tracking_code={code}")
                                else:
                                    # Th√™m s·ª± ki·ªán m·ªõi cho c√°c m√£ ti·∫øp theo
                                    segments.append((current_ts, current_te, duration_per_event, [code], video_path, None))
                                    logging.info(f"Th√™m s·ª± ki·ªán li√™n ti·∫øp m·ªõi {i+1}/{num_codes}: Ts={current_ts}, Te={current_te}, Duration={duration_per_event}, Tracking_code={code}")
                                current_ts = current_te
                        else:
                            current_ts = None
                            for i, code in enumerate(all_tracking_codes):
                                current_te = te - (num_codes - i - 1) * duration_per_event
                                segments.append((current_ts, current_te, duration_per_event, [code], video_path, None))
                                logging.info(f"Th√™m pending event li√™n ti·∫øp {i+1}/{num_codes}: Ts={current_ts}, Te={current_te}, Duration={duration_per_event}, Tracking_code={code}")
                                current_ts = current_te
                        
                        ts = None
                        has_pending = False
                    elif current_state == "Off":
                        pending_tracking_codes.extend([code for code in current_tracking_codes if code and code not in pending_tracking_codes])
                        # Ki·ªÉm tra v√† x√≥a s·ª± ki·ªán d·ªü dang n·∫øu kh√¥ng c√≥ tracking_codes
                        if not pending_tracking_codes and not current_tracking_codes:
                            cursor.execute("DELETE FROM events WHERE te IS NULL AND event_id = (SELECT MAX(event_id) FROM events WHERE te IS NULL AND camera_name = ?)", (camera_name,))
                            logging.info(f"X√≥a s·ª± ki·ªán d·ªü dang cu·ªëi c√πng c·ªßa camera {camera_name} do kh√¥ng c√≥ tracking_codes")              
                elif not has_pending:
                    if prev_state == "On" and current_state == "Off":
                        ts = current_second
                        pending_tracking_codes = current_tracking_codes[:]
                        logging.info(f"Ph√°t hi·ªán s·ª± ki·ªán Ts t·∫°i gi√¢y {current_second}")

                    elif prev_state == "Off" and current_state == "On" and ts is not None:
                        te = current_second
                        total_duration = calculate_duration(ts, te)
                        
                        # T√°ch tracking_codes th√†nh c√°c s·ª± ki·ªán li√™n ti·∫øp
                        all_tracking_codes = list(set(pending_tracking_codes + current_tracking_codes))  # Lo·∫°i b·ªè tr√πng l·∫∑p
                        num_codes = len(all_tracking_codes) if all_tracking_codes else 1
                        duration_per_event = max(round(total_duration / num_codes), min_packing_time)  # L√†m tr√≤n v√† ƒë·∫£m b·∫£o >= min_packing_time
                        total_duration = duration_per_event * num_codes  # C·∫≠p nh·∫≠t total_duration
                        te = ts + total_duration if ts is not None else te
                        logging.info(f"ƒêi·ªÅu ch·ªânh s·ª± ki·ªán: Ts={ts}, Te={te}, Duration m·ªói s·ª± ki·ªán th√†nh {duration_per_event}")
                        
                        if all_tracking_codes:
                            current_ts = ts
                            for i, code in enumerate(all_tracking_codes):
                                current_te = current_ts + duration_per_event if current_ts is not None else te
                                segments.append((current_ts, current_te, duration_per_event, [code], video_path, None))
                                logging.info(f"Th√™m s·ª± ki·ªán li√™n ti·∫øp {i+1}/{num_codes}: Ts={current_ts}, Te={current_te}, Duration={duration_per_event}, Tracking_code={code}")
                                current_ts = current_te
                        else:
                            segments.append((ts, te, duration_per_event, [], video_path, None))
                            logging.info(f"Th√™m s·ª± ki·ªán kh√¥ng c√≥ tracking_code: Ts={ts}, Te={te}, Duration={duration_per_event}")
                        
                        ts = None
                        pending_tracking_codes = []

                    elif ts is not None and current_state == "Off":
                        pending_tracking_codes.extend([code for code in current_tracking_codes if code and code not in pending_tracking_codes])

                prev_state = current_state

            if ts is not None:
                segments.append((ts, None, None, pending_tracking_codes, video_path, None))
                logging.info(f"Gi√¢y {frame_sampler_data[-1]['second']}: Ts={ts}, Te=Not finished")

            logging.info(f"All segments detected: {segments}")

            for segment in segments:
                ts, te, duration, tracking_codes, segment_video_path, segment_event_id = segment
                if te is not None and not tracking_codes:
                    logging.info(f"B·ªè qua s·ª± ki·ªán ho√†n ch·ªânh do tracking_codes r·ªóng: ts={ts}, te={te}")
                    continue
                packing_time_start = int((start_time_dt.timestamp() + ts) * 1000) if ts is not None else None
                packing_time_end = int((start_time_dt.timestamp() + te) * 1000) if te is not None else None
                if segment_event_id is not None:
                    cursor.execute("UPDATE events SET te=?, duration=?, tracking_codes=?, packing_time_end=? WHERE event_id=?",
                                   (te, duration, str(tracking_codes), packing_time_end, segment_event_id))
                    logging.info(f"Updated event_id {segment_event_id}: ts={ts}, te={te}, duration={duration}")
                else:
                    cursor.execute('''INSERT INTO events (ts, te, duration, tracking_codes, video_file, buffer, camera_name, packing_time_start, packing_time_end)
                                      VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                                   (ts, te, duration, str(tracking_codes), segment_video_path, 0, camera_name, packing_time_start, packing_time_end))
                    logging.info(f"Inserted new event: ts={ts}, te={te}, duration={duration}")

            cursor.execute("UPDATE processed_logs SET is_processed = 1, processed_at = ? WHERE log_file = ?", (datetime.now(), log_file_path))
            conn.commit()
            logging.info("Database changes committed")

    except Exception as e:
        logging.error(f"Error in process_single_log: {str(e)}")
        raise
    finally:
        conn.close()

@event_detector_bp.route('/process-events', methods=['GET'])
def process_events():
    try:
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                SELECT DISTINCT log_file_path, (
                    SELECT CAST(SUBSTR(header, INSTR(header, 'Start: ') + 7, INSTR(SUBSTR(header, INSTR(header, 'Start: ') + 7), ',') - 1) AS INTEGER)
                    FROM (
                        SELECT SUBSTR(CAST(READFILE(log_file_path) AS TEXT), 1, INSTR(CAST(READFILE(log_file_path) AS TEXT), '\n') - 1) AS header
                    )
                ) AS start_time
                FROM file_list 
                WHERE is_processed = 1 AND log_file_path IS NOT NULL 
                AND log_file_path IN (SELECT log_file FROM processed_logs WHERE is_processed = 0)
                ORDER BY start_time
            """)
            log_files = [row[0] for row in cursor.fetchall()]
            logging.info(f"Log files to process: {log_files}")
            conn.close()

        for log_file in log_files:
            if not os.path.isfile(log_file):
                logging.warning(f"Log file not found, skipping: {log_file}")
                continue
            if os.path.exists(log_file):
                logging.info(f"Starting to process file: {log_file}")
                process_single_log(log_file)
                logging.info(f"Finished processing file: {log_file}")
            else:
                logging.info(f"File not found: {log_file}")

        return jsonify({"message": "Event detection completed successfully"}), 200
    except Exception as e:
        logging.error(f"Error in process_events: {str(e)}")
        return jsonify({"error": str(e)}), 500

```
## üìÑ File: `__init__.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/__init__.py`

```python

```
## üìÑ File: `hand_detection.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/hand_detection.py`

```python
import cv2
import mediapipe as mp
import time
import logging
import json
import os
import glob
from datetime import datetime

# ƒê·ªãnh nghƒ©a BASE_DIR
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# C·∫•u h√¨nh logging (s·ª≠ d·ª•ng c√°ch c≈© - kh√¥ng import modules.config.logging_config)
log_dir = os.path.join(BASE_DIR, "resources", "output_clips", "LOG")
os.makedirs(log_dir, exist_ok=True)
log_file_path = os.path.join(log_dir, f"hand_detection_{datetime.now().strftime('%Y-%m-%d')}.log")
logging.basicConfig(
    filename=log_file_path,
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

# ƒê·∫∑t b∆∞·ªõc nh·∫£y frame
FRAME_STEP = 5

# ƒê∆∞·ªùng d·∫´n l∆∞u ·∫£nh
CAMERA_ROI_DIR = os.path.join(BASE_DIR, "resources", "output_clips", "CameraROI")

def ensure_directory_exists(directory):
    """ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i, n·∫øu kh√¥ng th√¨ t·∫°o m·ªõi."""
    try:
        if not os.path.exists(directory):
            os.makedirs(directory)
            logging.debug(f"ƒê√£ t·∫°o th∆∞ m·ª•c: {directory}")
        # Ki·ªÉm tra quy·ªÅn truy c·∫≠p
        if not os.access(directory, os.W_OK):
            logging.error(f"Kh√¥ng c√≥ quy·ªÅn ghi v√†o th∆∞ m·ª•c {directory}")
            raise PermissionError(f"Kh√¥ng c√≥ quy·ªÅn ghi v√†o th∆∞ m·ª•c {directory}")
    except Exception as e:
        logging.error(f"L·ªói khi t·∫°o th∆∞ m·ª•c {directory}: {str(e)}")
        raise

def select_roi(video_path, camera_id, step="packing"):
    """
    M·ªü video v√† cho ph√©p ng∆∞·ªùi d√πng v·∫Ω ROI b·∫±ng OpenCV, l∆∞u k·∫øt qu·∫£ v√†o CameraROI, sau ƒë√≥ ph√°t hi·ªán tay.
    Args:
        video_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file video.
        camera_id (str): ID c·ªßa camera.
        step (str): Giai ƒëo·∫°n hi·ªán t·∫°i (packing, trigger).
    Returns:
        dict: {'success': bool, 'roi': {'x': int, 'y': int, 'w': int, 'h': int}, 'roi_frame': str, 'hand_detected': bool} ho·∫∑c {'success': false, 'error': str}
    """
    try:
        logging.debug(f"B·∫Øt ƒë·∫ßu select_roi v·ªõi video_path: {video_path}, camera_id: {camera_id}, step: {step}")
        
        # ƒê·∫£m b·∫£o th∆∞ m·ª•c CameraROI t·ªìn t·∫°i
        ensure_directory_exists(CAMERA_ROI_DIR)

        # M·ªü video
        logging.debug("ƒêang m·ªü video...")
        cap = cv2.VideoCapture(video_path)
        try:
            if not cap.isOpened():
                logging.error("Kh√¥ng th·ªÉ m·ªü video.")
                return {"success": False, "error": "Kh√¥ng th·ªÉ m·ªü video."}
            
            # ƒê·ªçc frame ƒë·∫ßu ti√™n
            logging.debug("ƒêang ƒë·ªçc frame ƒë·∫ßu ti√™n...")
            ret, frame = cap.read()
            if not ret:
                logging.error("Kh√¥ng th·ªÉ ƒë·ªçc frame t·ª´ video.")
                return {"success": False, "error": "Kh√¥ng th·ªÉ ƒë·ªçc frame t·ª´ video."}
            
            # L∆∞u frame g·ªëc n·∫øu ·ªü b∆∞·ªõc packing
            if step == "packing":
                original_frame_path = os.path.join(CAMERA_ROI_DIR, f"camera_{camera_id}_original.jpg")
                ret = cv2.imwrite(original_frame_path, frame)
                if not ret:
                    logging.error(f"Kh√¥ng th·ªÉ l∆∞u ·∫£nh g·ªëc t·∫°i: {original_frame_path}")
                    return {"success": False, "error": f"Kh√¥ng th·ªÉ l∆∞u ·∫£nh g·ªëc t·∫°i {original_frame_path}"}
                logging.debug(f"ƒê√£ l∆∞u frame g·ªëc v√†o: {original_frame_path}")
            
            while True:
                # Hi·ªÉn th·ªã giao di·ªán ch·ªçn ROI
                logging.debug("G·ªçi cv2.selectROI...")
                current_frame = frame.copy()
                roi = cv2.selectROI(f"Click va keo chuot de chon -Vung {step.capitalize()}-", current_frame, showCrosshair=True, fromCenter=False)
                logging.debug(f"ROI tr·∫£ v·ªÅ: {roi}")
                cv2.destroyAllWindows()
                
                # Ki·ªÉm tra n·∫øu ROI h·ª£p l·ªá
                x, y, w, h = map(int, roi)
                if w == 0 or h == 0:
                    logging.debug("ROI kh√¥ng h·ª£p l·ªá, hi·ªÉn th·ªã l·∫°i frame g·ªëc ƒë·ªÉ v·∫Ω l·∫°i.")
                    continue  # Hi·ªÉn th·ªã l·∫°i frame g·ªëc, kh√¥ng l∆∞u file
                
                # V·∫Ω ROI l√™n frame
                color = (0, 255, 0) if step == "packing" else (0, 255, 255)
                cv2.rectangle(current_frame, (x, y), (x + w, y + h), color, 2)
                # Th√™m ti√™u ƒë·ªÅ "Packing" n·∫øu ·ªü b∆∞·ªõc packing
                if step == "packing":
                    cv2.putText(current_frame, "Packing", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                # Hi·ªÉn th·ªã frame v·ªõi ROI v√† ti√™u ƒë·ªÅ
                cv2.namedWindow("**** Da ve vung Packing ****", cv2.WINDOW_NORMAL)
                cv2.imshow("**** Da ve vung Packing ****", current_frame)
                cv2.waitKey(500)
                cv2.destroyAllWindows()
                
                # L∆∞u frame v·ªõi ROI v√†o CameraROI
                if step == "packing":
                    roi_frame_path = os.path.join(CAMERA_ROI_DIR, f"camera_{camera_id}_roi_packing.jpg")
                else:  # step == "trigger"
                    roi_frame_path = os.path.join(CAMERA_ROI_DIR, f"camera_{camera_id}_roi_trigger.jpg")
                
                ret = cv2.imwrite(roi_frame_path, current_frame)
                if not ret:
                    logging.error(f"Kh√¥ng th·ªÉ l∆∞u ·∫£nh t·∫°i: {roi_frame_path}")
                    return {"success": False, "error": f"Kh√¥ng th·ªÉ l∆∞u ·∫£nh t·∫°i {roi_frame_path}"}
                logging.debug(f"ƒê√£ l∆∞u frame v·ªõi ROI v√†o: {roi_frame_path}")
                
                # Ki·ªÉm tra file ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng
                if not os.path.exists(roi_frame_path):
                    logging.error(f"File kh√¥ng t·ªìn t·∫°i sau khi l∆∞u: {roi_frame_path}")
                    return {"success": False, "error": f"File kh√¥ng t·ªìn t·∫°i sau khi l∆∞u: {roi_frame_path}"}
                
                # N·∫øu l√† b∆∞·ªõc packing, g·ªçi detect_hands ƒë·ªÉ ki·ªÉm tra tay
                hand_detected = False
                if step == "packing":
                    detect_result = detect_hands(video_path, {"x": x, "y": y, "w": w, "h": h})
                    if not detect_result["success"]:
                        logging.error(f"L·ªói khi ph√°t hi·ªán tay: {detect_result['error']}")
                        return {"success": False, "error": detect_result["error"]}
                    hand_detected = detect_result["hand_detected"]
                
                # L∆∞u t·ªça ƒë·ªô ROI v√† tr·∫°ng th√°i hand_detected v√†o /tmp/roi.json
                result = {
                    "success": True,
                    "roi": {"x": x, "y": y, "w": w, "h": h},
                    "roi_frame": os.path.relpath(roi_frame_path, BASE_DIR),
                    "hand_detected": hand_detected
                }
                logging.debug(f"L∆∞u ROI v√†o /tmp/roi.json: {result}")
                with open("/tmp/roi.json", "w") as f:
                    json.dump(result, f)
                
                logging.debug(f"ROI h·ª£p l·ªá: x={x}, y={y}, w={w}, h={h}, hand_detected: {hand_detected}")
                return result
            
        finally:
            cap.release()
            logging.debug("ƒê√£ gi·∫£i ph√≥ng t√†i nguy√™n video (cap.release).")
        
    except Exception as e:
        logging.error(f"L·ªói trong select_roi: {str(e)}")
        cv2.destroyAllWindows()
        return {"success": False, "error": f"L·ªói h·ªá th·ªëng: {str(e)}"}

def detect_hands(video_path, roi):
    """
    Hi·ªÉn th·ªã video v·ªõi ph√°t hi·ªán tay trong v√πng ROI, tr·∫£ v·ªÅ tr·∫°ng th√°i ph√°t hi·ªán tay.
    Args:
        video_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file video.
        roi (dict): T·ªça ƒë·ªô ROI {'x': int, 'y': int, 'w': int, 'h': int}.
    Returns:
        dict: {'success': bool, 'hand_detected': bool, 'error': str n·∫øu c√≥ l·ªói}
    """
    try:
        x, y, w, h = roi["x"], roi["y"], roi["w"], roi["h"]
        if w <= 0 or h <= 0:
            logging.error("ROI kh√¥ng h·ª£p l·ªá (chi·ªÅu r·ªông ho·∫∑c chi·ªÅu cao b·∫±ng 0).")
            return {"success": False, "hand_detected": False, "error": "ROI kh√¥ng h·ª£p l·ªá."}

        # M·ªü video
        logging.debug("ƒêang m·ªü video ƒë·ªÉ ph√°t hi·ªán tay...")
        cap = cv2.VideoCapture(video_path)
        try:
            if not cap.isOpened():
                logging.error("Kh√¥ng th·ªÉ m·ªü video.")
                return {"success": False, "hand_detected": False, "error": "Kh√¥ng th·ªÉ m·ªü video."}

            hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)
            frame_count = 0
            start_time = time.time()
            hand_detected = False

            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                # C·∫Øt video theo ROI
                roi_frame = frame[y:y+h, x:x+w]

                # Ch·ªâ x·ª≠ l√Ω m·ªói FRAME_STEP frame
                if frame_count % FRAME_STEP == 0:
                    # Chuy·ªÉn ƒë·ªïi BGR sang RGB
                    rgb_frame = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2RGB)

                    # Ph√°t hi·ªán b√†n tay
                    results = hands.process(rgb_frame)

                    # Ki·ªÉm tra v√† x√°c nh·∫≠n ph√°t hi·ªán tay
                    if results.multi_hand_landmarks:
                        hand_detected = True
                        for hand_landmarks in results.multi_hand_landmarks:
                            # V·∫Ω keypoints ngay khi ph√°t hi·ªán tay
                            mp_drawing.draw_landmarks(roi_frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

                # Hi·ªÉn th·ªã video
                elapsed_time = time.time() - start_time
                cv2.putText(roi_frame, f"Time: {elapsed_time:.2f}s", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                cv2.imshow("ROI Hand Detection", roi_frame)

                if cv2.waitKey(1) == ord("q"):
                    break

                frame_count += 1

            logging.debug(f"Ph√°t hi·ªán tay: {hand_detected}")
            return {"success": True, "hand_detected": hand_detected}
        
        finally:
            cap.release()
            cv2.destroyWindow("ROI Hand Detection")  # Ch·ªâ ƒë√≥ng c·ª≠a s·ªï c·ªßa detect_hands
            logging.debug("ƒê√£ gi·∫£i ph√≥ng t√†i nguy√™n video (cap.release) trong detect_hands.")
    
    except Exception as e:
        logging.error(f"L·ªói trong detect_hands: {str(e)}")
        return {"success": False, "hand_detected": False, "error": f"L·ªói h·ªá th·ªëng: {str(e)}"}

def finalize_roi(video_path, camera_id, rois):
    """
    V·∫Ω t·∫•t c·∫£ c√°c v√πng ROI (packing, MVD, trigger) l√™n frame v√† l∆∞u v√†o th∆∞ m·ª•c CameraROI.
    Args:
        video_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file video.
        camera_id (str): ID c·ªßa camera.
        rois (list): Danh s√°ch c√°c v√πng ROI [{'type': str, 'x': int, 'y': int, 'w': int, 'h': int}, ...].
    Returns:
        dict: {'success': bool, 'final_roi_frame': str, 'error': str n·∫øu c√≥ l·ªói}
    """
    try:
        # ƒê·∫£m b·∫£o th∆∞ m·ª•c CameraROI t·ªìn t·∫°i
        ensure_directory_exists(CAMERA_ROI_DIR)

        # M·ªü video v√† l·∫•y frame ƒë·∫ßu ti√™n
        logging.debug("ƒêang m·ªü video ƒë·ªÉ t·∫°o ·∫£nh t·ªïng h·ª£p...")
        cap = cv2.VideoCapture(video_path)
        try:
            if not cap.isOpened():
                logging.error("Kh√¥ng th·ªÉ m·ªü video.")
                return {"success": False, "error": "Kh√¥ng th·ªÉ m·ªü video."}

            ret, frame = cap.read()
            if not ret:
                logging.error("Kh√¥ng th·ªÉ ƒë·ªçc frame t·ª´ video.")
                return {"success": False, "error": "Kh√¥ng th·ªÉ ƒë·ªçc frame t·ª´ video."}

            # V·∫Ω c√°c v√πng ROI v·ªõi m√†u s·∫Øc kh√°c nhau
            for roi in rois:
                x, y, w, h = roi["x"], roi["y"], roi["w"], roi["h"]
                roi_type = roi["type"]

                # ƒê·ªãnh nghƒ©a m√†u s·∫Øc cho t·ª´ng lo·∫°i ROI
                if roi_type == "packing":
                    color = (0, 255, 0)  # Xanh l√°
                elif roi_type == "mvd":
                    color = (0, 0, 255)  # ƒê·ªè
                elif roi_type == "trigger":
                    color = (0, 255, 255)  # V√†ng
                else:
                    color = (255, 255, 255)  # Tr·∫Øng (m·∫∑c ƒë·ªãnh)

                # V·∫Ω ROI l√™n frame
                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
                # Th√™m nh√£n cho v√πng ROI
                cv2.putText(frame, roi_type.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)

            # T·∫°o t√™n file v·ªõi timestamp v√† camera_id
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            final_roi_frame_path = os.path.join(CAMERA_ROI_DIR, f"camera_{camera_id}_roi_final_{timestamp}.jpg")

            # L∆∞u ·∫£nh t·ªïng h·ª£p
            ret = cv2.imwrite(final_roi_frame_path, frame)
            if not ret:
                logging.error(f"Kh√¥ng th·ªÉ l∆∞u ·∫£nh t·ªïng h·ª£p t·∫°i: {final_roi_frame_path}")
                return {"success": False, "error": f"Kh√¥ng th·ªÉ l∆∞u ·∫£nh t·ªïng h·ª£p t·∫°i {final_roi_frame_path}"}
            logging.debug(f"ƒê√£ l∆∞u ·∫£nh t·ªïng h·ª£p v·ªõi t·∫•t c·∫£ ROI v√†o: {final_roi_frame_path}")

            return {"success": True, "final_roi_frame": os.path.relpath(final_roi_frame_path, BASE_DIR)}
        
        finally:
            cap.release()
            logging.debug("ƒê√£ gi·∫£i ph√≥ng t√†i nguy√™n video (cap.release) trong finalize_roi.")
    
    except Exception as e:
        logging.error(f"L·ªói trong finalize_roi: {str(e)}")
        return {"success": False, "error": f"L·ªói h·ªá th·ªëng: {str(e)}"}

if __name__ == "__main__":
    import sys
    # ‚úÖ SUPPORT: Accept both 2 and 3 arguments, with optional step parameter
    if len(sys.argv) < 3 or len(sys.argv) > 4:
        print("Usage: python3 hand_detection.py <video_path> <camera_id> [step]")
        sys.exit(1)
    
    video_path = sys.argv[1]
    camera_id = sys.argv[2]
    step = sys.argv[3] if len(sys.argv) == 4 else "packing"  # Default to "packing"
    
    try:
        roi_result = select_roi(video_path, camera_id, step)
        if not roi_result["success"]:
            print(roi_result["error"])
            sys.exit(1)
    except Exception as e:
        logging.error(f"L·ªói khi ch·∫°y script: {str(e)}")
        print(f"L·ªói khi ch·∫°y script: {str(e)}")
        sys.exit(1)
```
## üìÑ File: `frame_sampler_no_trigger.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/frame_sampler_no_trigger.py`

```python
import cv2
import os
import logging
import sqlite3
import threading
import subprocess
import json
import queue
import numpy as np
import mediapipe as mp
from datetime import datetime, timezone, timedelta
from modules.db_utils import get_db_connection
from modules.scheduler.db_sync import frame_sampler_event, db_rwlock
import math
from modules.config.logging_config import get_logger


BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
MODEL_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "models", "wechat_qr")
DETECT_PROTO = os.path.join(MODEL_DIR, "detect.prototxt")
DETECT_MODEL = os.path.join(MODEL_DIR, "detect.caffemodel")
SR_PROTO = os.path.join(MODEL_DIR, "sr.prototxt")
SR_MODEL = os.path.join(MODEL_DIR, "sr.caffemodel")

class FrameSamplerNoTrigger:
    def __init__(self):
        self.setup_logging()
        self.load_config()
        self.video_lock = threading.Lock()
        self.setup_wechat_qr()
        # Initialize MediaPipe Hands
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)
        # Initialize log queue and writer thread
        self.log_queue = queue.Queue()
        self.log_thread = threading.Thread(target=self._log_writer)
        self.log_thread.daemon = True
        self.log_thread.start()

    def setup_logging(self):
        self.logger = get_logger(__name__, {"video_id": os.path.basename(self.video_file)})
        self.logger.info("Logging initialized")

    def setup_wechat_qr(self):
        for model_file in [DETECT_PROTO, DETECT_MODEL, SR_PROTO, SR_MODEL]:
            if not os.path.exists(model_file):
                logging.error(f"Model file not found: {model_file}")
                raise FileNotFoundError(f"Model file not found: {model_file}")
        try:
            self.qr_detector = cv2.wechat_qrcode_WeChatQRCode(DETECT_PROTO, DETECT_MODEL, SR_PROTO, SR_MODEL)
            logging.info("WeChat QRCode detector initialized")
        except Exception as e:
            logging.error(f"Failed to initialize WeChat QRCode: {str(e)}")
            raise

    def load_config(self):
        logging.info("Loading configuration from database")
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT input_path FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            self.video_root = result[0] if result else os.path.join(BASE_DIR, "Inputvideo")
            cursor.execute("SELECT output_path FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            self.output_path = result[0] if result else os.path.join(BASE_DIR, "output_clips")
            self.log_dir = os.path.join(self.output_path, "LOG", "Frame")
            os.makedirs(self.log_dir, exist_ok=True)
            cursor.execute("SELECT timezone FROM general_info WHERE id = 1")
            result = cursor.fetchone()
            tz_hours = int(result[0].split("+")[1]) if result and "+" in result[0] else 7
            self.video_timezone = timezone(timedelta(hours=tz_hours))
            cursor.execute("SELECT frame_rate, frame_interval, min_packing_time, motion_threshold, stable_duration_sec FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            self.fps, self.frame_interval, self.min_packing_time, self.motion_threshold, self.stable_duration_sec = result if result else (30, 5, 3, 0.1, 1.0)
            conn.close()
            logging.info(f"Config loaded: video_root={self.video_root}, output_path={self.output_path}, timezone={self.video_timezone}, fps={self.fps}, frame_interval={self.frame_interval}, min_packing_time={self.min_packing_time}, motion_threshold={self.motion_threshold}, stable_duration_sec={self.stable_duration_sec}")

    def get_packing_area(self, camera_name):
        logging.info(f"Querying qr_mvd_area for camera {camera_name}")
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT qr_mvd_area FROM packing_profiles WHERE profile_name = ?", (camera_name,))
            result = cursor.fetchone()
            conn.close()
        if result and result[0]:
            try:
                qr_mvd_area = result[0]
                if qr_mvd_area.startswith('(') and qr_mvd_area.endswith(')'):
                    x, y, w, h = map(int, qr_mvd_area.strip('()').split(','))
                else:
                    parsed = json.loads(qr_mvd_area)
                    if isinstance(parsed, list) and len(parsed) == 4:
                        x, y, w, h = parsed
                    else:
                        x, y, w, h = parsed['x'], parsed['y'], parsed['w'], parsed['h']
                roi = (x, y, w, h)
                logging.info(f"Using qr_mvd_area: {roi}")
            except (ValueError, json.JSONDecodeError, KeyError, TypeError) as e:
                logging.error(f"Error parsing qr_mvd_area for camera {camera_name}: {str(e)}")
                roi = None
        else:
            logging.warning(f"No qr_mvd_area found for camera {camera_name}")
            roi = None
        return roi

    def get_video_duration(self, video_file):
        try:
            cmd = ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", video_file]
            result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            return float(result.stdout.strip())
        except Exception:
            logging.error(f"Failed to get duration of video {video_file}")
            return None

    def load_video_files(self):
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT file_path FROM file_list WHERE is_processed = 0 AND status != 'xong' ORDER BY priority DESC, created_at ASC")
            video_files = [row[0] for row in cursor.fetchall()]
            conn.close()
        if not video_files:
            logging.info("No video files found with is_processed = 0 and status != 'xong'.")
        return video_files

    def process_frame(self, frame, frame_count):
        try:
            if len(frame.shape) == 2:
                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
            texts, _ = self.qr_detector.detectAndDecode(frame)
            for text in texts:
                if text and text != "TimeGo":
                    logging.info(f"Second {round((frame_count - 1) / self.fps)}: QR texts={texts}, mvd={text}")
                    return text
            return ""
        except Exception as e:
            logging.error(f"Error processing frame {frame_count}: {str(e)}")
            return ""

    def detect_hand(self, frame):
        try:
            # Convert BGR to RGB
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            # Process frame with MediaPipe Hands
            results = self.hands.process(rgb_frame)
            # Return True if hand landmarks are detected
            return bool(results.multi_hand_landmarks)
        except Exception as e:
            logging.error(f"Error in hand detection: {str(e)}")
            return False

    def compute_motion_level(self, prev_frame, curr_frame):
        try:
            if len(prev_frame.shape) == 3:
                prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
            if len(curr_frame.shape) == 3:
                curr_frame = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)
            diff = cv2.absdiff(prev_frame, curr_frame)
            motion_level = np.mean(diff) / 255.0
            return motion_level
        except Exception as e:
            logging.error(f"Error computing motion level: {str(e)}")
            return 1.0

    def _get_video_start_time(self, video_file):
        try:
            result = subprocess.check_output(['ffprobe', '-v', 'quiet', '-show_entries', 'format_tags=creation_time', '-of', 'default=noprint_wrappers=1:nokey=1', video_file])
            return datetime.strptime(result.decode().strip(), '%Y-%m-%dT%H:%M:%S.%fZ').replace(tzinfo=timezone.utc).astimezone(self.video_timezone)
        except (subprocess.CalledProcessError, ValueError):
            try:
                result = subprocess.check_output(['exiftool', '-CreateDate', '-d', '%Y-%m-%d %H:%M:%S', video_file])
                return datetime.strptime(result.decode().split('CreateDate')[1].strip().split('\n')[0].strip(), '%Y-%m-%d %H:%M:%S').replace(tzinfo=self.video_timezone)
            except (subprocess.CalledProcessError, IndexError):
                try:
                    result = subprocess.check_output(['exiftool', '-FileCreateDate', '-d', '%Y-%m-%d %H:%M:%S', video_file])
                    return datetime.strptime(result.decode().split('FileCreateDate')[1].strip().split('\n')[0].strip(), '%Y-%m-%d %H:%M:%S').replace(tzinfo=self.video_timezone)
                except (subprocess.CalledProcessError, IndexError):
                    logging.warning("No metadata found, using file creation time.")
                    return datetime.fromtimestamp(os.path.getctime(video_file), tz=self.video_timezone)

    def _log_writer(self):
        while True:
            log_file, entry, timestamp = self.log_queue.get()
            with threading.Lock():
                mode = 'w' if not os.path.exists(log_file) else 'a'
                with open(log_file, mode) as f:
                    if mode == 'w':
                        f.write(f"# Start: {timestamp['start']}, End: {timestamp['end']}, Start_Time: {timestamp['start_time']}, Camera_Name: {timestamp['camera']}, Video_File: {timestamp['video']}\n")
                    f.write(f"{entry}\n")
                    f.flush()
            self.log_queue.task_done()

    def _update_log_file(self, log_file, start_second, end_second, start_time, camera_name, video_file):
        timestamp = {
            'start': start_second,
            'end': end_second,
            'start_time': start_time.strftime('%Y-%m-%d %H:%M:%S'),
            'camera': camera_name,
            'video': video_file
        }
        with db_rwlock.gen_wlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT 1 FROM processed_logs WHERE log_file = ?", (log_file,))
            if not cursor.fetchone():
                cursor.execute("INSERT INTO processed_logs (log_file, is_processed) VALUES (?, 0)", (log_file,))
            conn.commit()
            conn.close()
        return lambda entry, ts: self.log_queue.put((log_file, f"{ts},{entry}", timestamp))

    def run(self):
        while True:
            frame_sampler_event.wait()
            video_files = self.load_video_files()
            if not video_files:
                logging.info("No videos to process")
                frame_sampler_event.clear()
                continue
            for video_file in video_files:
                log_file = self.process_video(video_file, self.video_lock, self.get_packing_area, self.process_frame, self.frame_interval)
                if log_file:
                    logging.info(f"Completed processing video {video_file}, log at {log_file}")
                else:
                    logging.error(f"Failed to process video {video_file}")
            frame_sampler_event.clear()

    def process_video(self, video_file, video_lock, get_packing_area_func, process_frame_func, frame_interval, start_time=0, end_time=None):
        with video_lock:
            logging.info(f"Processing video: {video_file} from {start_time}s to {end_time}s")
            if not os.path.exists(video_file):
                logging.error(f"File '{video_file}' does not exist")
                with db_rwlock.gen_wlock():
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("l·ªói", video_file))
                    conn.commit()
                    conn.close()
                return None
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("ƒëang frame sampler ...", video_file))
                cursor.execute("SELECT camera_name FROM file_list WHERE file_path = ?", (video_file,))
                result = cursor.fetchone()
                camera_name = result[0] if result and result[0] else "CamTest"
                conn.commit()
                conn.close()
            video = cv2.VideoCapture(video_file)
            if not video.isOpened():
                logging.error(f"Failed to open video '{video_file}'")
                with db_rwlock.gen_wlock():
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("l·ªói", video_file))
                    conn.commit()
                    conn.close()
                return None
            start_time_obj = self._get_video_start_time(video_file)
            roi = get_packing_area_func(camera_name)
            total_seconds = self.get_video_duration(video_file)
            if total_seconds is None:
                logging.error(f"Failed to get duration of video {video_file}")
                with db_rwlock.gen_wlock():
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("l·ªói", video_file))
                    conn.commit()
                    conn.close()
                return None
            logging.info(f"Video duration {video_file}: {total_seconds} seconds")
            video_name = os.path.splitext(os.path.basename(video_file))[0]
            segment_duration = 300
            # X√°c ƒë·ªãnh c√°c ƒëo·∫°n 300s ch·ª©a [start_time, end_time]
            end_time = total_seconds if end_time is None else min(end_time, total_seconds)
            start_segment = math.floor(start_time / segment_duration) * segment_duration
            end_segment = math.ceil(end_time / segment_duration) * segment_duration
            current_start_second = start_segment
            current_end_second = min(current_start_second + segment_duration, end_segment)
            camera_log_dir = os.path.join(self.log_dir, camera_name)
            os.makedirs(camera_log_dir, exist_ok=True)
            log_file = os.path.join(camera_log_dir, f"log_{video_name}_{current_start_second:04d}_{current_end_second:04d}.txt")
            log_file_handle = self._update_log_file(log_file, current_start_second, current_end_second, start_time_obj + timedelta(seconds=current_start_second), camera_name, video_file)
            # B·∫Øt ƒë·∫ßu t·ª´ khung h√¨nh t·∫°i start_time
            start_frame = int(start_time * self.fps)
            end_frame = int(end_time * self.fps)
            video.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            frame_count = start_frame
            prev_frame = None
            stable_segments = []
            qr_events = []
            stable_start = None
            last_te = -self.min_packing_time * self.fps
            is_stable = False
            while video.isOpened() and frame_count < end_frame:
                ret, frame = video.read()
                if not ret:
                    break
                if roi:
                    x, y, w, h = roi
                    frame_height, frame_width = frame.shape[:2]
                    if w > 0 and h > 0 and y + h <= frame_height and x + w <= frame_width:
                        frame = frame[y:y + h, x:x + w]
                    else:
                        logging.warning(f"Invalid ROI for frame {frame_count}: {roi}, frame_size: {frame_width}x{frame_height}")
                        frame = frame
                frame_count += 1
                if frame_count % frame_interval != 0:
                    continue
                if frame.size == 0 or frame.shape[0] == 0 or frame.shape[1] == 0:
                    logging.warning(f"Empty frame {frame_count}, skipping")
                    continue
                # QR detection
                mvd = process_frame_func(frame, frame_count)
                if mvd:
                    qr_events.append((frame_count, mvd))
                # Motion detection
                if prev_frame is not None:
                    motion_level = self.compute_motion_level(prev_frame, frame)
                    min_stable_frames = max(6, int(self.fps * self.stable_duration_sec / self.frame_interval))
                    if motion_level < self.motion_threshold:
                        if not is_stable:
                            stable_start = frame_count
                            is_stable = True
                    else:
                        if is_stable and (frame_count - stable_start) >= min_stable_frames * frame_interval:
                            start_second = round((stable_start - 1) / self.fps, 1)
                            end_second = round((frame_count - frame_interval - 1) / self.fps, 1)
                            if start_second >= start_time and end_second <= end_time:
                                stable_segments.append((stable_start, frame_count - frame_interval))
                                logging.info(f"Stable segment: start={start_second}s, end={end_second}s")
                        is_stable = False
                prev_frame = frame.copy()
                second_in_video = (frame_count - 1) / self.fps
                second = round(second_in_video)
                if second >= current_end_second and second < end_time:
                    current_start_second = current_end_second
                    current_end_second = min(current_start_second + segment_duration, end_segment)
                    camera_log_dir = os.path.join(self.log_dir, camera_name)
                    os.makedirs(camera_log_dir, exist_ok=True)
                    log_file = os.path.join(camera_log_dir, f"log_{video_name}_{current_start_second:04d}_{current_end_second:04d}.txt")
                    log_file_handle = self._update_log_file(log_file, current_start_second, current_end_second, start_time_obj + timedelta(seconds=current_start_second), camera_name, video_file)
            if is_stable and (frame_count - stable_start) >= min_stable_frames * frame_interval:
                start_second = round((stable_start - 1) / self.fps, 1)
                end_second = round((frame_count - 1) / self.fps, 1)
                if start_second >= start_time and end_second <= end_time:
                    stable_segments.append((stable_start, frame_count))
                    logging.info(f"Stable segment: start={start_second}s, end={end_second}s")
            # Group QR codes and select the last frame of each sequence
            grouped_qr = []
            current_mvd = None
            current_frames = []
            for frame, mvd in qr_events:
                if mvd == "TimeGo":
                    continue
                if mvd != current_mvd:
                    if current_mvd and current_frames:
                        grouped_qr.append((current_frames[-1], current_mvd))
                    current_mvd = mvd
                    current_frames = [frame]
                else:
                    current_frames.append(frame)
            if current_mvd and current_frames:
                grouped_qr.append((current_frames[-1], current_mvd))
            # Log last QR event
            if grouped_qr:
                last_qr_frame, last_qr_code = grouped_qr[-1]
                last_qr_second = round((last_qr_frame - 1) / self.fps, 1)
                if last_qr_second >= start_time and last_qr_second <= end_time:
                    logging.info(f"Last QR event: Te={last_qr_second}s, QR={last_qr_code}")
            # Find Ts/Te
            video.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            frame_count = start_frame
            last_te = -self.min_packing_time * self.fps
            prev_te_frame = None
            for idx, (te_frame, qr_code) in enumerate(grouped_qr):
                if te_frame <= last_te + self.min_packing_time * self.fps:
                    logging.info(f"Skipping event for QR {qr_code} at frame {te_frame}: too close to last_te {last_te}")
                    continue
                second_te = round((te_frame - 1) / self.fps)
                if second_te < start_time or second_te > end_time:
                    continue
                segment_index = math.floor(second_te / segment_duration)
                target_start_second = segment_index * segment_duration
                target_end_second = (segment_index + 1) * segment_duration
                if second_te >= current_end_second or target_start_second != current_start_second:
                    current_start_second = target_start_second
                    current_end_second = min(target_start_second + segment_duration, end_segment)
                    camera_log_dir = os.path.join(self.log_dir, camera_name)
                    os.makedirs(camera_log_dir, exist_ok=True)
                    log_file = os.path.join(camera_log_dir, f"log_{video_name}_{current_start_second:04d}_{current_end_second:04d}.txt")
                    log_file_handle = self._update_log_file(log_file, current_start_second, current_end_second, start_time_obj + timedelta(seconds=current_start_second), camera_name, video_file)
                ts_frame = None
                # Tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát: Te ƒë·∫ßu ti√™n
                if idx == 0:
                    has_stable_segment = any(start <= te_frame for start, end in stable_segments)
                    if not has_stable_segment:
                        log_file_handle(f"On,{qr_code}", second_te)
                        logging.info(f"Logged only Te for QR {qr_code} at second {second_te}: no stable segments for first Te")
                        last_te = te_frame
                        prev_te_frame = te_frame
                        continue
                # T√¨m v√πng ·ªïn ƒë·ªãnh sau Te ph√≠a tr∆∞·ªõc (ho·∫∑c ƒë·∫ßu video n·∫øu kh√¥ng c√≥ Te tr∆∞·ªõc)
                closest_stable = None
                search_start = max(prev_te_frame if prev_te_frame is not None else start_frame, start_frame)
                for start, end in stable_segments:
                    if start > search_start and end < te_frame:
                        if closest_stable is None or start < closest_stable[0]:
                            closest_stable = (start, end)
                if closest_stable:
                    # T√¨m tay sau v√πng ·ªïn ƒë·ªãnh
                    video.set(cv2.CAP_PROP_POS_FRAMES, closest_stable[1])
                    hand_frame_count = closest_stable[1]
                    while hand_frame_count < te_frame and hand_frame_count < end_frame:
                        ret, frame = video.read()
                        if not ret:
                            break
                        if roi:
                            x, y, w, h = roi
                            frame_height, frame_width = frame.shape[:2]
                            if w > 0 and h > 0 and y + h <= frame_height and x + w <= frame_width:
                                frame = frame[y:y + h, x:x + w]
                        hand_frame_count += 1
                        if hand_frame_count % self.frame_interval != 0:
                            continue
                        if self.detect_hand(frame) and hand_frame_count > last_te + self.min_packing_time * self.fps:
                            ts_frame = hand_frame_count
                            second_ts = round((ts_frame - 1) / self.fps, 1)
                            if second_ts >= start_time and second_ts <= end_time:
                                logging.info(f"Hand detected for Ts: frame={ts_frame}, time={second_ts}s")
                                break
                            ts_frame = None
                else:
                    # Kh√¥ng c√≥ v√πng ·ªïn ƒë·ªãnh, t√¨m tay ngay sau Te ph√≠a tr∆∞·ªõc
                    if prev_te_frame is not None:
                        video.set(cv2.CAP_PROP_POS_FRAMES, max(prev_te_frame, start_frame))
                        hand_frame_count = max(prev_te_frame, start_frame)
                        while hand_frame_count < te_frame and hand_frame_count < end_frame:
                            ret, frame = video.read()
                            if not ret:
                                break
                            if roi:
                                x, y, w, h = roi
                                frame_height, frame_width = frame.shape[:2]
                                if w > 0 and h > 0 and y + h <= frame_height and x + w <= frame_width:
                                    frame = frame[y:y + h, x:x + w]
                            hand_frame_count += 1
                            if hand_frame_count % self.frame_interval != 0:
                                continue
                            if self.detect_hand(frame) and hand_frame_count > last_te + self.min_packing_time * self.fps:
                                ts_frame = hand_frame_count
                                second_ts = round((ts_frame - 1) / self.fps, 1)
                                if second_ts >= start_time and second_ts <= end_time:
                                    logging.info(f"Hand detected for Ts: frame={ts_frame}, time={second_ts}s")
                                    break
                                ts_frame = None
                # Ghi log
                if ts_frame:
                    second_ts = round((ts_frame - 1) / self.fps)
                    segment_index = math.floor(max(second_ts, second_te) / segment_duration)
                    target_start_second = segment_index * segment_duration
                    target_end_second = (segment_index + 1) * segment_duration
                    if max(second_ts, second_te) >= current_end_second or target_start_second != current_start_second:
                        current_start_second = target_start_second
                        current_end_second = min(target_start_second + segment_duration, end_segment)
                        camera_log_dir = os.path.join(self.log_dir, camera_name)
                        os.makedirs(camera_log_dir, exist_ok=True)
                        log_file = os.path.join(camera_log_dir, f"log_{video_name}_{current_start_second:04d}_{current_end_second:04d}.txt")
                        log_file_handle = self._update_log_file(log_file, current_start_second, current_end_second, start_time_obj + timedelta(seconds=current_start_second), camera_name, video_file)
                    log_file_handle("On,", second_ts - 1)
                    log_file_handle("Off,", second_ts)
                    log_file_handle("Off,", second_te - 1)
                    log_file_handle(f"On,{qr_code}", second_te)
                    logging.info(f"Event logged: Ts={second_ts}, Te={second_te}, QR={qr_code}")
                else:
                    second_ts = second_te - self.min_packing_time - 1
                    if second_ts >= start_time and second_ts >= (last_te / self.fps):
                        segment_index = math.floor(max(second_ts, second_te) / segment_duration)
                        target_start_second = segment_index * segment_duration
                        target_end_second = (segment_index + 1) * segment_duration
                        if max(second_ts, second_te) >= current_end_second or target_start_second != current_start_second:
                            current_start_second = target_start_second
                            current_end_second = min(target_start_second + segment_duration, end_segment)
                            camera_log_dir = os.path.join(self.log_dir, camera_name)
                            os.makedirs(camera_log_dir, exist_ok=True)
                            log_file = os.path.join(camera_log_dir, f"log_{video_name}_{current_start_second:04d}_{current_end_second:04d}.txt")
                            log_file_handle = self._update_log_file(log_file, current_start_second, current_end_second, start_time_obj + timedelta(seconds=current_start_second), camera_name, video_file)
                        log_file_handle("On,", second_ts - 1)
                        log_file_handle("Off,", second_ts)
                        log_file_handle("Off,", second_te - 1)
                        log_file_handle(f"On,{qr_code}", second_te)
                        logging.info(f"Assumed Ts={second_ts} for Te={second_te}, QR={qr_code}")
                    else:
                        log_file_handle("Off,", second_te - 1)
                        log_file_handle(f"On,{qr_code}", second_te)
                        logging.info(f"Logged only Te for QR {qr_code} at second {second_te}: assumed Ts={second_ts} invalid (out of range or too close to last_te)")
                last_te = te_frame
                prev_te_frame = te_frame
            video.release()
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("UPDATE file_list SET is_processed = 1, status = ? WHERE file_path = ?", ("xong", video_file))
                conn.commit()
                conn.close()
            logging.info(f"Completed processing video: {video_file}")
            return log_file

```
## üìÑ File: `frame_sampler_trigger.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/frame_sampler_trigger.py`

```python
import cv2
import os
import logging
import sqlite3
import threading
import subprocess
import json
import numpy as np
from datetime import datetime, timezone, timedelta
from modules.db_utils import get_db_connection
from modules.scheduler.db_sync import frame_sampler_event, db_rwlock
import math
from modules.config.logging_config import get_logger


BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
MODEL_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "models", "wechat_qr")
DETECT_PROTO = os.path.join(MODEL_DIR, "detect.prototxt")
DETECT_MODEL = os.path.join(MODEL_DIR, "detect.caffemodel")
SR_PROTO = os.path.join(MODEL_DIR, "sr.prototxt")
SR_MODEL = os.path.join(MODEL_DIR, "sr.caffemodel")

class FrameSamplerTrigger:
    def __init__(self):
        self.setup_logging()
        self.load_config()
        self.video_lock = threading.Lock()
        self.setup_wechat_qr()

    def setup_logging(self):
        self.logger = get_logger(__name__, {"video_id": os.path.basename(self.video_file)})
        self.logger.info("Logging initialized")

    def load_config(self):
        logging.info("Loading configuration from database")
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT input_path FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            self.video_root = result[0] if result else os.path.join(BASE_DIR, "Inputvideo")
            cursor.execute("SELECT output_path FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            self.output_path = result[0] if result else os.path.join(BASE_DIR, "output_clips")
            self.log_dir = os.path.join(self.output_path, "LOG", "Frame")
            os.makedirs(self.log_dir, exist_ok=True)
            cursor.execute("SELECT timezone FROM general_info WHERE id = 1")
            result = cursor.fetchone()
            tz_hours = int(result[0].split("+")[1]) if result and "+" in result[0] else 7
            self.video_timezone = timezone(timedelta(hours=tz_hours))
            cursor.execute("SELECT frame_rate, frame_interval, min_packing_time FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            self.fps, self.frame_interval, self.min_packing_time = result if result else (30, 5, 5)
            logging.info(f"Config loaded: video_root={self.video_root}, output_path={self.output_path}, timezone={self.video_timezone}, fps={self.fps}, frame_interval={self.frame_interval}, min_packing_time={self.min_packing_time}")

    def get_packing_area(self, camera_name):
        logging.info(f"Querying qr_mvd_area and jump_time_ratio for camera {camera_name}")
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT qr_mvd_area, jump_time_ratio FROM packing_profiles WHERE profile_name = ?", (camera_name,))
            result = cursor.fetchone()
            cursor.execute("SELECT qr_trigger_area FROM packing_profiles WHERE profile_name = ?", (camera_name,))
            trigger_result = cursor.fetchone()
            conn.close()
        if result and result[1] is not None:
            self.jump_time_ratio = float(result[1])
            logging.info(f"Loaded jump_time_ratio: {self.jump_time_ratio}")
        else:
            self.jump_time_ratio = 0.5
            logging.info(f"Using default jump_time_ratio: {self.jump_time_ratio}")

        if result and result[0]:
            try:
                qr_mvd_area = result[0]
                if qr_mvd_area.startswith('(') and qr_mvd_area.endswith(')'):
                    x, y, w, h = map(int, qr_mvd_area.strip('()').split(','))
                else:
                    parsed = json.loads(qr_mvd_area)
                    if isinstance(parsed, list) and len(parsed) == 4:
                        x, y, w, h = parsed
                    else:
                        x, y, w, h = parsed['x'], parsed['y'], parsed['w'], parsed['h']
                roi = (x, y, w, h)
                logging.info(f"Using qr_mvd_area: {roi}")
            except (ValueError, json.JSONDecodeError, KeyError, TypeError) as e:
                logging.error(f"Error parsing qr_mvd_area for camera {camera_name}: {str(e)}")
                roi = None
        else:
            logging.warning(f"No qr_mvd_area found for camera {camera_name}")
            roi = None
        trigger = json.loads(trigger_result[0]) if trigger_result and trigger_result[0] else [0, 0, 0, 0]
        logging.info(f"Using trigger: {trigger}")
        return roi, trigger

    def get_video_duration(self, video_file):
        try:
            cmd = ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", video_file]
            result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            return float(result.stdout.strip())
        except Exception:
            logging.error(f"Failed to get duration of video {video_file}")
            return None

    def load_video_files(self):
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT file_path FROM file_list WHERE is_processed = 0 AND status != 'xong' ORDER BY priority DESC, created_at ASC")
            video_files = [row[0] for row in cursor.fetchall()]
            conn.close()
        if not video_files:
            logging.info("No video files found with is_processed = 0 and status != 'xong'.")
        return video_files

    def process_frame(self, frame, frame_count):
        try:
            if len(frame.shape) == 2:
                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
            texts, _ = self.qr_detector.detectAndDecode(frame)
            state = "Off"
            mvd = ""
            for text in texts:
                if text == "TimeGo":
                    state = "On"
                elif text:
                    mvd = text
            return state, mvd
        except Exception as e:
            logging.error(f"Error processing frame {frame_count}: {str(e)}")
            return "", ""

    def _get_video_start_time(self, video_file):
        try:
            result = subprocess.check_output(['ffprobe', '-v', 'quiet', '-show_entries', 'format_tags=creation_time', '-of', 'default=noprint_wrappers=1:nokey=1', video_file])
            return datetime.strptime(result.decode().strip(), '%Y-%m-%dT%H:%M:%S.%fZ').replace(tzinfo=timezone.utc).astimezone(self.video_timezone)
        except (subprocess.CalledProcessError, ValueError):
            try:
                result = subprocess.check_output(['exiftool', '-CreateDate', '-d', '%Y-%m-%d %H:%M:%S', video_file])
                return datetime.strptime(result.decode().split('CreateDate')[1].strip().split('\n')[0].strip(), '%Y-%m-%d %H:%M:%S').replace(tzinfo=self.video_timezone)
            except (subprocess.CalledProcessError, IndexError):
                try:
                    result = subprocess.check_output(['exiftool', '-FileCreateDate', '-d', '%Y-%m-%d %H:%M:%S', video_file])
                    return datetime.strptime(result.decode().split('FileCreateDate')[1].strip().split('\n')[0].strip(), '%Y-%m-%d %H:%M:%S').replace(tzinfo=self.video_timezone)
                except (subprocess.CalledProcessError, IndexError):
                    logging.warning("No metadata found, using file creation time.")
                    return datetime.fromtimestamp(os.path.getctime(video_file), tz=self.video_timezone)

    def _update_log_file(self, log_file, start_second, end_second, start_time, camera_name, video_file):
        log_file_handle = open(log_file, 'w')
        log_file_handle.write(f"# Start: {start_second}, End: {end_second}, Start_Time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}, Camera_Name: {camera_name}, Video_File: {video_file}\n")
        log_file_handle.flush()
        with db_rwlock.gen_wlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT 1 FROM processed_logs WHERE log_file = ?", (log_file,))
            if not cursor.fetchone():
                cursor.execute("INSERT INTO processed_logs (log_file, is_processed) VALUES (?, 0)", (log_file,))
            conn.commit()
            conn.close()
        return log_file_handle

    def run(self):
        while True:
            frame_sampler_event.wait()
            video_files = self.load_video_files()
            if not video_files:
                logging.info("No videos to process")
                frame_sampler_event.clear()
                continue
            for video_file in video_files:
                log_file = self.process_video(video_file, self.video_lock, self.get_packing_area, self.process_frame, self.frame_interval)
                if log_file:
                    logging.info(f"Completed processing video {video_file}, log at {log_file}")
                else:
                    logging.error(f"Failed to process video {video_file}")
            frame_sampler_event.clear()

    def process_video(self, video_file, video_lock, get_packing_area_func, process_frame_func, frame_interval, start_time=0, end_time=None):
        with video_lock:
            logging.info(f"Processing video: {video_file} from {start_time}s to {end_time}s")
            if not os.path.exists(video_file):
                logging.error(f"File '{video_file}' does not exist")
                with db_rwlock.gen_wlock():
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("l·ªói", video_file))
                    conn.commit()
                    conn.close()
                return None
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("ƒëang frame sampler ...", video_file))
                cursor.execute("SELECT camera_name FROM file_list WHERE file_path = ?", (video_file,))
                result = cursor.fetchone()
                camera_name = result[0] if result and result[0] else "CamTest"
                conn.commit()
                conn.close()
            video = cv2.VideoCapture(video_file)
            if not video.isOpened():
                logging.error(f"Failed to open video '{video_file}'")
                with db_rwlock.gen_wlock():
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("l·ªói", video_file))
                    conn.commit()
                    conn.close()
                return None
            start_time_obj = self._get_video_start_time(video_file)
            roi, trigger = get_packing_area_func(camera_name)
            total_seconds = self.get_video_duration(video_file)
            if total_seconds is None:
                logging.error(f"Failed to get duration of video {video_file}")
                with db_rwlock.gen_wlock():
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("l·ªói", video_file))
                    conn.commit()
                    conn.close()
                return None
            logging.info(f"Video duration {video_file}: {total_seconds} seconds")
            video_name = os.path.splitext(os.path.basename(video_file))[0]
            segment_duration = 300
            # X√°c ƒë·ªãnh c√°c ƒëo·∫°n 300s ch·ª©a [start_time, end_time]
            end_time = total_seconds if end_time is None else min(end_time, total_seconds)
            start_segment = math.floor(start_time / segment_duration) * segment_duration
            end_segment = math.ceil(end_time / segment_duration) * segment_duration
            current_start_second = start_segment
            current_end_second = min(current_start_second + segment_duration, end_segment)
            camera_log_dir = os.path.join(self.log_dir, camera_name)
            os.makedirs(camera_log_dir, exist_ok=True)
            log_file = os.path.join(camera_log_dir, f"log_{video_name}_{current_start_second:04d}_{current_end_second:04d}.txt")
            log_file_handle = self._update_log_file(log_file, current_start_second, current_end_second, start_time_obj + timedelta(seconds=current_start_second), camera_name, video_file)
            # B·∫Øt ƒë·∫ßu t·ª´ khung h√¨nh t·∫°i start_time
            start_frame = int(start_time * self.fps)
            end_frame = int(end_time * self.fps)
            video.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            frame_count = start_frame
            frame_states = []
            mvd_list = []
            last_state = None
            last_mvd = ""
            jump_time_ratio = getattr(self, 'jump_time_ratio', 0.5)  # L·∫•y t·ª´ config ho·∫∑c m·∫∑c ƒë·ªãnh 0.5
            while video.isOpened() and frame_count < end_frame:
                ret, frame = video.read()
                if not ret:
                    break
                if roi:
                    x, y, w, h = roi
                    frame_height, frame_width = frame.shape[:2]
                    if w > 0 and h > 0 and y + h <= frame_height and x + w <= frame_width:
                        frame = frame[y:y + h, x:x + w]
                    else:
                        logging.warning(f"Invalid ROI for frame {frame_count}: {roi}, frame size: {frame_width}x{frame_height}")
                        frame = frame
                frame_count += 1
                if frame_count % frame_interval != 0:
                    continue
                if frame.size == 0 or frame.shape[0] == 0 or frame.shape[1] == 0:
                    logging.warning(f"Empty frame {frame_count}, skipping")
                    continue
                state, mvd = process_frame_func(frame, frame_count)
                second_in_video = (frame_count - 1) / self.fps
                second = round(second_in_video)
                if second >= current_end_second and second < end_time:
                    log_file_handle.close()
                    current_start_second = current_end_second
                    current_end_second = min(current_start_second + segment_duration, end_segment)
                    camera_log_dir = os.path.join(self.log_dir, camera_name)
                    os.makedirs(camera_log_dir, exist_ok=True)
                    log_file = os.path.join(camera_log_dir, f"log_{video_name}_{current_start_second:04d}_{current_end_second:04d}.txt")
                    log_file_handle = self._update_log_file(log_file, current_start_second, current_end_second, start_time_obj + timedelta(seconds=current_start_second), camera_name, video_file)
                if second >= start_time and second <= end_time:
                    # Ghi MVD ngay n·∫øu c√≥ v√† kh√°c last_mvd
                    if mvd and mvd != last_mvd:
                        log_line = f"{second},{state},{mvd}\n"
                        log_file_handle.write(log_line)
                        logging.info(f"Log second {second}: state={state}, mvd={mvd}")
                        log_file_handle.flush()
                        last_mvd = mvd
                    # Ti·∫øp t·ª•c thu th·∫≠p tr·∫°ng th√°i cho final_state
                    frame_states.append(state)
                    mvd_list.append(mvd)
                    if len(frame_states) == 5:
                        on_count = sum(1 for s in frame_states if s == "On")
                        off_count = sum(1 for s in frame_states if s == "Off")
                        frame_states_str = " ".join(frame_states).lower()
                        final_state = None
                        if on_count >= 3:
                            final_state = "On"
                        elif off_count == 5:
                            final_state = "Off"
                        if final_state:
                            if final_state != last_state:
                                log_line = f"{second},{final_state},\n"
                                log_file_handle.write(log_line)
                                logging.info(f"Log second {second}: {frame_states_str}: {final_state}")
                                log_file_handle.flush()
                                if last_state == "On" and final_state == "Off":
                                    jump_frames = int(jump_time_ratio * self.min_packing_time * self.fps)
                                    new_frame_count = frame_count + jump_frames
                                    if new_frame_count < end_frame:
                                        video.set(cv2.CAP_PROP_POS_FRAMES, new_frame_count)
                                        frame_count = new_frame_count
                                        logging.info(f"Jumped {jump_frames} frames to {frame_count} after On->Off transition")
                                last_state = final_state
                        else:
                            logging.info(f"Skipped second {second}: {frame_states_str}, on_count={on_count}, off_count={off_count}")
                            log_file_handle.flush()
                        frame_states = []
                        mvd_list = []
            log_file_handle.close()
            video.release()
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("UPDATE file_list SET is_processed = 1, status = ? WHERE file_path = ?", ("xong", video_file))
                conn.commit()
                conn.close()
            logging.info(f"Completed processing video: {video_file}")
            return log_file

```
## üìÑ File: `cutter_complete.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/cutter/cutter_complete.py`

```python
import subprocess

def cut_complete_event(event, video_buffer, video_length, output_file):
    """C·∫Øt video cho s·ª± ki·ªán ho√†n ch·ªânh (c√≥ c·∫£ ts v√† te)."""
    ts = event.get("ts")
    te = event.get("te")
    video_file = event.get("video_file")

    start_time = max(0, ts - video_buffer)  # Th√™m buffer tr∆∞·ªõc ts
    end_time = min(te + video_buffer, video_length)  # Th√™m buffer sau te
    duration = end_time - start_time

    if duration <= 0:
        print(f"B·ªè qua: Duration kh√¥ng h·ª£p l·ªá ({duration}s) cho s·ª± ki·ªán {event.get('event_id')}")
        return False

    try:
        cmd = [
            "ffmpeg",
            "-i", video_file,
            "-ss", str(start_time),
            "-t", str(duration),
            "-c:v", "copy",
            "-c:a", "copy",
            "-y",
            output_file
        ]
        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        print(f"ƒê√£ c·∫Øt video: {output_file}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"L·ªói khi c·∫Øt video {video_file}: {e}")
        return False
    except Exception as e:
        print(f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")
        return False
```
## üìÑ File: `cutter_incomplete.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/cutter/cutter_incomplete.py`

```python
import os
import subprocess
from .cutter_utils import generate_merged_filename, generate_output_filename

def cut_incomplete_event(event, video_buffer, video_length, output_file):
    """C·∫Øt video cho s·ª± ki·ªán d·ªü dang (thi·∫øu ts ho·∫∑c te)."""
    ts = event.get("ts")
    te = event.get("te")
    video_file = event.get("video_file")

    # Log ƒë·ªô d√†i video g·ªëc
    print(f"Video g·ªëc {video_file} c√≥ ƒë·ªô d√†i: {video_length} gi√¢y")

    # Log gi√° tr·ªã video_buffer
    print(f"S·ª≠ d·ª•ng video_buffer: {video_buffer} gi√¢y")

    if ts is not None and te is None:  # Ch·ªâ c√≥ ts
        start_time = max(0, ts - video_buffer)
        duration = video_length - start_time
    elif ts is None and te is not None:  # Ch·ªâ c√≥ te
        start_time = 0
        duration = min(te + video_buffer, video_length)
    else:
        print(f"B·ªè qua: S·ª± ki·ªán {event.get('event_id')} kh√¥ng c√≥ ts ho·∫∑c te")
        return False

    if duration <= 0:
        print(f"B·ªè qua: Duration kh√¥ng h·ª£p l·ªá ({duration}s) cho s·ª± ki·ªán {event.get('event_id')}")
        return False

    try:
        cmd = [
            "ffmpeg",
            "-i", video_file,
            "-ss", str(start_time),
            "-t", str(duration),
            "-c:v", "copy",
            "-c:a", "copy",
            "-y",
            output_file
        ]
        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        print(f"ƒê√£ c·∫Øt video: {output_file}")

        # Log ƒë·ªô d√†i c·ªßa file d·ªü dang v·ª´a t·∫°o
        probe = subprocess.run(
            ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", output_file],
            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
        )
        duration = float(probe.stdout.strip())
        print(f"File d·ªü dang {output_file} c√≥ ƒë·ªô d√†i: {duration} gi√¢y")

        event["cut_video_file"] = output_file
        return True
    except subprocess.CalledProcessError as e:
        print(f"L·ªói khi c·∫Øt video {video_file}: {e}")
        return False
    except Exception as e:
        print(f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")
        return False

def merge_incomplete_events(event_a, event_b, video_buffer, video_length_a, video_length_b, output_dir, max_packing_time, brand_name="Alan"):
    """Gh√©p n·ªëi hai s·ª± ki·ªán d·ªü dang (A c√≥ ts, B c√≥ te) v√† t·∫°o file gh√©p v·ªõi t√™n t·ªëi ∆∞u."""
    video_file_a = event_a.get("video_file")
    video_file_b = event_b.get("video_file")

    # Ki·ªÉm tra n·∫øu file ƒë√£ ƒë∆∞·ª£c c·∫Øt s·∫µn
    temp_file_a = event_a.get("cut_video_file")
    temp_file_b = event_b.get("cut_video_file")
    files_to_cleanup = []

    # T·∫°o th∆∞ m·ª•c temp_clips ƒë·ªÉ l∆∞u file t·∫°m
    temp_clips_dir = os.path.join(os.path.dirname(output_dir), "temp_clips")
    if not os.path.exists(temp_clips_dir):
        os.makedirs(temp_clips_dir)

    # N·∫øu kh√¥ng c√≥ file c·∫Øt s·∫µn, c·∫Øt ngay l·∫≠p t·ª©c v√† l∆∞u v√†o temp_clips_dir
    if not temp_file_a or not os.path.exists(temp_file_a):
        temp_file_a = os.path.join(temp_clips_dir, f"temp_a_{event_a.get('event_id')}_incomplete.mp4")
        if not cut_incomplete_event(event_a, video_buffer, video_length_a, temp_file_a):
            print(f"L·ªói: Kh√¥ng th·ªÉ c·∫Øt file t·∫°m cho s·ª± ki·ªán {event_a.get('event_id')}")
            return None
    if not temp_file_b or not os.path.exists(temp_file_b):
        temp_file_b = os.path.join(temp_clips_dir, f"temp_b_{event_b.get('event_id')}_incomplete.mp4")
        if not cut_incomplete_event(event_b, video_buffer, video_length_b, temp_file_b):
            print(f"L·ªói: Kh√¥ng th·ªÉ c·∫Øt file t·∫°m cho s·ª± ki·ªán {event_b.get('event_id')}")
            return None

    # T·∫°o t√™n file ƒë·∫ßu ra t·ªëi ∆∞u d·ª±a tr√™n temp_file_a v√† temp_file_b
    file_name_a = os.path.basename(temp_file_a)
    file_name_b = os.path.basename(temp_file_b)
    parts_a = file_name_a.split("_")
    parts_b = file_name_b.split("_")

    # L·∫•y Brand t·ª´ file ƒë·∫ßu ti√™n
    brand_name = parts_a[0]

    # L·∫•y m√£ v·∫≠n ƒë∆°n t·ª´ c·∫£ hai file, lo·∫°i b·ªè "NoCode"
    tracking_codes = []
    if len(parts_a) >= 2 and parts_a[1] != "NoCode":
        tracking_codes.append(parts_a[1])
    if len(parts_b) >= 2 and parts_b[1] != "NoCode":
        tracking_codes.append(parts_b[1])

    # L·∫•y th·ªùi gian t·ª´ file ƒë·∫ßu ti√™n
    date = parts_a[2] if len(parts_a) >= 3 else "unknown"
    hour = parts_a[3].split(".")[0] if len(parts_a) >= 4 else "0000"
    time_str = f"{date}_{hour}"

    # T·∫°o t√™n m√£ v·∫≠n ƒë∆°n: d√πng m·ªôt m√£ ho·∫∑c gh√©p nhi·ªÅu m√£ b·∫±ng "-"
    tracking_str = "-".join(tracking_codes) if tracking_codes else "unknown"

    # T·∫°o t√™n file ƒë·∫ßu ra
    output_file = os.path.join(output_dir, f"{brand_name}_{tracking_str}_{time_str}.mp4")

    # Gh√©p n·ªëi video A v√† B
    concat_list_file = os.path.join(output_dir, f"concat_list_{event_a.get('event_id')}.txt")
    try:
        with open(concat_list_file, 'w') as f:
            f.write(f"file '{temp_file_a}'\nfile '{temp_file_b}'\n")

        cmd_concat = [
            "ffmpeg",
            "-f", "concat",
            "-safe", "0",
            "-i", concat_list_file,
            "-c", "copy",
            "-y",
            output_file
        ]
        subprocess.run(cmd_concat, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

        print(f"ƒê√£ gh√©p v√† c·∫Øt video: {output_file}")

        # Log ƒë·ªô d√†i c·ªßa file gh√©p
        probe = subprocess.run(
            ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", output_file],
            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
        )
        duration = float(probe.stdout.strip())
        print(f"File gh√©p {output_file} c√≥ ƒë·ªô d√†i: {duration} gi√¢y")

        # X√≥a c√°c file t·∫°m v√† file concat_list sau khi gh√©p th√†nh c√¥ng
        if os.path.exists(temp_file_a):
            os.remove(temp_file_a)
        if os.path.exists(temp_file_b):
            os.remove(temp_file_b)
        if os.path.exists(concat_list_file):
            os.remove(concat_list_file)

        return output_file

    except subprocess.CalledProcessError as e:
        print(f"L·ªói khi gh√©p video: {e}")
        # X√≥a c√°c file t·∫°m v√† file concat_list trong tr∆∞·ªùng h·ª£p l·ªói
        if os.path.exists(temp_file_a):
            os.remove(temp_file_a)
        if os.path.exists(temp_file_b):
            os.remove(temp_file_b)
        if os.path.exists(concat_list_file):
            os.remove(concat_list_file)
        return None
    except Exception as e:
        print(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi gh√©p video: {e}")
        # X√≥a c√°c file t·∫°m v√† file concat_list trong tr∆∞·ªùng h·ª£p l·ªói
        if os.path.exists(temp_file_a):
            os.remove(temp_file_a)
        if os.path.exists(temp_file_b):
            os.remove(temp_file_b)
        if os.path.exists(concat_list_file):
            os.remove(concat_list_file)
        return None
```
## üìÑ File: `cutter_utils.py`
**ƒê∆∞·ªùng d·∫´n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/cutter/cutter_utils.py`

```python
import os
from datetime import datetime
import ast

def is_reasonable_timestamp(ts):
    """Ki·ªÉm tra xem timestamp c√≥ h·ª£p l·ªá kh√¥ng (l·ªõn h∆°n nƒÉm 2020)."""
    return ts and int(ts) > 1577836800000  # Tr√™n nƒÉm 2020

def generate_output_filename(event, tracking_codes_filter, output_dir, brand_name="Alan"):
    """T·∫°o t√™n file ƒë·∫ßu ra d·ª±a tr√™n tracking code v√† th·ªùi gian ∆∞u ti√™n: packing_time_start > packing_time_end."""
    tracking_codes_str = event.get("tracking_codes")
    packing_time_start = event.get("packing_time_start")
    packing_time_end = event.get("packing_time_end")

    try:
        tracking_codes = ast.literal_eval(tracking_codes_str) if tracking_codes_str else []
    except (ValueError, SyntaxError) as e:
        print(f"L·ªói parse tracking_codes_str cho event {event.get('event_id')}: {e}")
        tracking_codes = []

    # Ch·ªçn tracking code ∆∞u ti√™n
    if tracking_codes_filter:
        selected_tracking_code = next((code for code in tracking_codes_filter if code in tracking_codes), "NoCode")
    else:
        selected_tracking_code = tracking_codes[-1] if tracking_codes else "NoCode"

    # ∆Øu ti√™n ch·ªçn th·ªùi gian: packing_time_start > packing_time_end > fallback
    timestamp = next(
        (t for t in [packing_time_start, packing_time_end] if is_reasonable_timestamp(t)),
        0  # Fallback
    )
    try:
        time_str = datetime.fromtimestamp(int(timestamp) / 1000).strftime("%Y%m%d_%H%M")
    except Exception:
        time_str = "19700101_0000"

    return os.path.join(output_dir, f"{brand_name}_{selected_tracking_code}_{time_str}.mp4")

def generate_merged_filename(event_a, event_b, output_dir, brand_name="Alan"):
    """T·∫°o t√™n file gh√©p cho hai s·ª± ki·ªán d·ªü dang, ∆∞u ti√™n th·ªùi gian: packing_time_start > packing_time_end."""
    tracking_codes_a_str = event_a.get("tracking_codes")
    tracking_codes_b_str = event_b.get("tracking_codes")
    packing_time_start_a = event_a.get("packing_time_start")
    packing_time_end_b = event_b.get("packing_time_end")

    # Ch·ªçn tracking code (∆∞u ti√™n s·ª± ki·ªán c√≥ m√£ v·∫≠n ƒë∆°n)
    try:
        tracking_codes_a = ast.literal_eval(tracking_codes_a_str) if tracking_codes_a_str else []
    except (ValueError, SyntaxError):
        tracking_codes_a = []
    try:
        tracking_codes_b = ast.literal_eval(tracking_codes_b_str) if tracking_codes_b_str else []
    except (ValueError, SyntaxError):
        tracking_codes_b = []

    selected_tracking_code = tracking_codes_b[-1] if tracking_codes_b else (tracking_codes_a[-1] if tracking_codes_a else "NoCode")

    # ∆Øu ti√™n ch·ªçn th·ªùi gian: packing_time_start > packing_time_end > fallback
    timestamp = next(
        (t for t in [packing_time_start_a, packing_time_end_b] if is_reasonable_timestamp(t)),
        0  # Fallback
    )
    try:
        time_str = datetime.fromtimestamp(int(timestamp) / 1000).strftime("%Y%m%d_%H%M")
    except Exception:
        time_str = "19700101_0000"

    return os.path.join(output_dir, f"{brand_name}_{selected_tracking_code}_{time_str}.mp4")

def update_event_in_db(cursor, event_id, output_file):
    """C·∫≠p nh·∫≠t CSDL cho m·ªôt s·ª± ki·ªán."""
    cursor.execute("""
        UPDATE events 
        SET is_processed = 1, output_file = ? 
        WHERE event_id = ?
    """, (output_file, event_id))
```