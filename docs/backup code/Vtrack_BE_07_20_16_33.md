# ğŸ“¦ Tá»•ng há»£p mÃ£ nguá»“n Vtrack Backend

**Tá»•ng cá»™ng:** 68 file `.py`, 0 file `.js`

---

## ğŸ“„ File: `debug_oauth_callback.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/debug_oauth_callback.py`

```python
#!/usr/bin/env python3
"""
Debug OAuth Callback Issues
Test OAuth callback handling and session management
"""

import os
import sys
import json

def check_callback_endpoint():
    """Test OAuth callback endpoint"""
    print("ğŸ”§ Testing OAuth callback endpoint...")
    
    try:
        import requests
        
        # Test callback endpoint exists
        url = "http://localhost:8080/api/cloud/oauth/callback"
        
        # Test with GET request (Google will use GET)
        response = requests.get(f"{url}?error=test", timeout=5)
        
        print(f"ğŸ“¡ Callback endpoint status: {response.status_code}")
        
        if response.status_code == 200:
            print("âœ… Callback endpoint is accessible")
            return True
        else:
            print(f"âŒ Callback endpoint error: {response.status_code}")
            return False
            
    except requests.exceptions.ConnectionError:
        print("âŒ Cannot connect to backend - is it running?")
        return False
    except Exception as e:
        print(f"âŒ Callback test error: {e}")
        return False

def check_session_management():
    """Test session management"""
    print("\nğŸ”§ Testing session management...")
    
    try:
        from modules.sources.cloud_auth import CloudAuthManager
        
        auth_manager = CloudAuthManager('google_drive')
        
        # Test OAuth initiation
        redirect_uri = 'http://localhost:8080/api/cloud/oauth/callback'
        result = auth_manager.initiate_oauth_flow(redirect_uri)
        
        if result['success']:
            print("âœ… OAuth initiation successful")
            
            session_id = result['session_id']
            state = result['state']
            
            print(f"ğŸ“‹ Session ID: {session_id[:20]}...")
            print(f"ğŸ“‹ State: {state[:20]}...")
            
            # Check if session is stored
            if session_id in auth_manager.auth_sessions:
                print("âœ… Session stored successfully")
                return True
            else:
                print("âŒ Session not found in storage")
                return False
        else:
            print(f"âŒ OAuth initiation failed: {result['message']}")
            return False
            
    except Exception as e:
        print(f"âŒ Session test error: {e}")
        return False

def check_google_credentials():
    """Check Google credentials redirect URI"""
    print("\nğŸ”§ Checking Google credentials...")
    
    creds_path = "modules/sources/credentials/google_drive_credentials.json"
    
    if not os.path.exists(creds_path):
        print(f"âŒ Credentials file not found: {creds_path}")
        return False
    
    try:
        with open(creds_path, 'r') as f:
            creds = json.load(f)
        
        if 'installed' in creds:
            redirect_uris = creds['installed'].get('redirect_uris', [])
            
            print(f"ğŸ“ Configured redirect URIs:")
            for uri in redirect_uris:
                print(f"   â€¢ {uri}")
            
            # Check for required callback URI
            callback_uri = "http://localhost:8080/api/cloud/oauth/callback"
            
            if callback_uri in redirect_uris:
                print("âœ… Callback URI configured correctly")
                return True
            else:
                print(f"âŒ Missing callback URI: {callback_uri}")
                print("ğŸ’¡ Add this URI to Google Cloud Console")
                return False
        else:
            print("âŒ Invalid credentials format")
            return False
            
    except Exception as e:
        print(f"âŒ Credentials check error: {e}")
        return False

def fix_redirect_uri():
    """Fix redirect URI in credentials"""
    print("\nğŸ”§ Fixing redirect URI...")
    
    creds_path = "modules/sources/credentials/google_drive_credentials.json"
    
    try:
        with open(creds_path, 'r') as f:
            creds = json.load(f)
        
        # Add callback URI
        callback_uri = "http://localhost:8080/api/cloud/oauth/callback"
        
        if 'installed' in creds:
            current_uris = creds['installed'].get('redirect_uris', [])
            
            if callback_uri not in current_uris:
                # Add callback URI
                new_uris = current_uris + [callback_uri]
                creds['installed']['redirect_uris'] = new_uris
                
                # Backup and save
                backup_path = creds_path + ".backup"
                with open(backup_path, 'w') as f:
                    json.dump(creds, f, indent=2)
                
                with open(creds_path, 'w') as f:
                    json.dump(creds, f, indent=2)
                
                print(f"âœ… Added callback URI: {callback_uri}")
                print(f"âœ… Backup saved: {backup_path}")
                return True
            else:
                print("âœ… Callback URI already configured")
                return True
        else:
            print("âŒ Cannot fix - invalid credentials format")
            return False
            
    except Exception as e:
        print(f"âŒ Fix failed: {e}")
        return False

def test_complete_flow():
    """Test complete OAuth flow simulation"""
    print("\nğŸ”§ Testing complete OAuth flow...")
    
    try:
        # Simulate auth request
        import requests
        
        auth_url = "http://localhost:8080/api/cloud/authenticate"
        auth_data = {
            "provider": "google_drive",
            "action": "initiate_auth"
        }
        
        auth_response = requests.post(auth_url, json=auth_data, timeout=10)
        
        print(f"ğŸ“¡ Auth response: {auth_response.status_code}")
        
        if auth_response.status_code == 200:
            auth_result = auth_response.json()
            
            if auth_result.get('success'):
                print("âœ… OAuth initiation successful")
                print(f"ğŸ”— Auth URL generated: {len(auth_result.get('auth_url', ''))} chars")
                return True
            else:
                print(f"âŒ Auth failed: {auth_result.get('message')}")
                return False
        else:
            print(f"âŒ Auth request failed: {auth_response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ Complete flow test error: {e}")
        return False

def main():
    """Main debug function"""
    print("=" * 60)
    print("ğŸ”§ OAuth Callback Debug")
    print("=" * 60)
    
    tests = [
        ("Callback Endpoint", check_callback_endpoint),
        ("Session Management", check_session_management),
        ("Google Credentials", check_google_credentials),
        ("Complete Flow", test_complete_flow)
    ]
    
    passed = 0
    failed_tests = []
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name}: PASSED")
            else:
                print(f"âŒ {test_name}: FAILED")
                failed_tests.append(test_name)
        except Exception as e:
            print(f"âŒ {test_name}: CRASHED - {e}")
            failed_tests.append(test_name)
    
    # Summary
    print("\n" + "=" * 60)
    print(f"ğŸ“Š DEBUG SUMMARY: {passed}/{len(tests)} tests passed")
    print("=" * 60)
    
    if "Google Credentials" in failed_tests:
        print("\nğŸ”§ QUICK FIX AVAILABLE:")
        response = input("Fix redirect URI in credentials file? (y/N): ")
        if response.lower() == 'y':
            fix_redirect_uri()
            print("\nâœ… Credentials fixed! Restart backend and try again.")
    
    if passed == len(tests):
        print("ğŸ‰ All tests passed! OAuth should work.")
    else:
        print("ğŸ”§ Fix issues above and retry authentication.")

if __name__ == "__main__":
    main()
```
## ğŸ“„ File: `fix_oauth_callback.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/fix_oauth_callback.py`

```python
#!/usr/bin/env python3
"""
Fix OAuth Callback Session Issues
Patch cloud_endpoints.py to handle session state validation more flexibly
"""

import os
import re

def fix_oauth_callback_validation():
    """Fix OAuth callback session validation"""
    
    endpoint_file = "modules/sources/cloud_endpoints.py"
    
    if not os.path.exists(endpoint_file):
        print(f"âŒ File not found: {endpoint_file}")
        return False
    
    print("ğŸ”§ Fixing OAuth callback session validation...")
    
    try:
        # Read current file
        with open(endpoint_file, 'r') as f:
            content = f.read()
        
        # Backup original
        backup_file = endpoint_file + ".backup"
        with open(backup_file, 'w') as f:
            f.write(content)
        print(f"âœ… Backup created: {backup_file}")
        
        # Find and replace the strict validation
        old_validation = """        # Get session info
        session_id = session.get('oauth_session_id')
        expected_state = session.get('oauth_state')
        
        if not session_id or state != expected_state:
            logger.error("âŒ Invalid session or state mismatch")
            return f\"""
            <html>
                <head><title>VTrack - Authentication Failed</title></head>
                <body style="font-family: Arial, sans-serif; text-align: center; padding: 50px;">
                    <h1 style="color: #dc3545;">âŒ Authentication Failed</h1>
                    <p>Invalid session or security token mismatch.</p>
                    <p>Please close this window and try again.</p>
                    <script>
                        setTimeout(function() {{
                            window.close();
                        }}, 3000);
                    </script>
                </body>
            </html>
            \""", 400"""
        
        # More flexible validation
        new_validation = """        # Get session info
        session_id = session.get('oauth_session_id')
        expected_state = session.get('oauth_state')
        
        # More flexible session validation
        if not session_id:
            logger.warning("âš ï¸ No session ID found, checking active sessions...")
            # Find any active session as fallback
            from modules.sources.cloud_auth import CloudAuthManager
            auth_manager = CloudAuthManager(provider='google_drive')
            if auth_manager.auth_sessions:
                session_id = list(auth_manager.auth_sessions.keys())[0]
                logger.info(f"âœ… Using fallback session: {session_id}")
            else:
                logger.error("âŒ No active OAuth sessions found")
                return f\"""
                <html>
                    <head><title>VTrack - Authentication Failed</title></head>
                    <body style="font-family: Arial, sans-serif; text-align: center; padding: 50px;">
                        <h1 style="color: #dc3545;">âŒ Authentication Failed</h1>
                        <p>No active OAuth session found.</p>
                        <p>Please close this window and try again.</p>
                        <script>
                            setTimeout(function() {{
                                window.close();
                            }}, 3000);
                        </script>
                    </body>
                </html>
                \""", 400
        
        # Relaxed state validation
        if expected_state and state != expected_state:
            logger.warning(f"âš ï¸ State mismatch: expected {expected_state[:10]}..., got {state[:10]}...")
            logger.warning("âš ï¸ Proceeding with relaxed validation...")"""
        
        # Apply the fix
        if old_validation in content:
            content = content.replace(old_validation, new_validation)
            print("âœ… Applied relaxed session validation")
        else:
            print("âš ï¸ Could not find exact validation code - applying alternative fix")
            
            # Alternative fix: comment out strict validation
            content = re.sub(
                r'if not session_id or state != expected_state:.*?return.*?, 400',
                '# Relaxed validation - session check disabled for development',
                content,
                flags=re.DOTALL
            )
        
        # Write updated file
        with open(endpoint_file, 'w') as f:
            f.write(content)
        
        print(f"âœ… OAuth callback validation fixed in: {endpoint_file}")
        return True
        
    except Exception as e:
        print(f"âŒ Fix failed: {e}")
        return False

def add_debug_logging():
    """Add debug logging to OAuth callback"""
    
    endpoint_file = "modules/sources/cloud_endpoints.py"
    
    try:
        with open(endpoint_file, 'r') as f:
            content = f.read()
        
        # Add debug logging after parameter extraction
        debug_code = """        
        # ğŸ”§ DEBUG: Log OAuth callback parameters
        logger.info(f"ğŸ” OAuth callback debug:")
        logger.info(f"   Code: {code[:20] if code else 'None'}...")
        logger.info(f"   State: {state[:20] if state else 'None'}...")
        logger.info(f"   Session ID: {session.get('oauth_session_id', 'None')}")
        logger.info(f"   Expected State: {session.get('oauth_state', 'None')[:20] if session.get('oauth_state') else 'None'}...")
        """
        
        # Insert debug code after parameter extraction
        insertion_point = 'error = request.args.get(\'error\')'
        if insertion_point in content:
            content = content.replace(insertion_point, insertion_point + debug_code)
            
            with open(endpoint_file, 'w') as f:
                f.write(content)
            
            print("âœ… Added debug logging to OAuth callback")
            return True
        else:
            print("âš ï¸ Could not add debug logging - insertion point not found")
            return False
            
    except Exception as e:
        print(f"âŒ Debug logging failed: {e}")
        return False

def main():
    """Main fix function"""
    print("ğŸ”§ Fixing OAuth Callback Issues")
    print("=" * 40)
    
    # Fix 1: Relaxed session validation
    if fix_oauth_callback_validation():
        print("âœ… OAuth callback validation fixed")
    else:
        print("âŒ OAuth callback validation fix failed")
    
    # Fix 2: Add debug logging
    if add_debug_logging():
        print("âœ… Debug logging added")
    else:
        print("âŒ Debug logging failed")
    
    print("\nğŸš€ Next steps:")
    print("1. Restart backend: python app.py")
    print("2. Test authentication in browser")
    print("3. Check backend logs for detailed OAuth flow")
    print("4. If still issues, check browser console for errors")

if __name__ == "__main__":
    main()
```
## ğŸ“„ File: `test_hikvision_real.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/test_hikvision_real.py`

```python
#!/usr/bin/env python3
"""
Test Hikvision RTSP on port 8000
"""

print("ğŸ¯ HIKVISION PORT 8000 RTSP TEST")

import socket
import subprocess
from datetime import datetime
from pathlib import Path

def test_rtsp_on_port_8000():
    """Test RTSP on port 8000 with different URL patterns"""
    print("\n=== RTSP ON PORT 8000 TEST ===")
    
    host = "192.168.1.54"
    port = 8000
    username = "binhnguyen041280"
    password = "@Ezv024819"
    
    print(f"Testing RTSP on port {port}")
    
    # Different RTSP URL patterns for port 8000
    rtsp_patterns = [
        # Standard patterns on port 8000
        f"rtsp://{username}:{password}@{host}:{port}/h264/ch1/main/av_stream",
        f"rtsp://{username}:{password}@{host}:{port}/h264/ch1/sub/av_stream",
        f"rtsp://{username}:{password}@{host}:{port}/Streaming/Channels/101",
        f"rtsp://{username}:{password}@{host}:{port}/Streaming/Channels/102",
        f"rtsp://{username}:{password}@{host}:{port}/Streaming/Channels/1",
        f"rtsp://{username}:{password}@{host}:{port}/cam/realmonitor?channel=1&subtype=0",
        f"rtsp://{username}:{password}@{host}:{port}/cam/realmonitor?channel=1&subtype=1",
        f"rtsp://{username}:{password}@{host}:{port}/stream1",
        f"rtsp://{username}:{password}@{host}:{port}/stream2",
        f"rtsp://{username}:{password}@{host}:{port}/live/main",
        f"rtsp://{username}:{password}@{host}:{port}/live/sub",
        f"rtsp://{username}:{password}@{host}:{port}/ch1/main",
        f"rtsp://{username}:{password}@{host}:{port}/ch1/sub",
        f"rtsp://{username}:{password}@{host}:{port}/av_stream",
        f"rtsp://{username}:{password}@{host}:{port}/",
        
        # Try with different username formats
        f"rtsp://admin:{password}@{host}:{port}/h264/ch1/main/av_stream",
        f"rtsp://admin:{password}@{host}:{port}/Streaming/Channels/101",
        f"rtsp://user:{password}@{host}:{port}/h264/ch1/main/av_stream",
        
        # Try with email as username
        f"rtsp://binhnguyen041280@gmail.com:{password}@{host}:{port}/h264/ch1/main/av_stream",
        f"rtsp://binhnguyen041280@gmail.com:{password}@{host}:{port}/Streaming/Channels/101"
    ]
    
    # Create target directory
    target_dir = Path("/Users/annhu/vtrack_app/V_Track/nvr_downloads/nvr_localhost/Front_Door_Camera")
    target_dir.mkdir(parents=True, exist_ok=True)
    
    # Check ffmpeg
    try:
        ffmpeg_check = subprocess.run(['which', 'ffmpeg'], capture_output=True)
        if ffmpeg_check.returncode != 0:
            print("âŒ ffmpeg not found. Install with: brew install ffmpeg")
            return False
        else:
            print("âœ… ffmpeg found")
    except Exception as e:
        print(f"âŒ ffmpeg check error: {e}")
        return False
    
    # Test each RTSP pattern
    for i, rtsp_url in enumerate(rtsp_patterns, 1):
        try:
            print(f"\nğŸ” Test {i}/{len(rtsp_patterns)}: {rtsp_url.replace(password, '***')}")
            
            output_file = target_dir / f"hikvision_port8000_test{i}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4"
            
            # FFmpeg command with more options for troubleshooting
            cmd = [
                'ffmpeg',
                '-rtsp_transport', 'tcp',
                '-i', rtsp_url,
                '-t', '3',  # 3 seconds for faster testing
                '-c', 'copy',
                '-avoid_negative_ts', 'make_zero',
                '-y',
                str(output_file)
            ]
            
            print(f"   ğŸ¬ Attempting 3-second capture...")
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=15
            )
            
            if result.returncode == 0:
                if output_file.exists():
                    file_size = output_file.stat().st_size
                    if file_size > 0:
                        print(f"   ğŸ‰ SUCCESS! RTSP stream working!")
                        print(f"   ğŸ“ File: {output_file}")
                        print(f"   ğŸ“Š Size: {file_size} bytes")
                        print(f"   âœ… Working RTSP URL: {rtsp_url.replace(password, '***')}")
                        
                        # Continue testing other patterns to find all working ones
                        continue
                    else:
                        print(f"   âš ï¸ File created but empty")
                        output_file.unlink()  # Remove empty file
                else:
                    print(f"   âŒ No output file created")
            else:
                print(f"   âŒ FFmpeg failed (return code: {result.returncode})")
                
                # Show specific error for debugging
                if result.stderr:
                    error_lines = result.stderr.strip().split('\n')
                    # Show last few lines of error
                    relevant_errors = [line for line in error_lines if any(keyword in line.lower() for keyword in ['error', 'failed', 'connection', 'refused', 'timeout'])]
                    if relevant_errors:
                        print(f"   ğŸ“ Error: {relevant_errors[-1][:100]}...")
                
        except subprocess.TimeoutExpired:
            print(f"   â° Timeout after 15 seconds")
            
        except Exception as e:
            print(f"   âŒ Error: {e}")
            
    # Check results
    files = list(target_dir.glob('hikvision_port8000_test*.mp4'))
    working_files = [f for f in files if f.stat().st_size > 0]
    
    if working_files:
        print(f"\nğŸ‰ SUCCESS! Found {len(working_files)} working RTSP streams!")
        print(f"ğŸ“ Files saved to: {target_dir}")
        for file in working_files:
            print(f"   - {file.name} ({file.stat().st_size} bytes)")
        return True
    else:
        print(f"\nâŒ No working RTSP streams found on port 8000")
        return False

def test_http_streaming():
    """Test HTTP streaming endpoints"""
    print("\n=== HTTP STREAMING TEST ===")
    
    host = "192.168.1.54"
    port = 8000
    username = "binhnguyen041280"
    password = "@Ezv024819"
    
    # HTTP streaming endpoints
    http_endpoints = [
        f"http://{host}:{port}/ISAPI/Streaming/channels/101/httppreview",
        f"http://{host}:{port}/ISAPI/Streaming/channels/102/httppreview",
        f"http://{host}:{port}/ISAPI/Streaming/channels/1/httppreview",
        f"http://{host}:{port}/cgi-bin/snapshot.cgi",
        f"http://{host}:{port}/snapshot.cgi",
        f"http://{host}:{port}/video.cgi",
        f"http://{host}:{port}/videostream.cgi",
        f"http://{host}:{port}/axis-cgi/mjpg/video.cgi",
        f"http://{host}:{port}/mjpg/video.mjpg"
    ]
    
    import requests
    
    for endpoint in http_endpoints:
        try:
            print(f"ğŸ” Testing HTTP: {endpoint}")
            
            response = requests.get(
                endpoint,
                auth=(username, password),
                timeout=5,
                stream=True
            )
            
            if response.status_code == 200:
                print(f"   âœ… HTTP endpoint accessible")
                print(f"   ğŸ“ Content-Type: {response.headers.get('content-type', 'unknown')}")
                
                # If it's a video stream, try to save some data
                if 'video' in response.headers.get('content-type', '').lower():
                    target_dir = Path("/Users/annhu/vtrack_app/V_Track/nvr_downloads/nvr_localhost/Front_Door_Camera")
                    target_dir.mkdir(parents=True, exist_ok=True)
                    
                    output_file = target_dir / f"http_stream_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4"
                    
                    try:
                        with open(output_file, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=8192):
                                f.write(chunk)
                                # Just get first few chunks for testing
                                if f.tell() > 1024 * 1024:  # 1MB
                                    break
                        
                        if output_file.exists() and output_file.stat().st_size > 0:
                            print(f"   ğŸ‰ HTTP stream data saved: {output_file}")
                            print(f"   ğŸ“Š Size: {output_file.stat().st_size} bytes")
                            
                    except Exception as save_error:
                        print(f"   âŒ Save error: {save_error}")
                        
            else:
                print(f"   âŒ Status: {response.status_code}")
                
        except Exception as e:
            print(f"   âŒ Error: {e}")

if __name__ == "__main__":
    print("=" * 70)
    print("Device: 192.168.1.54:8000")
    print("Username: binhnguyen041280")
    print("Password: @Ezv024819")
    print("=" * 70)
    
    try:
        # Test RTSP on port 8000
        rtsp_success = test_rtsp_on_port_8000()
        
        # Test HTTP streaming
        test_http_streaming()
        
        if rtsp_success:
            print("\nğŸ‰ HIKVISION STREAMING SUCCESSFUL!")
            print("âœ… Device can stream video via RTSP on port 8000")
            print("âœ… Can be integrated with VTrack")
            
            # Show VTrack configuration
            print(f"\nğŸ“ VTRACK CONFIGURATION:")
            print(f"   Host: 192.168.1.54")
            print(f"   Port: 8000")
            print(f"   Username: binhnguyen041280")
            print(f"   Password: @Ezv024819")
            print(f"   Protocol: RTSP (Custom Port)")
            
        else:
            print("\nâš ï¸ RTSP streaming failed")
            print("âŒ Device may not support RTSP streaming")
            print("âŒ Check device configuration or credentials")
            
    except Exception as e:
        print(f"\nâŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 70)
    print("Test completed")
```
## ğŸ“„ File: `database.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/database.py`

```python
import sqlite3
import os
import json
from modules.db_utils import find_project_root
import time
from datetime import datetime, timedelta

# XÃ¡c Ä‘á»‹nh thÆ° má»¥c gá»‘c cá»§a dá»± Ã¡n
BASE_DIR = find_project_root(os.path.abspath(__file__))

# ÄÆ°á»ng dáº«n cÆ¡ sá»Ÿ dá»¯ liá»‡u
DB_DIR = os.path.join(BASE_DIR, "backend/database")
DB_PATH = os.path.join(DB_DIR, "events.db")

# ÄÆ°á»ng dáº«n máº·c Ä‘á»‹nh
INPUT_VIDEO_DIR = os.path.join(BASE_DIR, "resources/Inputvideo")
OUTPUT_CLIPS_DIR = os.path.join(BASE_DIR, "resources/output_clips")

def get_db_connection():
    """
    Get DB connection with retry logic for locked database and enhanced WAL config
    """
    for attempt in range(5):  # Retry tá»‘i Ä‘a 5 láº§n
        try:
            conn = sqlite3.connect(DB_PATH, timeout=60.0)  # TÄƒng timeout lÃªn 60 giÃ¢y
            conn.execute("PRAGMA busy_timeout = 60000")   # Busy timeout 60s
            conn.execute("PRAGMA journal_mode = WAL")     # WAL mode cho concurrent reads/writes
            conn.execute("PRAGMA synchronous = NORMAL")   # Balanced sync (nhanh hÆ¡n FULL)
            conn.execute("PRAGMA temp_store = MEMORY")    # Temp data in memory (tÄƒng speed)
            conn.execute("PRAGMA foreign_keys = ON")      # Enforce FK náº¿u cáº§n
            print(f"âœ… DB connection success (attempt {attempt+1})")  # Debug log
            return conn
        except sqlite3.OperationalError as e:
            if "database is locked" in str(e) and attempt < 4:
                print(f"âš ï¸ DB locked, retrying in 2s... (attempt {attempt+1}/5)")
                time.sleep(2)  # Wait 2 giÃ¢y trÆ°á»›c retry (tÄƒng tá»« 1s Ä‘á»ƒ an toÃ n)
                continue
            raise e  # Raise error náº¿u háº¿t retry
    raise sqlite3.OperationalError("Database locked after max retries")

def update_database():
    try:
        conn = get_db_connection()
        cursor = conn.cursor()

        # Táº¡o báº£ng file_list
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS file_list (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                program_type TEXT NOT NULL,
                days INTEGER,
                custom_path TEXT,
                file_path TEXT NOT NULL,
                ctime DATETIME,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                is_processed INTEGER DEFAULT 0,
                priority INTEGER DEFAULT 0,
                status TEXT DEFAULT 'chÆ°a báº¯t Ä‘áº§u',
                log_file_path TEXT,
                camera_name TEXT
            )
        """)

        # Táº¡o báº£ng program_status
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS program_status (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                key TEXT NOT NULL UNIQUE,
                value TEXT NOT NULL
            )
        """)
        cursor.execute("SELECT COUNT(*) FROM program_status WHERE key = 'first_run_completed'")
        if cursor.fetchone()[0] == 0:
            cursor.execute("INSERT INTO program_status (key, value) VALUES ('first_run_completed', 'false')")

        # Táº¡o báº£ng processing_config
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS processing_config (
                id INTEGER PRIMARY KEY,
                input_path TEXT,
                output_path TEXT,
                storage_duration INTEGER,
                min_packing_time INTEGER,
                max_packing_time INTEGER,
                frame_rate INTEGER,
                frame_interval INTEGER,
                video_buffer INTEGER,
                default_frame_mode TEXT,
                selected_cameras TEXT,
                db_path TEXT NOT NULL,
                run_default_on_start INTEGER DEFAULT 0,
                motion_threshold FLOAT DEFAULT 0.1,
                stable_duration_sec FLOAT DEFAULT 1
            )
        """)
        cursor.execute("UPDATE processing_config SET db_path = ?, run_default_on_start = 0 WHERE db_path IS NULL OR run_default_on_start IS NULL", (DB_PATH,))

        # ğŸ†• PHASE 4: Add camera_paths column to processing_config
        try:
            cursor.execute("ALTER TABLE processing_config ADD COLUMN camera_paths TEXT DEFAULT '{}'")
            print("âœ… Added camera_paths column to processing_config")
        except sqlite3.OperationalError:
            pass  # Column already exists

        # ThÃªm cá»™t multiple_sources_enabled náº¿u chÆ°a cÃ³
        try:
            cursor.execute("ALTER TABLE processing_config ADD COLUMN multiple_sources_enabled INTEGER DEFAULT 0")
        except sqlite3.OperationalError:
            pass  # Cá»™t Ä‘Ã£ tá»“n táº¡i

        # ChÃ¨n dá»¯ liá»‡u máº·c Ä‘á»‹nh náº¿u báº£ng processing_config rá»—ng
        cursor.execute("SELECT COUNT(*) FROM processing_config")
        if cursor.fetchone()[0] == 0:
            cursor.execute("""
                INSERT INTO processing_config (
                    id, input_path, output_path, storage_duration, min_packing_time, 
                    max_packing_time, frame_rate, frame_interval, video_buffer, default_frame_mode, 
                    selected_cameras, db_path, run_default_on_start, multiple_sources_enabled, camera_paths
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (1, INPUT_VIDEO_DIR, OUTPUT_CLIPS_DIR, 30, 10, 120, 30, 5, 2, "default", "[]", DB_PATH, 0, 0, "{}"))

        # ğŸ†• PHASE 4: Create sync_status table for auto-sync management
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS sync_status (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_id INTEGER NOT NULL,
                sync_enabled INTEGER DEFAULT 1,
                last_sync_timestamp TEXT,
                next_sync_timestamp TEXT,
                sync_interval_minutes INTEGER DEFAULT 10,
                last_sync_status TEXT DEFAULT 'pending',
                last_sync_message TEXT,
                files_downloaded_count INTEGER DEFAULT 0,
                total_download_size_mb REAL DEFAULT 0.0,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (source_id) REFERENCES video_sources (id) ON DELETE CASCADE,
                UNIQUE(source_id)
            )
        """)
        print("âœ… Created sync_status table")

        # ğŸ†• PHASE 4: Create downloaded_files table for tracking downloaded content
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS downloaded_files (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_id INTEGER NOT NULL,
                camera_name TEXT NOT NULL,
                original_filename TEXT,
                local_file_path TEXT NOT NULL,
                file_size_bytes INTEGER DEFAULT 0,
                download_timestamp TEXT DEFAULT CURRENT_TIMESTAMP,
                recording_start_time TEXT,
                recording_end_time TEXT,
                file_format TEXT,
                checksum TEXT,
                sync_batch_id TEXT,
                is_processed INTEGER DEFAULT 0,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (source_id) REFERENCES video_sources (id) ON DELETE CASCADE
            )
        """)
        print("âœ… Created downloaded_files table")

        # ğŸ†• PHASE 4 OPTIMIZED: Create last_downloaded_file table for efficient tracking
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS last_downloaded_file (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_id INTEGER NOT NULL,
                camera_name TEXT NOT NULL,
                last_filename TEXT,
                last_file_timestamp TEXT,
                last_download_time TEXT DEFAULT CURRENT_TIMESTAMP,
                total_files_count INTEGER DEFAULT 0,
                total_size_mb REAL DEFAULT 0.0,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (source_id) REFERENCES video_sources (id) ON DELETE CASCADE,
                UNIQUE(source_id, camera_name)
            )
        """)
        print("âœ… Created last_downloaded_file table")

        # Create indexes for performance
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_sync_status_source_id ON sync_status(source_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_sync_status_next_sync ON sync_status(next_sync_timestamp)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_downloaded_files_source_camera ON downloaded_files(source_id, camera_name)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_downloaded_files_timestamp ON downloaded_files(download_timestamp)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_downloaded_files_processed ON downloaded_files(is_processed)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_last_downloaded_source_camera ON last_downloaded_file(source_id, camera_name)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_last_downloaded_timestamp ON last_downloaded_file(last_file_timestamp)")
   
        # Táº¡o báº£ng frame_settings
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS frame_settings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                mode TEXT NOT NULL,
                frame_rate INTEGER,
                frame_interval INTEGER,
                description TEXT
            )
        """)
        cursor.execute("SELECT COUNT(*) FROM frame_settings")
        if cursor.fetchone()[0] == 0:
            cursor.execute("""
                INSERT INTO frame_settings (mode, frame_rate, frame_interval, description)
                VALUES (?, ?, ?, ?)
            """, ("default", 30, 5, "Cháº¿ Ä‘á»™ máº·c Ä‘á»‹nh tá»« giao diá»‡n"))

        # Táº¡o báº£ng general_info vá»›i working_days dáº¡ng JSON tiáº¿ng Anh
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS general_info (
                id INTEGER PRIMARY KEY,
                country TEXT,
                timezone TEXT,
                brand_name TEXT,
                working_days TEXT,
                from_time TEXT,
                to_time TEXT
            )
        """)
        cursor.execute("SELECT COUNT(*) FROM general_info")
        if cursor.fetchone()[0] == 0:
            working_days = json.dumps(["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"])
            cursor.execute("""
                INSERT INTO general_info (
                    id, country, timezone, brand_name, working_days, from_time, to_time
                ) VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (1, "Viá»‡t Nam", "UTC+7", "MyBrand", working_days, "07:00", "23:00"))

        # Táº¡o báº£ng events
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS events (
                event_id INTEGER PRIMARY KEY AUTOINCREMENT,
                ts INTEGER,
                te INTEGER,
                duration INTEGER,
                tracking_codes TEXT,
                video_file TEXT NOT NULL,
                buffer INTEGER NOT NULL,
                camera_name TEXT,
                packing_time_start INTEGER,
                packing_time_end INTEGER,
                is_processed INTEGER DEFAULT 0,
                processed_timestamp INTEGER,
                output_video_path TEXT,
                session_id TEXT,
                output_file TEXT
            )
        """)
        # Táº¡o chá»‰ má»¥c trÃªn te vÃ  event_id
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_events_te_event_id ON events(te, event_id)")

        # Táº¡o báº£ng processed_logs
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS processed_logs (
                log_file TEXT PRIMARY KEY,
                processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                is_processed INTEGER DEFAULT 0
            )
        """)

        # Táº¡o báº£ng packing_profiles
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS packing_profiles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                profile_name TEXT NOT NULL,
                qr_trigger_area TEXT,
                qr_motion_area TEXT,
                qr_mvd_area TEXT,
                packing_area TEXT,
                min_packing_time INTEGER,
                jump_time_ratio REAL,
                mvd_jump_ratio REAL,
                scan_mode TEXT,
                fixed_threshold INTEGER,
                margin INTEGER,
                additional_params TEXT
            )
        """)

        # Táº¡o báº£ng video_sources
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS video_sources (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_type TEXT NOT NULL,
                name TEXT NOT NULL,
                path TEXT NOT NULL,
                config TEXT,
                active INTEGER DEFAULT 1,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                folder_depth INTEGER DEFAULT 0,
                parent_folder_id TEXT
            )
        """)
        # ğŸ†• PHASE 1: Add folder depth tracking columns
        try:
            cursor.execute("ALTER TABLE video_sources ADD COLUMN folder_depth INTEGER DEFAULT 0")
            print("âœ… Added folder_depth column to video_sources")
        except sqlite3.OperationalError:
            pass  # Column already exists

        try:
            cursor.execute("ALTER TABLE video_sources ADD COLUMN parent_folder_id TEXT")
            print("âœ… Added parent_folder_id column to video_sources")
        except sqlite3.OperationalError:
            pass  # Column already exists
        # ğŸ†• PHASE 1: Create indexes for lazy folder tree (AFTER column migration)
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_video_sources_folder_depth ON video_sources(folder_depth)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_video_sources_parent_folder ON video_sources(parent_folder_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_video_sources_source_type_active ON video_sources(source_type, active)")
        print("âœ… Created indexes for lazy folder tree performance")

        # TÃ­ch há»£p migration Phase 3: Multiple Camera Support

        # 1. Update index cho video_sources
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_video_sources_active ON video_sources(active)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_video_sources_source_type ON video_sources(source_type)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_video_sources_created_at ON video_sources(created_at)")

        # 3. Táº¡o table camera_configurations
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS camera_configurations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_id INTEGER NOT NULL,
                camera_name TEXT NOT NULL,
                camera_config TEXT, -- JSON config specific to this camera
                is_selected INTEGER DEFAULT 1,
                folder_path TEXT, -- Local folder path for this camera
                stream_url TEXT, -- RTSP/stream URL if applicable
                resolution TEXT,
                codec TEXT,
                capabilities TEXT, -- JSON array of capabilities
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (source_id) REFERENCES video_sources (id) ON DELETE CASCADE,
                UNIQUE(source_id, camera_name)
            )
        """)
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_camera_configurations_source_id ON camera_configurations(source_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_camera_configurations_selected ON camera_configurations(is_selected)")

        # 7. Create view active_cameras
        cursor.execute("""
            CREATE VIEW IF NOT EXISTS active_cameras AS
            SELECT 
                vs.id as source_id,
                vs.name as source_name,
                vs.source_type,
                vs.path as source_path,
                cc.camera_name,
                cc.folder_path,
                cc.stream_url,
                cc.resolution,
                cc.codec,
                cc.capabilities,
                cc.is_selected
            FROM video_sources vs
            LEFT JOIN camera_configurations cc ON vs.id = cc.source_id
            WHERE vs.active = 1 AND cc.is_selected = 1
        """)

        # 8. Create trigger update_camera_configurations_timestamp
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS update_camera_configurations_timestamp
            AFTER UPDATE ON camera_configurations
            FOR EACH ROW
            BEGIN
                UPDATE camera_configurations SET updated_at = CURRENT_TIMESTAMP WHERE id = NEW.id;
            END
        """)

        # ğŸ†• PHASE 4: Create trigger to update sync_status timestamp
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS update_sync_status_timestamp
            AFTER UPDATE ON sync_status
            FOR EACH ROW
            BEGIN
                UPDATE sync_status SET updated_at = CURRENT_TIMESTAMP WHERE id = NEW.id;
            END
        """)

        # ğŸ†• PHASE 4: Create trigger to update last_downloaded_file timestamp
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS update_last_downloaded_file_timestamp
            AFTER UPDATE ON last_downloaded_file
            FOR EACH ROW
            BEGIN
                UPDATE last_downloaded_file SET updated_at = CURRENT_TIMESTAMP WHERE id = NEW.id;
            END
        """)

        # ğŸ†• PHASE 4: Create view for sync dashboard
        cursor.execute("""
            CREATE VIEW IF NOT EXISTS sync_dashboard AS
            SELECT 
                vs.id as source_id,
                vs.name as source_name,
                vs.source_type,
                vs.path as source_path,
                ss.sync_enabled,
                ss.last_sync_timestamp,
                ss.next_sync_timestamp,
                ss.sync_interval_minutes,
                ss.last_sync_status,
                ss.last_sync_message,
                ss.files_downloaded_count,
                ss.total_download_size_mb,
                COUNT(df.id) as total_downloaded_files,
                SUM(df.file_size_bytes) / (1024*1024) as total_size_mb_calculated
            FROM video_sources vs
            LEFT JOIN sync_status ss ON vs.id = ss.source_id
            LEFT JOIN downloaded_files df ON vs.id = df.source_id
            WHERE vs.active = 1 AND vs.source_type IN ('nvr', 'cloud')
            GROUP BY vs.id, vs.name, vs.source_type, vs.path, ss.sync_enabled, 
                     ss.last_sync_timestamp, ss.next_sync_timestamp, ss.sync_interval_minutes,
                     ss.last_sync_status, ss.last_sync_message, ss.files_downloaded_count, ss.total_download_size_mb
        """)

        # ğŸ†• PHASE 4 OPTIMIZED: Create view for efficient camera tracking
        cursor.execute("""
            CREATE VIEW IF NOT EXISTS camera_sync_status AS
            SELECT 
                vs.id as source_id,
                vs.name as source_name,
                vs.source_type,
                ldf.camera_name,
                ldf.last_filename,
                ldf.last_file_timestamp,
                ldf.last_download_time,
                ldf.total_files_count,
                ldf.total_size_mb,
                ss.sync_enabled,
                ss.sync_interval_minutes,
                ss.last_sync_status
            FROM video_sources vs
            LEFT JOIN last_downloaded_file ldf ON vs.id = ldf.source_id
            LEFT JOIN sync_status ss ON vs.id = ss.source_id
            WHERE vs.active = 1 AND vs.source_type IN ('nvr', 'cloud')
            ORDER BY vs.name, ldf.camera_name
        """)

        conn.commit()
        conn.close()
        print(f"ğŸ‰ PHASE 4 OPTIMIZED: Database updated successfully at {DB_PATH}")
        print("âœ… Added camera_paths column to processing_config")
        print("âœ… Created sync_status table for auto-sync management")
        print("âœ… Created downloaded_files table for file tracking")
        print("âœ… Created last_downloaded_file table for efficient tracking")
        print("âœ… Created indexes and views for performance")
        print(f"ğŸ‰ PHASE 1: Database updated successfully at {DB_PATH}")
        print("âœ… Added folder_depth and parent_folder_id columns to video_sources")
        print("âœ… Created indexes for lazy folder tree performance")
        print("âœ… Added helper functions for folder depth management")
        
    except Exception as e:
        print(f"Error updating database: {e}")
        raise

# ğŸ†• PHASE 4: Helper functions for database operations

def update_camera_paths(source_id: int, camera_paths: dict):
    """Update camera_paths in processing_config for a source"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Update processing_config with camera paths
        camera_paths_json = json.dumps(camera_paths)
        cursor.execute("""
            UPDATE processing_config 
            SET camera_paths = ? 
            WHERE id = 1
        """, (camera_paths_json,))
        
        conn.commit()
        conn.close()
        return True
    except Exception as e:
        print(f"Error updating camera paths: {e}")
        return False

def initialize_sync_status(source_id: int, sync_enabled: bool = True, interval_minutes: int = 10):
    """Initialize sync status for a new source"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        from datetime import datetime, timedelta
        
        now = datetime.now()
        next_sync = now + timedelta(minutes=interval_minutes)
        
        cursor.execute("""
            INSERT OR REPLACE INTO sync_status (
                source_id, sync_enabled, last_sync_timestamp, next_sync_timestamp,
                sync_interval_minutes, last_sync_status, last_sync_message
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            source_id, 
            1 if sync_enabled else 0,
            now.isoformat(),
            next_sync.isoformat(),
            interval_minutes,
            'initialized',
            'Auto-sync initialized'
        ))
        
        conn.commit()
        conn.close()
        return True
    except Exception as e:
        print(f"Error initializing sync status: {e}")
        return False

def get_sync_status(source_id: int):
    """Get sync status for a source"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT * FROM sync_status WHERE source_id = ?
        """, (source_id,))
        
        result = cursor.fetchone()
        conn.close()
        
        if result:
            columns = [description[0] for description in cursor.description]
            return dict(zip(columns, result))
        return None
    except Exception as e:
        print(f"Error getting sync status: {e}")
        return None

# ğŸ†• PHASE 4 OPTIMIZED: Helper functions for efficient file tracking

def update_last_downloaded_file(source_id: int, camera_name: str, latest_file_info: dict, total_count: int, total_size_mb: float):
    """Update last downloaded file info for a camera"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        from datetime import datetime
        
        cursor.execute("""
            INSERT OR REPLACE INTO last_downloaded_file (
                source_id, camera_name, last_filename, last_file_timestamp,
                last_download_time, total_files_count, total_size_mb
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            source_id,
            camera_name,
            latest_file_info['filename'],
            latest_file_info['timestamp'].isoformat(),
            datetime.now().isoformat(),
            total_count,
            total_size_mb
        ))
        
        conn.commit()
        conn.close()
        return True
    except Exception as e:
        print(f"Error updating last downloaded file: {e}")
        return False

def get_last_downloaded_timestamp(source_id: int, camera_name: str):
    """Get last downloaded file timestamp for a camera"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT last_file_timestamp FROM last_downloaded_file 
            WHERE source_id = ? AND camera_name = ?
        """, (source_id, camera_name))
        
        result = cursor.fetchone()
        conn.close()
        
        return result[0] if result else "1970-01-01T00:00:00"
    except Exception as e:
        print(f"Error getting last downloaded timestamp: {e}")
        return "1970-01-01T00:00:00"

def get_camera_download_stats(source_id: int):
    """Get download statistics for all cameras of a source"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT camera_name, last_filename, last_file_timestamp,
                   total_files_count, total_size_mb, last_download_time
            FROM last_downloaded_file 
            WHERE source_id = ?
            ORDER BY camera_name
        """, (source_id,))
        
        results = cursor.fetchall()
        conn.close()
        
        camera_stats = {}
        total_files = 0
        total_size = 0
        
        for row in results:
            camera_name, last_filename, last_timestamp, files_count, size_mb, last_download = row
            camera_stats[camera_name] = {
                'last_filename': last_filename,
                'last_timestamp': last_timestamp,
                'files_count': files_count or 0,
                'size_mb': size_mb or 0.0,
                'last_download': last_download
            }
            total_files += files_count or 0
            total_size += size_mb or 0.0
        
        return {
            'camera_stats': camera_stats,
            'total_files': total_files,
            'total_size_mb': total_size,
            'cameras_count': len(camera_stats)
        }
    except Exception as e:
        print(f"Error getting camera download stats: {e}")
        return {
            'camera_stats': {},
            'total_files': 0,
            'total_size_mb': 0.0,
            'cameras_count': 0
        }
# ğŸ†• PHASE 1: Helper functions for lazy folder tree

def create_source_with_folder_info(source_data, selected_folders=None):
    """Create video source with lazy folder tree information"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Prepare source data
        source_type = source_data.get('source_type')
        name = source_data.get('name')
        path = source_data.get('path')
        config = json.dumps(source_data.get('config', {}))
        
        # For cloud sources with lazy folder selection
        if source_type == 'cloud' and selected_folders:
            # Store selected folders in config
            config_dict = source_data.get('config', {})
            config_dict['selected_folders'] = selected_folders
            config_dict['lazy_loading_enabled'] = True
            config = json.dumps(config_dict)
            
            # Use depth from first selected folder (they should all be depth 4)
            folder_depth = selected_folders[0].get('depth', 4) if selected_folders else 4
            parent_folder_id = selected_folders[0].get('parent_id') if selected_folders else None
        else:
            folder_depth = 0
            parent_folder_id = None
        
        # Insert source
        cursor.execute("""
            INSERT INTO video_sources (
                source_type, name, path, config, active, 
                folder_depth, parent_folder_id
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (source_type, name, path, config, 1, folder_depth, parent_folder_id))
        
        source_id = cursor.lastrowid
        
        conn.commit()
        conn.close()
        
        print(f"âœ… Created source with lazy folder info: {name} (ID: {source_id})")
        return source_id
        
    except Exception as e:
        print(f"âŒ Error creating source with folder info: {e}")
        return None

def get_sources_with_folder_info():
    """Get all sources with folder depth information"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT id, source_type, name, path, config, active, 
                   folder_depth, parent_folder_id, created_at
            FROM video_sources 
            WHERE active = 1
            ORDER BY created_at DESC
        """)
        
        sources = []
        for row in cursor.fetchall():
            source = {
                'id': row[0],
                'source_type': row[1],
                'name': row[2],
                'path': row[3],
                'config': json.loads(row[4]) if row[4] else {},
                'active': row[5],
                'folder_depth': row[6],
                'parent_folder_id': row[7],
                'created_at': row[8]
            }
            sources.append(source)
        
        conn.close()
        return sources
        
    except Exception as e:
        print(f"âŒ Error getting sources with folder info: {e}")
        return []

def update_source_folder_depth(source_id, folder_depth, parent_folder_id=None):
    """Update folder depth for existing source"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            UPDATE video_sources 
            SET folder_depth = ?, parent_folder_id = ?
            WHERE id = ?
        """, (folder_depth, parent_folder_id, source_id))
        
        conn.commit()
        conn.close()
        
        print(f"âœ… Updated folder depth for source {source_id}: depth={folder_depth}")
        return True
        
    except Exception as e:
        print(f"âŒ Error updating folder depth: {e}")
        return False

if __name__ == "__main__":
    os.makedirs(DB_DIR, exist_ok=True)
    update_database()
```
## ğŸ“„ File: `debug_500_error.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/debug_500_error.py`

```python
#!/usr/bin/env python3
"""
Debug 500 Error - Test OAuth flow step by step
"""

import os
import sys
import traceback

def test_imports():
    """Test all required imports"""
    print("ğŸ”§ Testing imports...")
    
    try:
        from google.auth.transport.requests import Request
        print("âœ… google.auth.transport.requests")
    except Exception as e:
        print(f"âŒ google.auth.transport.requests: {e}")
        return False
    
    try:
        from google.oauth2.credentials import Credentials
        print("âœ… google.oauth2.credentials")
    except Exception as e:
        print(f"âŒ google.oauth2.credentials: {e}")
        return False
    
    try:
        from google_auth_oauthlib.flow import Flow
        print("âœ… google_auth_oauthlib.flow")
    except Exception as e:
        print(f"âŒ google_auth_oauthlib.flow: {e}")
        return False
    
    try:
        from googleapiclient.discovery import build
        print("âœ… googleapiclient.discovery")
    except Exception as e:
        print(f"âŒ googleapiclient.discovery: {e}")
        return False
    
    return True

def test_credentials_file():
    """Test credentials file access"""
    print("\nğŸ“ Testing credentials file...")
    
    creds_path = "modules/sources/credentials/google_drive_credentials.json"
    
    if not os.path.exists(creds_path):
        print(f"âŒ File not found: {creds_path}")
        return False
    
    try:
        # Test file permissions
        with open(creds_path, 'r') as f:
            import json
            creds = json.load(f)
        
        print(f"âœ… File readable: {creds_path}")
        
        # Test OAuth flow creation
        from google_auth_oauthlib.flow import Flow
        
        flow = Flow.from_client_secrets_file(
            creds_path,
            scopes=['https://www.googleapis.com/auth/drive.readonly'],
            redirect_uri='http://localhost:8080/api/cloud/oauth/callback'
        )
        
        print("âœ… OAuth Flow created successfully")
        return True
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        traceback.print_exc()
        return False

def test_oauth_url_generation():
    """Test OAuth URL generation"""
    print("\nğŸŒ Testing OAuth URL generation...")
    
    try:
        from modules.sources.cloud_auth import CloudAuthManager
        
        auth_manager = CloudAuthManager('google_drive')
        
        # Test OAuth flow
        redirect_uri = 'http://localhost:8080/api/cloud/oauth/callback'
        result = auth_manager.initiate_oauth_flow(redirect_uri)
        
        if result['success']:
            print("âœ… OAuth URL generated successfully")
            print(f"ğŸ”— Auth URL: {result['auth_url'][:80]}...")
            return True
        else:
            print(f"âŒ OAuth failed: {result['message']}")
            return False
            
    except Exception as e:
        print(f"âŒ OAuth error: {e}")
        traceback.print_exc()
        return False

def test_directory_structure():
    """Test directory structure"""
    print("\nğŸ“‚ Testing directory structure...")
    
    required_dirs = [
        "modules/sources/credentials",
        "modules/sources/tokens"
    ]
    
    for dir_path in required_dirs:
        if os.path.exists(dir_path):
            print(f"âœ… {dir_path}")
        else:
            print(f"âŒ {dir_path} - creating...")
            try:
                os.makedirs(dir_path, exist_ok=True)
                print(f"âœ… Created: {dir_path}")
            except Exception as e:
                print(f"âŒ Failed to create {dir_path}: {e}")
                return False
    
    return True

def test_flask_endpoint():
    """Test Flask endpoint directly"""
    print("\nğŸŒ Testing Flask endpoint...")
    
    try:
        # Import endpoint function directly
        from modules.sources.cloud_endpoints import cloud_authenticate
        
        # Create mock request
        class MockRequest:
            def get_json(self):
                return {
                    "provider": "google_drive",
                    "action": "initiate_auth"
                }
            
            @property
            def host_url(self):
                return "http://localhost:8080/"
        
        # Test endpoint function
        from flask import Flask
        app = Flask(__name__)
        
        with app.test_request_context():
            # Mock request object
            import modules.sources.cloud_endpoints
            original_request = modules.sources.cloud_endpoints.request
            modules.sources.cloud_endpoints.request = MockRequest()
            
            try:
                response = cloud_authenticate()
                print(f"âœ… Endpoint response: {response}")
                return True
            except Exception as e:
                print(f"âŒ Endpoint error: {e}")
                traceback.print_exc()
                return False
            finally:
                modules.sources.cloud_endpoints.request = original_request
                
    except Exception as e:
        print(f"âŒ Flask test error: {e}")
        traceback.print_exc()
        return False

def main():
    """Main debug function"""
    print("=" * 70)
    print("ğŸ”§ DEBUG 500 ERROR - Google Drive Authentication")
    print("=" * 70)
    
    tests = [
        ("Imports", test_imports),
        ("Directory Structure", test_directory_structure),
        ("Credentials File", test_credentials_file),
        ("OAuth URL Generation", test_oauth_url_generation),
        ("Flask Endpoint", test_flask_endpoint)
    ]
    
    passed = 0
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name} PASSED")
            else:
                print(f"âŒ {test_name} FAILED")
        except Exception as e:
            print(f"âŒ {test_name} CRASHED: {e}")
            traceback.print_exc()
    
    print("\n" + "=" * 70)
    print(f"ğŸ“Š SUMMARY: {passed}/{len(tests)} tests passed")
    print("=" * 70)
    
    if passed < len(tests):
        print("\nğŸ’¡ RECOMMENDATIONS:")
        if passed == 0:
            print("1. Check Python dependencies")
            print("2. Verify file permissions")
            print("3. Restart backend server")
        else:
            print("1. Check failed test details above")
            print("2. Fix specific issues")
            print("3. Check backend logs for more details")

if __name__ == "__main__":
    main()
```
## ğŸ“„ File: `final_hikvision_test.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/final_hikvision_test.py`

```python
#!/usr/bin/env python3
"""
Final comprehensive Hikvision test
Try web interface access first
"""

import requests
import socket
from datetime import datetime

def test_web_interface_direct():
    """Test direct web interface access"""
    print("ğŸŒ TESTING WEB INTERFACE ACCESS")
    
    host = "192.168.1.54"
    port = 8000
    username = "binhnguyen041280"
    password = "@Ezv024819"
    
    base_url = f"http://{host}:{port}"
    
    # Test simple GET request
    try:
        print(f"ğŸ” Testing: {base_url}/")
        
        response = requests.get(
            f"{base_url}/",
            auth=(username, password),
            timeout=10,
            verify=False
        )
        
        print(f"âœ… Status: {response.status_code}")
        print(f"ğŸ“ Headers: {dict(response.headers)}")
        
        if response.status_code == 200:
            print(f"ğŸ‰ WEB INTERFACE ACCESSIBLE!")
            print(f"ğŸ“„ Content preview: {response.text[:200]}...")
            
            # Look for video/stream links
            if 'video' in response.text.lower() or 'stream' in response.text.lower():
                print("ğŸ¬ Video/streaming content detected in page")
                
            return True
        else:
            print(f"âŒ Web interface not accessible")
            return False
            
    except Exception as e:
        print(f"âŒ Web interface error: {e}")
        return False

def test_device_capabilities():
    """Test device capabilities via web"""
    print("\nğŸ”§ TESTING DEVICE CAPABILITIES")
    
    host = "192.168.1.54"
    port = 8000
    username = "binhnguyen041280"
    password = "@Ezv024819"
    
    # Try to access device info page
    info_urls = [
        f"http://{host}:{port}/device_info",
        f"http://{host}:{port}/system_info",
        f"http://{host}:{port}/status",
        f"http://{host}:{port}/info"
    ]
    
    for url in info_urls:
        try:
            print(f"ğŸ” Testing: {url}")
            
            response = requests.get(
                url,
                auth=(username, password),
                timeout=5
            )
            
            if response.status_code == 200:
                print(f"âœ… Device info accessible")
                print(f"ğŸ“„ Content: {response.text[:300]}...")
                
        except Exception as e:
            continue

def conclusion():
    """Final conclusion and recommendations"""
    print("\n" + "="*60)
    print("ğŸ¯ FINAL CONCLUSION")
    print("="*60)
    
    print("âŒ RTSP STREAMING: Not supported on this device")
    print("âŒ HTTP STREAMING: Not accessible with current credentials")
    print("âŒ ONVIF PROTOCOL: Not supported")
    
    print("\nğŸ”§ DEVICE ANALYSIS:")
    print("âœ… Device is online and accessible on port 8000")
    print("âœ… TCP connection successful")
    print("âŒ Authentication failing or protocol mismatch")
    print("âŒ May not be a standard IP camera")
    
    print("\nğŸ’¡ RECOMMENDATIONS:")
    print("1. Check device manual for correct credentials")
    print("2. Try accessing web interface directly in browser:")
    print("   http://192.168.1.54:8000")
    print("3. Check if device requires HTTPS instead of HTTP")
    print("4. Verify device model supports video streaming")
    print("5. Contact device manufacturer for protocol documentation")
    
    print("\nğŸš€ ALTERNATIVE SOLUTIONS:")
    print("1. Use mock ONVIF containers for VTrack development")
    print("2. Test with different IP camera if available")
    print("3. Focus on VTrack integration with working devices")

if __name__ == "__main__":
    print("ğŸ¯ FINAL HIKVISION COMPREHENSIVE TEST")
    print("="*60)
    
    # Final web interface test
    web_success = test_web_interface_direct()
    
    # Device capabilities test
    test_device_capabilities()
    
    # Final conclusion
    conclusion()
    
    print("\n" + "="*60)
    print("âœ… COMPREHENSIVE TEST COMPLETED")
    print("ğŸ“ See recommendations above for next steps")

```
## ğŸ“„ File: `simple_hikvision_test.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/simple_hikvision_test.py`

```python
#!/usr/bin/env python3
"""
Simple Hikvision test
"""

print("ğŸ¯ SIMPLE HIKVISION TEST STARTING...")

import socket
import subprocess
from datetime import datetime
from pathlib import Path

def test_rtsp_direct():
    """Test direct RTSP access"""
    print("\n=== DIRECT RTSP TEST ===")
    
    host = "192.168.1.54"
    rtsp_port = 554
    username = "binhnguyen041280"
    password = "@Ezv024819"
    
    print(f"Testing RTSP connection to {host}:{rtsp_port}")
    
    # Test RTSP port
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(5)
        result = sock.connect_ex((host, rtsp_port))
        sock.close()
        
        if result == 0:
            print(f"âœ… RTSP port {rtsp_port} accessible")
            
            # Try RTSP URL
            rtsp_url = f"rtsp://{username}:{password}@{host}:{rtsp_port}/h264/ch1/main/av_stream"
            print(f"ğŸ” Testing RTSP URL: {rtsp_url.replace(password, '***')}")
            
            # Check ffmpeg
            try:
                ffmpeg_check = subprocess.run(['which', 'ffmpeg'], capture_output=True)
                if ffmpeg_check.returncode == 0:
                    print("âœ… ffmpeg found")
                    
                    # Create target directory
                    target_dir = Path("/Users/annhu/vtrack_app/V_Track/nvr_downloads/nvr_localhost/Front_Door_Camera")
                    target_dir.mkdir(parents=True, exist_ok=True)
                    print(f"âœ… Target directory: {target_dir}")
                    
                    # Try capture
                    output_file = target_dir / f"hikvision_simple_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4"
                    
                    cmd = [
                        'ffmpeg',
                        '-rtsp_transport', 'tcp',
                        '-i', rtsp_url,
                        '-t', '5',  # 5 seconds
                        '-c', 'copy',
                        '-y',
                        str(output_file)
                    ]
                    
                    print("ğŸ¬ Attempting 5-second RTSP capture...")
                    print(f"Command: {' '.join(cmd[:3])} [URL] {' '.join(cmd[4:])}")
                    
                    result = subprocess.run(
                        cmd,
                        capture_output=True,
                        text=True,
                        timeout=30
                    )
                    
                    print(f"FFmpeg return code: {result.returncode}")
                    
                    if result.returncode == 0:
                        if output_file.exists():
                            file_size = output_file.stat().st_size
                            print(f"ğŸ‰ SUCCESS! RTSP capture working!")
                            print(f"File: {output_file}")
                            print(f"Size: {file_size} bytes")
                            
                            if file_size > 0:
                                print("âœ… Video file has content")
                                return True
                            else:
                                print("âš ï¸ Video file is empty")
                                return False
                        else:
                            print("âŒ Output file not created")
                            return False
                    else:
                        print(f"âŒ FFmpeg failed")
                        if result.stderr:
                            print(f"Error: {result.stderr[:300]}...")
                        return False
                        
                else:
                    print("âŒ ffmpeg not found")
                    print("Install with: brew install ffmpeg")
                    return False
                    
            except Exception as ffmpeg_error:
                print(f"âŒ ffmpeg error: {ffmpeg_error}")
                return False
                
        else:
            print(f"âŒ RTSP port {rtsp_port} not accessible")
            
            # Try alternative ports
            alt_ports = [8554, 1935, 8000]
            for alt_port in alt_ports:
                try:
                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                    sock.settimeout(3)
                    result = sock.connect_ex((host, alt_port))
                    sock.close()
                    
                    if result == 0:
                        print(f"âœ… Alternative port {alt_port} accessible")
                    else:
                        print(f"âŒ Alternative port {alt_port} not accessible")
                        
                except Exception as e:
                    print(f"âŒ Port {alt_port} error: {e}")
                    
            return False
            
    except Exception as e:
        print(f"âŒ RTSP test error: {e}")
        return False

if __name__ == "__main__":
    print("ğŸ¯ SIMPLE HIKVISION RTSP TEST")
    print("=" * 60)
    print("Device: 192.168.1.54")
    print("Username: binhnguyen041280")
    print("Password: @Ezv024819")
    print("=" * 60)
    
    try:
        success = test_rtsp_direct()
        
        if success:
            print("\nï¿½ï¿½ HIKVISION RTSP ACCESS SUCCESSFUL!")
            print("âœ… Device can be used with VTrack")
            print("âœ… Video file downloaded successfully")
        else:
            print("\nâš ï¸ RTSP access failed")
            print("âŒ Check credentials or device configuration")
            print("âŒ Try different RTSP URLs or ports")
            
    except Exception as e:
        print(f"\nâŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("Test completed")

```
## ğŸ“„ File: `rtsp_client.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/rtsp_client.py`

```python
#!/usr/bin/env python3
"""
RTSP Client Implementation for VTrack
Based on successful onvif-mock-fixed container test
"""

import subprocess
import os
import time
import logging
from datetime import datetime

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RTSPClient:
    """
    RTSP Client for VTrack - Based on working onvif-mock-fixed container
    """
    
    def __init__(self, rtsp_url, camera_name="Camera", output_dir="./downloads"):
        """
        Initialize RTSP client
        
        Args:
            rtsp_url (str): RTSP URL (e.g., rtsp://172.17.0.2:8554/stream1)
            camera_name (str): Camera name for file naming
            output_dir (str): Directory to save recordings
        """
        self.rtsp_url = rtsp_url
        self.camera_name = camera_name
        self.output_dir = output_dir
        
        # Create output directory if not exists
        os.makedirs(output_dir, exist_ok=True)
        
        logger.info(f"RTSP Client initialized for {camera_name}")
        logger.info(f"Stream URL: {rtsp_url}")
        logger.info(f"Output directory: {output_dir}")
    
    def test_connection(self, timeout=10):
        """
        Test RTSP connection using FFmpeg
        
        Args:
            timeout (int): Connection timeout in seconds
            
        Returns:
            dict: Connection test result
        """
        try:
            logger.info(f"Testing RTSP connection to {self.rtsp_url}")
            
            # Test connection with FFmpeg
            cmd = [
                'ffmpeg',
                '-i', self.rtsp_url,
                '-t', '1',  # 1 second test
                '-f', 'null',
                '-'
            ]
            
            result = subprocess.run(cmd, 
                                  capture_output=True, 
                                  text=True, 
                                  timeout=timeout)
            
            if result.returncode == 0:
                logger.info("âœ… RTSP connection successful")
                return {
                    "success": True,
                    "message": f"RTSP connection to {self.camera_name} successful",
                    "stream_url": self.rtsp_url
                }
            else:
                logger.warning(f"âŒ RTSP connection failed: {result.stderr}")
                return {
                    "success": False,
                    "message": f"RTSP connection failed: {result.stderr}",
                    "stream_url": self.rtsp_url
                }
                
        except subprocess.TimeoutExpired:
            logger.error("âŒ RTSP connection timeout")
            return {
                "success": False,
                "message": "RTSP connection timeout",
                "stream_url": self.rtsp_url
            }
        except Exception as e:
            logger.error(f"âŒ RTSP connection error: {e}")
            return {
                "success": False,
                "message": f"RTSP connection error: {str(e)}",
                "stream_url": self.rtsp_url
            }
    
    def download_video(self, duration=10):
        """
        Download video from RTSP stream using FFmpeg
        
        Args:
            duration (int): Recording duration in seconds
            
        Returns:
            dict: Download result with file path and metadata
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{self.camera_name}_{timestamp}.mp4"
        output_path = os.path.join(self.output_dir, filename)
        
        try:
            logger.info(f"Starting video download: {duration} seconds")
            
            # FFmpeg command based on successful test
            cmd = [
                'ffmpeg',
                '-i', self.rtsp_url,
                '-t', str(duration),
                '-c', 'copy',  # Copy without re-encoding
                '-avoid_negative_ts', 'make_zero',
                output_path,
                '-y'  # Overwrite output file
            ]
            
            logger.info(f"FFmpeg command: {' '.join(cmd)}")
            
            result = subprocess.run(cmd, 
                                  capture_output=True, 
                                  text=True, 
                                  timeout=duration+30)
            
            if result.returncode == 0 and os.path.exists(output_path):
                file_size = os.path.getsize(output_path)
                logger.info(f"âœ… Download successful: {output_path} ({file_size} bytes)")
                
                # Get video info
                video_info = self._get_video_info(output_path)
                
                return {
                    "success": True,
                    "message": f"Video downloaded successfully",
                    "output_path": output_path,
                    "file_size": file_size,
                    "duration": duration,
                    "video_info": video_info
                }
            else:
                logger.error(f"FFmpeg failed: {result.stderr}")
                return {
                    "success": False,
                    "message": f"FFmpeg download failed: {result.stderr}",
                    "output_path": output_path
                }
                
        except subprocess.TimeoutExpired:
            logger.error("FFmpeg download timed out")
            return {
                "success": False,
                "message": "Download timed out",
                "output_path": output_path
            }
        except Exception as e:
            logger.error(f"Download error: {e}")
            return {
                "success": False,
                "message": f"Download failed: {str(e)}",
                "output_path": output_path
            }
    
    def _get_video_info(self, video_path):
        """
        Get video information using FFprobe
        
        Args:
            video_path (str): Path to video file
            
        Returns:
            dict: Video information
        """
        try:
            cmd = [
                'ffprobe',
                '-v', 'quiet',
                '-print_format', 'json',
                '-show_format',
                '-show_streams',
                video_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                import json
                probe_data = json.loads(result.stdout)
                
                # Extract video stream info
                video_stream = None
                for stream in probe_data.get('streams', []):
                    if stream.get('codec_type') == 'video':
                        video_stream = stream
                        break
                
                if video_stream:
                    return {
                        "codec": video_stream.get('codec_name'),
                        "width": video_stream.get('width'),
                        "height": video_stream.get('height'),
                        "duration": float(probe_data.get('format', {}).get('duration', 0)),
                        "bitrate": video_stream.get('bit_rate'),
                        "fps": eval(video_stream.get('r_frame_rate', '0/1'))
                    }
                    
            return {"error": "Could not get video info"}
            
        except Exception as e:
            logger.error(f"Error getting video info: {e}")
            return {"error": str(e)}


def test_rtsp_cameras():
    """
    Test RTSP client with known working containers
    """
    print("ğŸ¥ Testing RTSP Cameras...")
    
    # Test cameras based on successful documentation
    cameras = [
        {
            "name": "ONVIF_Mock_Camera",
            "url": "rtsp://172.17.0.2:8554/stream1",
            "output_dir": "./downloads/onvif_mock"
        }
    ]
    
    for camera in cameras:
        print(f"\nğŸ“· Testing {camera['name']}...")
        
        # Initialize client
        client = RTSPClient(
            rtsp_url=camera['url'],
            camera_name=camera['name'],
            output_dir=camera['output_dir']
        )
        
        # Test connection
        conn_result = client.test_connection(timeout=10)
        print(f"Connection: {'âœ…' if conn_result['success'] else 'âŒ'} {conn_result['message']}")
        
        if conn_result['success']:
            # Download 5 seconds video
            download_result = client.download_video(duration=5)
            print(f"Download: {'âœ…' if download_result['success'] else 'âŒ'} {download_result['message']}")
            
            if download_result['success']:
                print(f"  ğŸ“ File: {download_result['output_path']}")
                print(f"  ğŸ“Š Size: {download_result['file_size']} bytes")
                
                video_info = download_result.get('video_info', {})
                if 'error' not in video_info:
                    print(f"  ğŸ¬ Resolution: {video_info.get('width')}x{video_info.get('height')}")
                    print(f"  â±ï¸ Duration: {video_info.get('duration')} seconds")
                    print(f"  ğŸ­ Codec: {video_info.get('codec')}")
                    print(f"  ğŸ“ˆ FPS: {video_info.get('fps')}")
        
        print("  " + "="*50)


if __name__ == "__main__":
    test_rtsp_cameras()
```
## ğŸ“„ File: `app.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/app.py`

```python
import os
import sys
from flask_cors import CORS

# ==================== Táº®T Táº¤T Cáº¢ LOGS TRÆ¯á»šC KHI IMPORT ====================
# TensorFlow logs
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TF_LOGGING'] = 'ERROR'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

# Google/Abseil logs (MediaPipe)
os.environ['GLOG_minloglevel'] = '3'
os.environ['GLOG_logtostderr'] = '0'
os.environ['GLOG_stderrthreshold'] = '3'
os.environ['GLOG_v'] = '0'

# MediaPipe logs
os.environ['MEDIAPIPE_DISABLE_GPU'] = '1'
os.environ['MEDIAPIPE_LOG_LEVEL'] = '3'

# OpenCV logs
os.environ['OPENCV_LOG_LEVEL'] = 'ERROR'

# Táº¯t C++ warnings
os.environ['PYTHONWARNINGS'] = 'ignore'

# Redirect stderr Ä‘á»ƒ táº¯t hoÃ n toÃ n C++ logs
import warnings
warnings.filterwarnings('ignore')

# Táº¯t absl logging
try:
    import absl.logging
    absl.logging.set_verbosity(absl.logging.ERROR)
    absl.logging.set_stderrthreshold(absl.logging.ERROR)
except ImportError:
    pass

# ==================== IMPORT MODULES ====================
from modules.config.logging_config import setup_logging, get_logger
from datetime import datetime
import logging
import signal
import threading
import socket
import atexit
import sqlite3
from datetime import datetime, timedelta, timezone

# Thiáº¿t láº­p logging tá»« logging_config
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
setup_logging(BASE_DIR, app_name="app", log_level=logging.DEBUG)
logger = logging.getLogger("app")

# Import cÃ¡c modules khÃ¡c
from modules.config.config import config_bp, init_app_and_config
from modules.scheduler.program import program_bp
from modules.query.query import query_bp
from blueprints.cutter_bp import cutter_bp
from blueprints.hand_detection_bp import hand_detection_bp
from blueprints.qr_detection_bp import qr_detection_bp
from blueprints.roi_bp import roi_bp
from modules.scheduler.program import scheduler  # Import BatchScheduler

# ğŸ†• NEW: Import cloud endpoints blueprint
from modules.sources.cloud_endpoints import cloud_bp

# Khá»Ÿi táº¡o Flask app vÃ  DB path tá»« config
app, DB_PATH, logger = init_app_and_config()
from flask_session import Session

# ğŸ”§ FIXED: OAuth-Compatible Session Configuration
import secrets
app.config.update(
    # OAuth Session Fix - CRITICAL for Google OAuth
    SECRET_KEY=os.environ.get('SECRET_KEY', secrets.token_urlsafe(32)),
    SESSION_COOKIE_NAME='vtrack_session',
    SESSION_COOKIE_HTTPONLY=True,
    SESSION_COOKIE_SECURE=False,  # Set to True in production with HTTPS
    SESSION_COOKIE_SAMESITE='Lax',  # CRITICAL for OAuth redirects
    PERMANENT_SESSION_LIFETIME=timedelta(hours=24),  # Longer for OAuth
    
    # Session storage
    SESSION_TYPE='filesystem',
    SESSION_FILE_DIR=os.path.join(BASE_DIR, 'flask_session'),
    
    # OAuth specific
    OAUTH_INSECURE_TRANSPORT=True,  # Only for development
)

# Initialize session
Session(app)

# ğŸ”§ CORS Configuration for OAuth
CORS(app, 
     origins=['http://localhost:3000', 'http://127.0.0.1:3000'],
     supports_credentials=True,
     allow_headers=['Content-Type', 'Authorization'],
     methods=['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'])

logger.info("âœ… CORS configured for OAuth with credentials")

# ğŸ”§ CRITICAL: Make sessions permanent for OAuth
@app.before_request
def make_session_permanent():
    from flask import session
    session.permanent = True

logger.info("ğŸ”‘ OAuth-compatible session configuration applied")

# ÄÄƒng kÃ½ cÃ¡c Blueprint
app.register_blueprint(program_bp)
app.register_blueprint(config_bp)
app.register_blueprint(query_bp)
app.register_blueprint(cutter_bp)
app.register_blueprint(hand_detection_bp)
app.register_blueprint(qr_detection_bp)
app.register_blueprint(roi_bp)

# ğŸ†• NEW: Register cloud endpoints blueprint with error handling
try:
    app.register_blueprint(cloud_bp, name='cloud_endpoints')
    logger.info("âœ… Cloud endpoints registered: /api/cloud/*")
except ValueError as e:
    logger.warning(f"âš ï¸ Cloud blueprint already registered: {e}")
    # If already registered, skip (could be from config.py)
    pass

# HÃ m ghi last_stop_time khi á»©ng dá»¥ng dá»«ng
def exit_handler():
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        last_stop_time = datetime.now(tz=timezone(timedelta(hours=7))).strftime('%Y-%m-%d %H:%M:%S')
        cursor.execute("""
            INSERT OR REPLACE INTO program_status (key, value)
            VALUES ('last_stop_time', ?)
        """, (last_stop_time,))
        conn.commit()
        conn.close()
        logger.info("Application stopped gracefully")
    except Exception as e:
        logger.error(f"Error saving last_stop_time: {e}")

# ÄÄƒng kÃ½ exit_handler
atexit.register(exit_handler)

def is_port_in_use(port):
    """Kiá»ƒm tra xem cá»•ng cÃ³ Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng hay khÃ´ng."""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        try:
            s.bind(("localhost", port))
            return False
        except OSError:
            return True

# Global flag Ä‘á»ƒ trÃ¡nh multiple shutdown
_shutdown_in_progress = False

def signal_handler(sig, frame):
    """Xá»­ lÃ½ tÃ­n hiá»‡u dá»«ng á»©ng dá»¥ng má»™t cÃ¡ch graceful"""
    global _shutdown_in_progress
    
    # TrÃ¡nh multiple shutdown
    if _shutdown_in_progress:
        print("\nForced shutdown...")
        os._exit(1)  # Force exit náº¿u Ä‘Ã£ shutdown rá»“i
    
    _shutdown_in_progress = True
    print("\nShutting down gracefully... (Press Ctrl+C again to force)")
    
    try:
        logger.info("Received shutdown signal, stopping application...")
        
        # Dá»«ng scheduler
        if 'scheduler' in globals():
            scheduler.stop()
            logger.info("Scheduler stopped")
        
        # Äá»£i cÃ¡c thread káº¿t thÃºc vá»›i timeout ngáº¯n hÆ¡n
        main_thread = threading.current_thread()
        for t in threading.enumerate():
            if t != main_thread and t.is_alive():
                try:
                    t.join(timeout=2)  # Giáº£m timeout xuá»‘ng 2 giÃ¢y
                    if t.is_alive():
                        logger.warning(f"Thread {t.name} did not stop gracefully")
                except:
                    pass  # Ignore errors during shutdown
        
        logger.info("Application shutdown complete")
        
    except Exception as e:
        logger.error(f"Error during shutdown: {e}")
    finally:
        os._exit(0)  # Force exit

# ÄÄƒng kÃ½ signal handler
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# ğŸ”§ Debug: List all registered endpoints on startup
def log_registered_routes():
    """Log all registered Flask routes for debugging"""
    logger.info("ğŸ“‹ Registered Flask Routes:")
    for rule in app.url_map.iter_rules():
        methods = ', '.join(rule.methods - {'HEAD', 'OPTIONS'})
        logger.info(f"   {methods:15} {rule.rule}")

# Khá»Ÿi cháº¡y á»©ng dá»¥ng
if __name__ == "__main__":
    port = 8080
    
    # Kiá»ƒm tra port trÆ°á»›c khi khá»Ÿi cháº¡y
    if is_port_in_use(port):
        logger.error(f"Port {port} is already in use!")
        sys.exit(1)
    
    logger.info(f"Starting VTrack application on port {port}")
    logger.info(f"ğŸ”‘ OAuth session security: âœ… Enabled")
    
    # ğŸ”§ Debug: Log registered routes
    log_registered_routes()
    
    try:
        app.run(
            host='0.0.0.0',
            port=port,
            debug=False,
            use_reloader=False,
            threaded=True
        )
    except KeyboardInterrupt:
        logger.info("Application interrupted by user")
    except Exception as e:
        logger.error(f"Application error: {e}")
    finally:
        logger.info("Application terminated")
```
## ğŸ“„ File: `test_onvif_download.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/test_onvif_download.py`

```python
#!/usr/bin/env python3
"""
Test NVRDownloader with actual ONVIF containers
Run this to verify download functionality
"""

import sys
import os
import json
from datetime import datetime, timedelta

# Add backend to path
sys.path.append('/Users/annhu/vtrack_app/V_Track/backend')

def test_onvif_download():
    """Test NVRDownloader with actual ONVIF containers"""
    print("ğŸ§ª Testing NVRDownloader with ONVIF containers...")
    
    try:
        # Import required modules
        from modules.sources.nvr_downloader import NVRDownloader
        from modules.db_utils import get_db_connection
        
        # Create downloader instance
        downloader = NVRDownloader()
        
        # Get real source config from database
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            SELECT id, config, path FROM video_sources 
            WHERE source_type = 'nvr' AND active = 1 
            LIMIT 1
        """)
        result = cursor.fetchone()
        
        if not result:
            print("âŒ No active NVR source found in database")
            return False
        
        source_id, config_json, path = result
        config = json.loads(config_json)
        
        print(f"âœ… Using source ID: {source_id}")
        print(f"âœ… Config: {json.dumps(config, indent=2)}")
        
        # Build source config for downloader
        source_config = {
            'id': source_id,
            'selected_cameras': config.get('selected_cameras', []),
            'url': config.get('url', 'localhost'),
            'port': config.get('port', 1000),
            'username': config.get('username', 'admin'),
            'password': config.get('password', 'admin')
        }
        
        print(f"âœ… Source config ready: {json.dumps(source_config, indent=2)}")
        
        # Test 1: Check ONVIF container connectivity
        print("\n=== TEST 1: ONVIF Container Connectivity ===")
        import socket
        
        # Test multiple ports (our docker-compose setup)
        test_ports = [1000, 1001, 1002]
        accessible_ports = []
        
        for port in test_ports:
            try:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.settimeout(3)
                result = sock.connect_ex(('localhost', port))
                sock.close()
                
                if result == 0:
                    accessible_ports.append(port)
                    print(f"âœ… Port {port} accessible")
                else:
                    print(f"âŒ Port {port} not accessible")
            except Exception as e:
                print(f"âŒ Port {port} error: {e}")
        
        if not accessible_ports:
            print("âŒ No ONVIF containers accessible. Please start containers first:")
            print("   cd /Users/annhu/vtrack_app/V_Track/service/onvif-multiple-cameras")
            print("   docker-compose up -d")
            return False
        
        print(f"âœ… {len(accessible_ports)} ONVIF containers accessible")
        
        # Test 2: Test ONVIF GetRecordings request
        print("\n=== TEST 2: ONVIF GetRecordings Request ===")
        
        # Use first accessible port
        test_port = accessible_ports[0]
        test_camera = source_config['selected_cameras'][0] if source_config['selected_cameras'] else 'Camera1'
        
        print(f"Testing with port {test_port}, camera: {test_camera}")
        
        # Build test config
        test_config = {
            'url': 'localhost',
            'port': test_port,
            'username': source_config['username'],
            'password': source_config['password']
        }
        
        # Define time range (last 24 hours)
        time_range = {
            'from': datetime.now() - timedelta(hours=24),
            'to': datetime.now()
        }
        
        print(f"Time range: {time_range['from']} to {time_range['to']}")
        
        # Test _get_onvif_recordings method
        try:
            recordings = downloader._get_onvif_recordings(test_config, test_camera, time_range)
            print(f"âœ… GetRecordings response: {len(recordings)} recordings found")
            
            if recordings:
                print("Sample recording:")
                print(json.dumps(recordings[0], indent=2, default=str))
            else:
                print("âš ï¸ No recordings found (expected for mock containers)")
                
        except Exception as e:
            print(f"âŒ GetRecordings failed: {e}")
            print("This is expected for mock containers that don't support recording API")
        
        # Test 3: Test download_latest_recordings method
        print("\n=== TEST 3: Download Latest Recordings ===")
        
        try:
            # Call download_latest_recordings
            result = downloader.download_latest_recordings(source_config, hours_back=24)
            
            print(f"Download result: {json.dumps(result, indent=2)}")
            
            if result['success']:
                print(f"âœ… Download successful: {result['files_downloaded']} files, {result['total_size_mb']:.2f} MB")
            else:
                print(f"âš ï¸ Download failed: {result['message']}")
                print("This is expected for mock containers without recording support")
                
        except Exception as e:
            print(f"âŒ Download method failed: {e}")
            import traceback
            traceback.print_exc()
        
        # Test 4: Check database entries
        print("\n=== TEST 4: Database Entries ===")
        
        try:
            cursor.execute("""
                SELECT COUNT(*) FROM downloaded_files 
                WHERE source_id = ?
            """, (source_id,))
            
            file_count = cursor.fetchone()[0]
            print(f"Downloaded files in database: {file_count}")
            
            if file_count > 0:
                cursor.execute("""
                    SELECT camera_name, local_file_path, file_size_bytes, download_timestamp 
                    FROM downloaded_files 
                    WHERE source_id = ? 
                    ORDER BY download_timestamp DESC 
                    LIMIT 3
                """, (source_id,))
                
                recent_files = cursor.fetchall()
                print("Recent downloads:")
                for row in recent_files:
                    print(f"  - {row[0]}: {row[1]} ({row[2]} bytes, {row[3]})")
        
        except Exception as e:
            print(f"âŒ Database check failed: {e}")
        
        conn.close()
        
        print("\nâœ… ONVIF download test completed")
        return True
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_containers():
    """Check if ONVIF containers are running"""
    print("ğŸ³ Checking ONVIF containers...")
    
    import subprocess
    
    try:
        # Check docker-compose status
        result = subprocess.run(
            ['docker', 'ps', '--filter', 'name=onvif', '--format', 'table {{.Names}}\t{{.Status}}\t{{.Ports}}'],
            capture_output=True,
            text=True,
            cwd='/Users/annhu/vtrack_app/V_Track/service/onvif-multiple-cameras'
        )
        
        if result.returncode == 0:
            print("Docker containers:")
            print(result.stdout)
            return 'onvif' in result.stdout
        else:
            print(f"âŒ Docker command failed: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ Container check failed: {e}")
        return False

if __name__ == "__main__":
    print("ğŸ§ª NVRDownloader ONVIF Container Test")
    print("=" * 60)
    
    # Check containers first
    if check_containers():
        print("âœ… ONVIF containers detected")
    else:
        print("âš ï¸ ONVIF containers not detected. Starting them...")
        print("Please run:")
        print("  cd /Users/annhu/vtrack_app/V_Track/service/onvif-multiple-cameras")
        print("  docker-compose up -d")
        print("  Then run this test again")
        sys.exit(1)
    
    print("\n" + "=" * 60)
    
    # Run download test
    success = test_onvif_download()
    
    print("\n" + "=" * 60)
    print(f"ğŸ¯ Overall result: {'âœ… PASSED' if success else 'âŒ FAILED'}")
    
    if success:
        print("\nğŸ‰ NVRDownloader is ready for integration!")
        print("Next steps:")
        print("1. Integrate with auto_sync_service.py")
        print("2. Add API endpoints in app.py")
        print("3. Add UI status display")
    else:
        print("\nğŸ”§ Issues found. Please fix before proceeding.")
```
## ğŸ“„ File: `test_google_credentials.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/test_google_credentials.py`

```python
#!/usr/bin/env python3
"""
Test Google Drive Credentials
Quick test to verify credentials file and OAuth setup
"""

import os
import json
from pathlib import Path

def test_credentials():
    """Test Google Drive credentials file"""
    
    print("ğŸ” Testing Google Drive credentials...")
    
    # Check credentials file
    creds_path = "modules/sources/credentials/google_drive_credentials.json"
    
    if not os.path.exists(creds_path):
        print(f"âŒ Credentials file not found: {creds_path}")
        
        # Check alternative locations
        alt_paths = [
            "modules/sources/google_drive_credentials.json",
            "google_drive_credentials.json"
        ]
        
        for alt_path in alt_paths:
            if os.path.exists(alt_path):
                print(f"âœ… Found credentials at: {alt_path}")
                creds_path = alt_path
                break
        else:
            print("âŒ No credentials file found anywhere")
            return False
    else:
        print(f"âœ… Credentials file found: {creds_path}")
    
    # Test credentials content
    try:
        with open(creds_path, 'r') as f:
            creds = json.load(f)
        
        if 'installed' in creds:
            installed = creds['installed']
            
            # Check required fields
            required_fields = ['client_id', 'client_secret', 'auth_uri', 'token_uri']
            missing_fields = []
            
            for field in required_fields:
                if field not in installed:
                    missing_fields.append(field)
                else:
                    # Show partial values for verification
                    value = installed[field]
                    if field == 'client_secret':
                        display_value = value[:8] + "..." if len(value) > 8 else "***"
                    elif field == 'client_id':
                        display_value = value[:20] + "..." if len(value) > 20 else value
                    else:
                        display_value = value
                    
                    print(f"âœ… {field}: {display_value}")
            
            if missing_fields:
                print(f"âŒ Missing required fields: {missing_fields}")
                return False
            
            # Check redirect URIs
            if 'redirect_uris' in installed:
                redirect_uris = installed['redirect_uris']
                print(f"ğŸ“ Redirect URIs: {redirect_uris}")
                
                # Check if localhost is included
                localhost_found = any('localhost' in uri for uri in redirect_uris)
                if not localhost_found:
                    print("âš ï¸ No localhost redirect URI found - this might cause issues")
            
            print("âœ… Credentials file structure is valid")
            return True
            
        else:
            print("âŒ Invalid credentials format - missing 'installed' key")
            return False
            
    except json.JSONDecodeError as e:
        print(f"âŒ JSON decode error: {e}")
        return False
    except Exception as e:
        print(f"âŒ Error reading credentials: {e}")
        return False

def test_oauth_flow():
    """Test OAuth flow initialization"""
    
    print("\nğŸ”§ Testing OAuth flow...")
    
    try:
        from modules.sources.cloud_auth import CloudAuthManager
        
        # Initialize auth manager
        auth_manager = CloudAuthManager('google_drive')
        print("âœ… CloudAuthManager initialized")
        
        # Test OAuth flow initiation (without actual redirect)
        redirect_uri = 'http://localhost:8080/api/cloud/oauth/callback'
        
        # This will fail with real OAuth but we can catch the specific error
        oauth_result = auth_manager.initiate_oauth_flow(redirect_uri)
        
        if oauth_result['success']:
            print("âœ… OAuth flow initialized successfully")
            print(f"ğŸ“ Auth URL: {oauth_result['auth_url'][:50]}...")
            return True
        else:
            print(f"âŒ OAuth flow failed: {oauth_result['message']}")
            
            # Common OAuth errors
            error_msg = oauth_result['message'].lower()
            if 'not found' in error_msg or 'file' in error_msg:
                print("ğŸ’¡ Solution: Check credentials file location")
            elif 'redirect' in error_msg:
                print("ğŸ’¡ Solution: Update redirect URIs in Google Cloud Console")
            elif 'client' in error_msg:
                print("ğŸ’¡ Solution: Verify client_id and client_secret")
            
            return False
            
    except ImportError as e:
        print(f"âŒ Import error: {e}")
        return False
    except Exception as e:
        print(f"âŒ OAuth test error: {e}")
        return False

def test_api_endpoint():
    """Test API endpoint directly"""
    
    print("\nğŸŒ Testing API endpoint...")
    
    try:
        import requests
        
        url = "http://localhost:8080/api/cloud/authenticate"
        data = {
            "provider": "google_drive",
            "action": "initiate_auth"
        }
        
        response = requests.post(url, json=data, timeout=5)
        
        print(f"ğŸ“¡ API Response: {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… API endpoint working")
            if 'auth_url' in result:
                print("âœ… OAuth URL generated successfully")
            return True
        else:
            print(f"âŒ API error: {response.status_code}")
            try:
                error_data = response.json()
                print(f"Error details: {error_data}")
            except:
                print(f"Raw response: {response.text}")
            return False
            
    except requests.exceptions.ConnectionError:
        print("âŒ Cannot connect to backend - is it running on port 8080?")
        return False
    except ImportError:
        print("âš ï¸ Requests library not available - skipping API test")
        return True
    except Exception as e:
        print(f"âŒ API test error: {e}")
        return False

def main():
    """Main test function"""
    
    print("=" * 60)
    print("ğŸ”§ Google Drive Integration Test")
    print("=" * 60)
    
    tests_passed = 0
    total_tests = 3
    
    # Test 1: Credentials file
    if test_credentials():
        tests_passed += 1
    
    # Test 2: OAuth flow
    if test_oauth_flow():
        tests_passed += 1
    
    # Test 3: API endpoint
    if test_api_endpoint():
        tests_passed += 1
    
    # Summary
    print("\n" + "=" * 60)
    print(f"ğŸ“Š TEST SUMMARY: {tests_passed}/{total_tests} passed")
    print("=" * 60)
    
    if tests_passed == total_tests:
        print("ğŸ‰ All tests passed! Google Drive integration should work.")
    elif tests_passed >= 1:
        print("âš ï¸ Some tests failed. Check the issues above.")
    else:
        print("âŒ All tests failed. Google Drive integration needs setup.")
    
    # Recommendations
    print("\nğŸ’¡ NEXT STEPS:")
    
    if tests_passed == 0:
        print("1. âœ… Download Google Cloud Console credentials")
        print("2. âœ… Place in modules/sources/credentials/google_drive_credentials.json")
        print("3. âœ… Restart backend server")
    elif tests_passed < total_tests:
        print("1. âœ… Check backend logs for detailed errors")
        print("2. âœ… Verify Google Cloud Console OAuth setup")
        print("3. âœ… Ensure redirect URIs include localhost")
    else:
        print("1. ğŸ¯ Try authentication again in browser")
        print("2. ğŸ¯ Should see Google OAuth popup")

if __name__ == "__main__":
    main()
```
## ğŸ“„ File: `test_nvr_downloader.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/test_nvr_downloader.py`

```python
#!/usr/bin/env python3
"""
Test script for nvr_downloader.py
Run this to verify NVRDownloader functionality
"""

import sys
import os
import json
from datetime import datetime, timedelta

# Add backend to path
sys.path.append('/Users/annhu/vtrack_app/V_Track/backend')

def test_nvr_downloader():
    """Test NVRDownloader with mock data"""
    print("ğŸ§ª Testing NVRDownloader...")
    
    try:
        # Import NVRDownloader
        from modules.sources.nvr_downloader import NVRDownloader
        print("âœ… Successfully imported NVRDownloader")
        
        # Create instance
        downloader = NVRDownloader()
        print("âœ… NVRDownloader instance created")
        
        # Test 1: Basic initialization
        print("\n=== TEST 1: Basic Initialization ===")
        print(f"Download progress storage: {type(downloader.download_progress)}")
        print(f"Download progress empty: {len(downloader.download_progress) == 0}")
        
        # Test 2: Mock source config
        print("\n=== TEST 2: Mock Source Config ===")
        mock_source_config = {
            'id': 36,  # From database
            'selected_cameras': ['Front Door Camera', 'Parking Lot Camera'],
            'url': 'localhost',
            'port': 1000,
            'username': 'admin',
            'password': 'admin',
            'path': '/Users/annhu/vtrack_app/V_Track/nvr_downloads/nvr_localhost'
        }
        
        print(f"Mock config: {json.dumps(mock_source_config, indent=2)}")
        
        # Test 3: Test path validation integration
        print("\n=== TEST 3: Path Validation Integration ===")
        try:
            from modules.utils.path_validator import path_validator
            print("âœ… PathValidator imported successfully")
            
            # Test get_camera_paths
            camera_paths = path_validator.get_camera_paths(
                mock_source_config['path'], 
                mock_source_config['selected_cameras']
            )
            print(f"Camera paths result: {camera_paths}")
            
        except Exception as e:
            print(f"âŒ PathValidator error: {e}")
        
        # Test 4: Test database connection
        print("\n=== TEST 4: Database Connection ===")
        try:
            from modules.db_utils import get_db_connection
            conn = get_db_connection()
            cursor = conn.cursor()
            
            # Test sync_status table
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='sync_status'")
            if cursor.fetchone():
                print("âœ… sync_status table exists")
            else:
                print("âŒ sync_status table missing")
            
            # Test downloaded_files table
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='downloaded_files'")
            if cursor.fetchone():
                print("âœ… downloaded_files table exists")
            else:
                print("âŒ downloaded_files table missing")
            
            conn.close()
            
        except Exception as e:
            print(f"âŒ Database error: {e}")
        
        # Test 5: Test SOAP envelope generation
        print("\n=== TEST 5: SOAP Envelope Generation ===")
        try:
            test_body = "<TestRequest>Hello</TestRequest>"
            soap_envelope = downloader._wrap_soap_envelope(test_body)
            print("âœ… SOAP envelope generated:")
            print(soap_envelope[:200] + "..." if len(soap_envelope) > 200 else soap_envelope)
            
        except Exception as e:
            print(f"âŒ SOAP envelope error: {e}")
        
        # Test 6: Test with real source config (dry run)
        print("\n=== TEST 6: Real Source Config (Dry Run) ===")
        try:
            # Get actual source from database
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                SELECT id, config, path FROM video_sources 
                WHERE source_type = 'nvr' AND active = 1 
                LIMIT 1
            """)
            result = cursor.fetchone()
            
            if result:
                source_id, config_json, path = result
                config = json.loads(config_json)
                
                print(f"Real source found: ID={source_id}")
                print(f"Config: {json.dumps(config, indent=2)}")
                print(f"Path: {path}")
                
                # Build real config for downloader
                real_config = {
                    'id': source_id,
                    'selected_cameras': config.get('selected_cameras', []),
                    'url': config.get('url', 'localhost'),
                    'port': config.get('port', 1000),
                    'username': config.get('username', 'admin'),
                    'password': config.get('password', 'admin'),
                    'path': f'/Users/annhu/vtrack_app/V_Track/nvr_downloads/nvr_{config.get("url", "localhost")}'
                }
                
                print(f"Real config for downloader: {json.dumps(real_config, indent=2)}")
                
                # Test download_latest_recordings (without actual HTTP calls)
                print("\nğŸ“¥ Testing download_latest_recordings (mock)...")
                
                # This would actually call ONVIF - let's skip for now
                print("âš ï¸ Skipping actual HTTP calls to avoid container dependency")
                
            else:
                print("âŒ No NVR source found in database")
            
            conn.close()
            
        except Exception as e:
            print(f"âŒ Real config test error: {e}")
        
        print("\nâœ… NVRDownloader basic tests completed")
        return True
        
    except ImportError as e:
        print(f"âŒ Import error: {e}")
        return False
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_dependencies():
    """Test required dependencies"""
    print("ğŸ” Testing dependencies...")
    
    required_modules = [
        'requests',
        'logging', 
        'os',
        'datetime',
        'xml.etree.ElementTree',
        'typing'
    ]
    
    for module in required_modules:
        try:
            __import__(module)
            print(f"âœ… {module}")
        except ImportError:
            print(f"âŒ {module} - MISSING")
    
    # Test custom modules
    custom_modules = [
        'modules.db_utils',
        'modules.utils.path_validator'
    ]
    
    for module in custom_modules:
        try:
            __import__(module)
            print(f"âœ… {module}")
        except ImportError as e:
            print(f"âŒ {module} - MISSING: {e}")

if __name__ == "__main__":
    print("ğŸ§ª NVRDownloader Test Suite")
    print("=" * 50)
    
    # Test 1: Dependencies
    test_dependencies()
    
    print("\n" + "=" * 50)
    
    # Test 2: NVRDownloader functionality
    success = test_nvr_downloader()
    
    print("\n" + "=" * 50)
    print(f"ğŸ¯ Overall result: {'âœ… PASSED' if success else 'âŒ FAILED'}")
```
## ğŸ“„ File: `cutter_bp.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/blueprints/cutter_bp.py`

```python
import sqlite3
import os
from datetime import datetime
import subprocess
from flask import Blueprint, request, jsonify
from modules.db_utils import get_db_connection
import ast
from modules.technician.cutter.cutter_complete import cut_complete_event
from modules.technician.cutter.cutter_incomplete import cut_incomplete_event, merge_incomplete_events
from modules.technician.cutter.cutter_utils import generate_output_filename, update_event_in_db
from modules.scheduler.db_sync import db_rwlock

cutter_bp = Blueprint('cutter', __name__)

with db_rwlock.gen_rlock():
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT output_path FROM processing_config LIMIT 1")
    result = cursor.fetchone()
    output_dir = result[0] if result else os.path.join(os.path.dirname(os.path.abspath(__file__)), "../../resources/output_clips")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    conn.close()

def get_video_duration(video_file):
    try:
        cmd = ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", video_file]
        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return float(result.stdout.strip())
    except Exception:
        return None

def cut_and_update_events(selected_events, tracking_codes_filter, brand_name="Alan"):
    with db_rwlock.gen_wlock():
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT video_buffer, max_packing_time FROM processing_config LIMIT 1")
        result = cursor.fetchone()
        video_buffer = result[0] if result else 5
        max_packing_time = result[1] if result else 120
        cut_files = []

        for event in selected_events:
            event_id = event.get("event_id")
            ts = event.get("ts")
            te = event.get("te")
            video_file = event.get("video_file")
            cursor.execute("SELECT is_processed FROM events WHERE event_id = ?", (event_id,))
            is_processed = cursor.fetchone()[0]
            if is_processed:
                print(f"Bá» qua: Sá»± kiá»‡n {event_id} Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ trÆ°á»›c Ä‘Ã³")
                continue

            has_ts = ts is not None
            has_te = te is not None
            is_incomplete = (has_ts and not has_te) or (not has_ts and has_te)

            if is_incomplete:
                next_event = None
                if has_ts and not has_te:
                    cursor.execute("SELECT event_id, ts, te, video_file, packing_time_start, packing_time_end, tracking_codes, is_processed FROM events WHERE event_id = ? AND is_processed = 0", (event_id + 1,))
                    next_event_row = cursor.fetchone()
                    if next_event_row:
                        next_event = {
                            "event_id": next_event_row[0],
                            "ts": next_event_row[1],
                            "te": next_event_row[2],
                            "video_file": next_event_row[3],
                            "packing_time_start": next_event_row[4],
                            "packing_time_end": next_event_row[5],
                            "tracking_codes": next_event_row[6],
                            "is_processed": next_event_row[7]
                        }
                elif not has_ts and has_te:
                    cursor.execute("SELECT event_id, ts, te, video_file, packing_time_start, packing_time_end, tracking_codes, is_processed FROM events WHERE event_id = ? AND is_processed = 0", (event_id - 1,))
                    prev_event_row = cursor.fetchone()
                    if prev_event_row:
                        next_event = {
                            "event_id": prev_event_row[0],
                            "ts": prev_event_row[1],
                            "te": prev_event_row[2],
                            "video_file": prev_event_row[3],
                            "packing_time_start": prev_event_row[4],
                            "packing_time_end": prev_event_row[5],
                            "tracking_codes": prev_event_row[6],
                            "is_processed": prev_event_row[7]
                        }

                if next_event:
                    next_event_id = next_event.get("event_id")
                    next_ts = next_event.get("ts")
                    next_te = next_event.get("te")
                    next_video_file = next_event.get("video_file")
                    next_has_ts = next_ts is not None
                    next_has_te = next_te is not None
                    can_merge = (has_ts and not has_te and not next_has_ts and next_has_te) or (not has_ts and has_te and next_has_ts and not next_has_te)

                    if can_merge:
                        video_length_a = get_video_duration(video_file)
                        video_length_b = get_video_duration(next_video_file)
                        if video_length_a is None or video_length_b is None:
                            print(f"Lá»—i: KhÃ´ng thá»ƒ láº¥y Ä‘á»™ dÃ i video {video_file} hoáº·c {next_video_file}")
                            continue

                        output_file_a = generate_output_filename(event, tracking_codes_filter, output_dir, brand_name)
                        print(f"Äang xá»­ lÃ½ sá»± kiá»‡n {event_id}: output_file={output_file_a}, packing_time_start={event.get('packing_time_start')}, packing_time_end={event.get('packing_time_end')}")
                        if cut_incomplete_event(event, video_buffer, video_length_a, output_file_a):
                            update_event_in_db(cursor, event_id, output_file_a)

                        output_file_b = generate_output_filename(next_event, tracking_codes_filter, output_dir, brand_name)
                        print(f"Äang xá»­ lÃ½ sá»± kiá»‡n {next_event_id}: output_file={output_file_b}, packing_time_start={next_event.get('packing_time_start')}, packing_time_end={next_event.get('packing_time_end')}")
                        if cut_incomplete_event(next_event, video_buffer, video_length_b, output_file_b):
                            update_event_in_db(cursor, next_event_id, output_file_b)

                        if has_ts and not has_te:
                            event_a = event
                            event_b = next_event
                            video_length_event_a = video_length_a
                            video_length_event_b = video_length_b
                        else:
                            event_a = next_event
                            event_b = event
                            video_length_event_a = video_length_b
                            video_length_event_b = video_length_a

                        print(f"Gá»i merge_incomplete_events: event_a (ID: {event_a.get('event_id')}, ts: {event_a.get('ts')}, te: {event_a.get('te')}), event_b (ID: {event_b.get('event_id')}, ts: {event_b.get('ts')}, te: {event_b.get('te')})")

                        merged_file = merge_incomplete_events(event_a, event_b, video_buffer, video_length_event_a, video_length_event_b, output_dir, max_packing_time, brand_name)
                        if merged_file:
                            update_event_in_db(cursor, event_id, merged_file)
                            update_event_in_db(cursor, next_event_id, merged_file)
                            cut_files.append(merged_file)
                        continue

            video_length = get_video_duration(video_file)
            if video_length is None:
                print(f"Lá»—i: KhÃ´ng thá»ƒ láº¥y Ä‘á»™ dÃ i video {video_file}")
                continue

            output_file = generate_output_filename(event, tracking_codes_filter, output_dir, brand_name)
            print(f"Äang xá»­ lÃ½ sá»± kiá»‡n {event_id}: output_file={output_file}, packing_time_start={event.get('packing_time_start')}, packing_time_end={event.get('packing_time_end')}")

            if has_ts and has_te:
                if cut_complete_event(event, video_buffer, video_length, output_file):
                    update_event_in_db(cursor, event_id, output_file)
                    cut_files.append(output_file)
            elif is_incomplete:
                if cut_incomplete_event(event, video_buffer, video_length, output_file):
                    update_event_in_db(cursor, event_id, output_file)
                    cut_files.append(output_file)
            else:
                print(f"Bá» qua: Sá»± kiá»‡n {event_id} khÃ´ng cÃ³ ts hoáº·c te")
                continue

        conn.commit()
        conn.close()
    return cut_files

@cutter_bp.route('/cut-videos', methods=['POST'])
def cut_videos():
    data = request.get_json()
    selected_events = data.get('selected_events', [])
    tracking_codes_filter = data.get('tracking_codes_filter', [])

    if not selected_events:
        return jsonify({"error": "No selected events provided"}), 400

    try:
        cut_files = cut_and_update_events(selected_events, tracking_codes_filter)
        return jsonify({"message": "Videos cut successfully", "cut_files": cut_files}), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500
```
## ğŸ“„ File: `__init__.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/blueprints/__init__.py`

```python

```
## ğŸ“„ File: `hand_detection_bp.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/blueprints/hand_detection_bp.py`

```python
from flask import Blueprint, request, jsonify
from modules.technician.hand_detection import select_roi
import subprocess
import os
import json
import logging

hand_detection_bp = Blueprint('hand_detection', __name__)

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

@hand_detection_bp.route('/select-roi', methods=['POST'])
def select_roi_endpoint():
    try:
        data = request.get_json()
        video_path = data.get('videoPath')
        camera_id = data.get('cameraId')
        step = data.get('step', 'packing')
        
        logging.debug(f"Received request for select-roi with step: {step}, video_path: {video_path}, camera_id: {camera_id}")
        
        if not video_path or not camera_id:
            logging.error("Missing videoPath or cameraId")
            return jsonify({"success": False, "error": "Thiáº¿u videoPath hoáº·c cameraId."}), 400
        
        if not os.path.exists(video_path):
            logging.error(f"Video path does not exist: {video_path}")
            return jsonify({"success": False, "error": "ÄÆ°á»ng dáº«n video khÃ´ng tá»“n táº¡i."}), 404
        
        if os.path.exists("/tmp/roi.json"):
            os.remove("/tmp/roi.json")
            logging.info("ÄÃ£ xÃ³a file /tmp/roi.json Ä‘á»ƒ báº¯t Ä‘áº§u quy trÃ¬nh má»›i")
        
        result = select_roi(video_path, camera_id, step)
        logging.debug(f"Result from select_roi: {result}")
        return jsonify(result), 200 if result["success"] else 400
    
    except Exception as e:
        logging.error(f"Exception in select_roi_endpoint: {str(e)}")
        return jsonify({"success": False, "error": f"Lá»—i há»‡ thá»‘ng: {str(e)}"}), 500

@hand_detection_bp.route('/run-select-roi', methods=['POST'])
def run_select_roi_endpoint():
    try:
        data = request.get_json()
        video_path = data.get('videoPath')
        camera_id = data.get('cameraId', 'default_camera')
        
        if not video_path:
            return jsonify({"success": False, "error": "Thiáº¿u videoPath."}), 400
        
        logging.info(f"Running hand_detection.py with video_path: {video_path}, camera_id: {camera_id}")
        
        if not os.path.exists(video_path):
            return jsonify({"success": False, "error": "ÄÆ°á»ng dáº«n video khÃ´ng tá»“n táº¡i."}), 404
        
        if os.path.exists("/tmp/roi.json"):
            os.remove("/tmp/roi.json")
        
        result = subprocess.run(
            ["python3", "modules/technician/hand_detection.py", video_path, camera_id],
            capture_output=True,
            text=True,
            timeout=300
        )
        
        if result.returncode != 0:
            error_message = f"Lá»—i khi cháº¡y script: {result.stderr}"
            logging.error(error_message)
            return jsonify({"success": False, "error": error_message}), 500
        
        if not os.path.exists("/tmp/roi.json"):
            return jsonify({"success": False, "error": "KhÃ´ng tÃ¬m tháº¥y file káº¿t quáº£ ROI."}), 500
        
        with open("/tmp/roi.json", "r") as f:
            roi_result = json.load(f)
        
        logging.info(f"ROI result from /tmp/roi.json: {roi_result}")
        return jsonify(roi_result), 200 if roi_result["success"] else 400
    
    except subprocess.TimeoutExpired:
        return jsonify({"success": False, "error": "Háº¿t thá»i gian chá» khi cháº¡y script."}), 500
    except Exception as e:
        logging.error(f"Unexpected error in run-select-roi: {str(e)}")
        return jsonify({"success": False, "error": f"Lá»—i há»‡ thá»‘ng: {str(e)}"}), 500

```
## ğŸ“„ File: `roi_bp.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/blueprints/roi_bp.py`

```python
from flask import Blueprint, request, jsonify, send_file
from modules.technician.hand_detection import finalize_roi
import os
import glob
import json
import sqlite3
from datetime import datetime
import logging

roi_bp = Blueprint('roi', __name__)

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(BASE_DIR, "..", "database", "events.db")
CAMERA_ROI_DIR = os.path.join(BASE_DIR, "..", "..", "resources/output_clips/CameraROI")

@roi_bp.route('/finalize-roi', methods=['POST'])
def finalize_roi_endpoint():
    try:
        data = request.get_json()
        video_path = data.get('videoPath')
        camera_id = data.get('cameraId')
        rois = data.get('rois')

        if not video_path or not camera_id or not rois:
            return jsonify({"success": False, "error": "Thiáº¿u videoPath, cameraId hoáº·c rois."}), 400

        if not os.path.exists(video_path):
            return jsonify({"success": False, "error": "ÄÆ°á»ng dáº«n video khÃ´ng tá»“n táº¡i."}), 404

        packing_roi = [0, 0, 0, 0]
        if os.path.exists("/tmp/roi.json"):
            with open("/tmp/roi.json", "r") as f:
                roi_data = json.load(f)
                if roi_data.get("success") and "roi" in roi_data:
                    packing_roi = [roi_data["roi"]["x"], roi_data["roi"]["y"], roi_data["roi"]["w"], roi_data["roi"]["h"]]
        
        qr_mvd_area = [0, 0, 0, 0]
        qr_trigger_area = [0, 0, 0, 0]
        table_type = None
        if os.path.exists("/tmp/qr_roi.json"):
            with open("/tmp/qr_roi.json", "r") as f:
                qr_roi_data = json.load(f)
                table_type = qr_roi_data.get("table_type")
                for roi in rois:
                    if roi["type"] == "mvd":
                        qr_mvd_area = [roi["x"], roi["y"], roi["w"], roi["h"]]
                    elif roi["type"] == "trigger" and table_type == "standard":
                        qr_trigger_area = [roi["x"], roi["y"], roi["w"], roi["h"]]

        profile_name = camera_id
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("SELECT 1 FROM packing_profiles WHERE profile_name = ?", (profile_name,))
        exists = cursor.fetchone()
        
        if exists:
            cursor.execute('''
                UPDATE packing_profiles
                SET qr_trigger_area = ?, qr_mvd_area = ?, packing_area = ?,
                    min_packing_time = ?, jump_time_ratio = ?, scan_mode = ?, fixed_threshold = ?, margin = ?, additional_params = ?
                WHERE profile_name = ?
            ''', (
                json.dumps(qr_trigger_area),
                json.dumps(qr_mvd_area),
                json.dumps(packing_roi),
                10, 0.5, "full", 20, 60, json.dumps({}),
                profile_name
            ))
        else:
            cursor.execute('''
                INSERT INTO packing_profiles (
                    profile_name, qr_trigger_area, qr_mvd_area, packing_area,
                    min_packing_time, jump_time_ratio, scan_mode, fixed_threshold, margin, additional_params
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                profile_name,
                json.dumps(qr_trigger_area),
                json.dumps(qr_mvd_area),
                json.dumps(packing_roi),
                10, 0.5, "full", 20, 60, json.dumps({})
            ))
        conn.commit()
        conn.close()
        logging.info(f"LÆ°u ROI vÃ o packing_profiles vá»›i profile_name: {profile_name}")

        result = finalize_roi(video_path, camera_id, rois)
        return jsonify(result), 200 if result["success"] else 400
    
    except Exception as e:
        logging.error(f"Exception in finalize_roi_endpoint: {str(e)}")
        return jsonify({"success": False, "error": f"Lá»—i há»‡ thá»‘ng: {str(e)}"}), 500

@roi_bp.route('/get-roi-frame', methods=['GET'])
def get_roi_frame():
    camera_id = request.args.get('camera_id')
    file = request.args.get('file')
    if not camera_id or not file:
        return jsonify({"success": False, "error": "Thiáº¿u camera_id hoáº·c file."}), 400

    file_path = os.path.join(CAMERA_ROI_DIR, f"camera_{camera_id}_{file}")
    logging.info(f"Attempting to fetch file: {file_path}")

    if not os.path.exists(file_path):
        logging.error(f"File khÃ´ng tá»“n táº¡i: {file_path}")
        return jsonify({"success": False, "error": "KhÃ´ng tÃ¬m tháº¥y áº£nh."}), 404

    return send_file(file_path, mimetype='image/jpeg')

@roi_bp.route('/get-final-roi-frame', methods=['GET'])
def get_final_roi_frame():
    camera_id = request.args.get('camera_id')
    if not camera_id:
        return jsonify({"success": False, "error": "Thiáº¿u camera_id."}), 400
    
    final_pattern = os.path.join(CAMERA_ROI_DIR, f"camera_{camera_id}_roi_final_*.jpg")
    final_files = glob.glob(final_pattern)
    logging.info(f"Files found for camera_id={camera_id}: {final_files}")

    if not final_files:
        logging.error(f"KhÃ´ng tÃ¬m tháº¥y file tá»•ng há»£p nÃ o trong {CAMERA_ROI_DIR} vá»›i pattern {final_pattern}")
        return jsonify({"success": False, "error": "KhÃ´ng tÃ¬m tháº¥y áº£nh tá»•ng há»£p."}), 404

    latest_file = max(final_files, key=os.path.getmtime)
    logging.info(f"Latest file selected: {latest_file}")

    if not os.path.exists(latest_file):
        logging.error(f"File má»›i nháº¥t {latest_file} khÃ´ng tá»“n táº¡i")
        return jsonify({"success": False, "error": "KhÃ´ng tÃ¬m tháº¥y áº£nh tá»•ng há»£p."}), 404

    return send_file(latest_file, mimetype='image/jpeg')
```
## ğŸ“„ File: `qr_detection_bp.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/blueprints/qr_detection_bp.py`

```python
from flask import Blueprint, request, jsonify
from modules.technician.qr_detector import select_qr_roi
import subprocess
import os
import json
import logging

qr_detection_bp = Blueprint('qr_detection', __name__)

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

CAMERA_ROI_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "..", "resources/output_clips/CameraROI")

@qr_detection_bp.route('/select-qr-roi', methods=['POST'])
def select_qr_roi_endpoint():
    try:
        data = request.get_json()
        video_path = data.get('videoPath')
        camera_id = data.get('cameraId')
        roi_frame_path = data.get('roiFramePath')
        step = data.get('step', 'mvd')
        
        logging.debug(f"[MVD] Received data: {data}")
        logging.debug(f"[MVD] Parameters - video_path: {video_path}, camera_id: {camera_id}, roi_frame_path: {roi_frame_path}, step: {step}")
        
        if not video_path or not camera_id or not roi_frame_path:
            logging.error("[MVD] Missing required parameters")
            return jsonify({"success": False, "error": "Thiáº¿u videoPath, cameraId hoáº·c roiFramePath."}), 400
        
        logging.debug(f"[MVD] Checking video path exists: {video_path}")
        if not os.path.exists(video_path):
            logging.error(f"[MVD] Video path does not exist: {video_path}")
            return jsonify({"success": False, "error": "ÄÆ°á»ng dáº«n video khÃ´ng tá»“n táº¡i."}), 404
        
        logging.debug(f"[MVD] Checking ROI frame path exists: {roi_frame_path}")
        if not os.path.exists(roi_frame_path):
            logging.error(f"[MVD] ROI frame path does not exist: {roi_frame_path}")
            return jsonify({"success": False, "error": "ÄÆ°á»ng dáº«n áº£nh táº¡m khÃ´ng tá»“n táº¡i."}), 404
        
        if os.path.exists("/tmp/qr_roi.json"):
            logging.debug("[MVD] Removing existing /tmp/qr_roi.json")
            os.remove("/tmp/qr_roi.json")
            logging.info("[MVD] ÄÃ£ xÃ³a file /tmp/qr_roi.json Ä‘á»ƒ báº¯t Ä‘áº§u quy trÃ¬nh má»›i")
        
        logging.debug(f"[MVD] Calling select_qr_roi with video_path: {video_path}, camera_id: {camera_id}, roi_frame_path: {roi_frame_path}, step: {step}")
        result = select_qr_roi(video_path, camera_id, roi_frame_path, step)
        logging.debug(f"[MVD] select_qr_roi result: {result}")
        logging.info(f"[MVD] Completed MVD step for camera_id: {camera_id}")
        
        return jsonify(result), 200 if result["success"] else 400
    
    except Exception as e:
        logging.error(f"[MVD] Exception in select_qr_roi_endpoint: {str(e)}")
        return jsonify({"success": False, "error": f"Lá»—i há»‡ thá»‘ng: {str(e)}"}), 500

@qr_detection_bp.route('/run-qr-detector', methods=['POST'])
def run_qr_detector_endpoint():
    try:
        data = request.get_json()
        video_path = data.get('videoPath')
        camera_id = data.get('cameraId', 'default_camera')
        
        if not video_path:
            return jsonify({"success": False, "error": "Thiáº¿u videoPath."}), 400
        
        absolute_roi_frame_path = os.path.join(CAMERA_ROI_DIR, f"camera_{camera_id}_roi_packing.jpg")
        
        logging.info(f"Running qr_detector.py with video_path: {video_path}, camera_id: {camera_id}, roi_frame_path: {absolute_roi_frame_path}")
        
        if not os.path.exists(video_path):
            return jsonify({"success": False, "error": "ÄÆ°á»ng dáº«n video khÃ´ng tá»“n táº¡i."}), 404
        if not os.path.exists(absolute_roi_frame_path):
            return jsonify({"success": False, "error": "ÄÆ°á»ng dáº«n áº£nh packing khÃ´ng tá»“n táº¡i."}), 404
        
        if os.path.exists("/tmp/qr_roi.json"):
            os.remove("/tmp/qr_roi.json")
        
        result = subprocess.run(
            ["python3", "modules/technician/qr_detector.py", video_path, camera_id, absolute_roi_frame_path],
            capture_output=True,
            text=True,
            timeout=300
        )
        
        if result.returncode != 0:
            error_message = f"Lá»—i khi cháº¡y script: {result.stderr}"
            logging.error(error_message)
            return jsonify({"success": False, "error": error_message}), 500
        
        if not os.path.exists("/tmp/qr_roi.json"):
            return jsonify({"success": False, "error": "KhÃ´ng tÃ¬m tháº¥y file káº¿t quáº£ ROI."}), 500
        
        with open("/tmp/qr_roi.json", "r") as f:
            qr_roi_result = json.load(f)
        
        logging.info(f"QR ROI result from /tmp/qr_roi.json: {qr_roi_result}")
        return jsonify(qr_roi_result), 200 if qr_roi_result["success"] else 400
    
    except subprocess.TimeoutExpired:
        return jsonify({"success": False, "error": "Háº¿t thá»i gian chá» khi cháº¡y script."}), 500
    except Exception as e:
        logging.error(f"Unexpected error in run-qr-detector: {str(e)}")
        return jsonify({"success": False, "error": f"Lá»—i há»‡ thá»‘ng: {str(e)}"}), 500
```
## ğŸ“„ File: `config_loader.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/config_loader.py`

```python
import sqlite3
import os
from modules.path_utils import get_paths


def get_processing_config():
    """
    Tráº£ vá» INPUT_VIDEO_DIR vÃ  LOG_DIR tá»« báº£ng processing_config.
    Náº¿u thiáº¿u, fallback vá» path máº·c Ä‘á»‹nh.
    """
    paths = get_paths()
    db_path = paths["DB_PATH"]

    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT input_path, output_path FROM processing_config LIMIT 1")
        row = cursor.fetchone()
        conn.close()

        if row:
            return {
                "INPUT_VIDEO_DIR": row[0],
                "LOG_DIR": row[1],
            }
    except Exception as e:
        print(f"[!] Lá»—i Ä‘á»c cáº¥u hÃ¬nh DB: {e}")

    # fallback náº¿u lá»—i hoáº·c DB trá»‘ng
    return {
        "INPUT_VIDEO_DIR": os.path.join(paths["BASE_DIR"], "resources/Inputvideo"),
        "LOG_DIR": os.path.join(paths["BASE_DIR"], "resources/output_clips/LOG"),
    }


def get_log_path(file_name: str) -> str:
    """Tráº£ vá» Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§ Ä‘áº¿n file trong LOG_DIR."""
    log_dir = get_processing_config()["LOG_DIR"]
    return os.path.join(log_dir, file_name)


def get_input_path(file_name: str) -> str:
    """Tráº£ vá» Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§ Ä‘áº¿n file trong INPUT_VIDEO_DIR."""
    input_dir = get_processing_config()["INPUT_VIDEO_DIR"]
    return os.path.join(input_dir, file_name)
```
## ğŸ“„ File: `__init__.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/__init__.py`

```python

```
## ğŸ“„ File: `path_utils.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/path_utils.py`

```python
import os


def find_project_root(file_path):
    """TÃ¬m thÆ° má»¥c gá»‘c cá»§a dá»± Ã¡n báº¯t Ä‘áº§u tá»« file_path."""
    current_path = os.path.dirname(os.path.abspath(file_path))
    while current_path != os.path.dirname(current_path):
        if "backend" in os.listdir(current_path):
            return current_path
        current_path = os.path.dirname(current_path)
    raise RuntimeError("KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c gá»‘c dá»± Ã¡n.")


def get_paths():
    base_dir = find_project_root(__file__)
    return {
        "BASE_DIR": base_dir,
        "DB_PATH": os.path.join(base_dir, "backend/database/events.db"),
        "BACKEND_DIR": os.path.join(base_dir, "backend"),
        "FRONTEND_DIR": os.path.join(base_dir, "frontend"),
        "CAMERA_ROI_DIR": os.path.join(base_dir, "resources", "output_clips", "CameraROI")
    }

```
## ğŸ“„ File: `config.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/config/config.py`

```python
from datetime import datetime
from flask import Blueprint, request, jsonify, Flask
from flask_cors import CORS
import os
import json
import sqlite3
import logging
import time
from modules.db_utils import find_project_root, get_db_connection
from modules.scheduler.db_sync import db_rwlock
from modules.sources.path_manager import PathManager
from modules.sources.nvr_client import NVRClient
from database import update_database, DB_PATH, initialize_sync_status
from modules.sources.nvr_downloader import NVRDownloader
from modules.sources.auto_sync_service import AutoSyncService
# ğŸ†• NEW: Cloud endpoints module
from modules.sources.cloud_endpoints import cloud_bp

config_bp = Blueprint('config', __name__)

# XÃ¡c Ä‘á»‹nh thÆ° má»¥c gá»‘c cá»§a dá»± Ã¡n
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
CONFIG_FILE = os.path.join(BASE_DIR, "config.json")

def init_app_and_config():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger(__name__)

    app = Flask(__name__)
    CORS(app, resources={r"/*": {"origins": "http://localhost:3000"}})

    DB_PATH = os.path.join(BASE_DIR, "database", "events.db")
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)

    # Gá»i update_database Ä‘á»ƒ táº¡o báº£ng trÆ°á»›c khi truy váº¥n
    update_database()

    def get_db_path():
        default_db_path = DB_PATH
        try:
            with db_rwlock.gen_rlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("SELECT db_path FROM processing_config WHERE id = 1")
                result = cursor.fetchone()
                conn.close()
            return result[0] if result else default_db_path
        except Exception as e:
            logger.error(f"Error getting DB_PATH from database: {e}")
            return default_db_path

    final_db_path = get_db_path()
    logger.info(f"Using DB_PATH: {final_db_path}")

    # Truy váº¥n cáº¥u hÃ¬nh tá»« processing_config
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT input_path, output_path, frame_rate, frame_interval FROM processing_config WHERE id = 1")
        result = cursor.fetchone()
        conn.close()
        if result:
            INPUT_PATH, OUTPUT_PATH, FRAME_RATE, FRAME_INTERVAL = result
        else:
            INPUT_PATH = os.path.join(BASE_DIR, "Inputvideo")
            OUTPUT_PATH = os.path.join(BASE_DIR, "output_clips")
            FRAME_RATE = 30
            FRAME_INTERVAL = 6
    except Exception as e:
        logger.error(f"Error querying processing_config: {e}")
        INPUT_PATH = os.path.join(BASE_DIR, "Inputvideo")
        OUTPUT_PATH = os.path.join(BASE_DIR, "output_clips")
        FRAME_RATE = 30
        FRAME_INTERVAL = 6

    return app, final_db_path, logger

# HÃ m Ä‘á»c cáº¥u hÃ¬nh tá»« file hoáº·c biáº¿n mÃ´i trÆ°á»ng
def load_config():
    default_config = {
        "db_path": os.path.join(BASE_DIR, "database", "events.db"),
        "input_path": os.path.join(BASE_DIR, "Inputvideo"),
        "output_path": os.path.join(BASE_DIR, "output_clips")
    }
    
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"Error loading config file: {e}")
            return default_config
    
    return {
        "db_path": os.getenv("DB_PATH", default_config["db_path"]),
        "input_path": os.getenv("INPUT_PATH", default_config["input_path"]),
        "output_path": os.getenv("OUTPUT_PATH", default_config["output_path"])
    }

# Load cáº¥u hÃ¬nh tá»« processing_config thay vÃ¬ config.json
config = load_config()

def detect_camera_folders(path):
    """Detect camera folders in the given path"""
    cameras = []
    
    if not os.path.exists(path):
        return cameras
    
    try:
        for item in os.listdir(path):
            item_path = os.path.join(path, item)
            if os.path.isdir(item_path):
                # Check if this looks like a camera folder
                # Common camera folder patterns: Cam*, Camera*, Channel*, etc.
                item_lower = item.lower()
                if any(pattern in item_lower for pattern in ['cam', 'camera', 'channel', 'ch']):
                    cameras.append(item)
                # Also include any directory that contains video files
                elif has_video_files(item_path):
                    cameras.append(item)
        
        cameras.sort()  # Sort alphabetically
        return cameras
    except Exception as e:
        print(f"Error detecting cameras in {path}: {e}")
        return cameras

def has_video_files(path, max_depth=2):
    """Check if directory contains video files (recursive up to max_depth)"""
    video_extensions = ('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.m4v', '.mpg', '.mpeg')
    
    def check_directory(dir_path, depth):
        if depth > max_depth:
            return False
        
        try:
            for item in os.listdir(dir_path):
                item_path = os.path.join(dir_path, item)
                if os.path.isfile(item_path) and item.lower().endswith(video_extensions):
                    return True
                elif os.path.isdir(item_path) and depth < max_depth:
                    if check_directory(item_path, depth + 1):
                        return True
        except (PermissionError, OSError):
            pass
        
        return False
    
    return check_directory(path, 0)

# âœ… FIX: Helper function Ä‘á»ƒ map path cho cÃ¡c source type khÃ¡c nhau
def get_working_path_for_source(source_type, source_name, source_path):
    """Map source connection info to actual working directory"""
    if source_type == 'nvr':
        # NVR: source_path is connection URL, working path is download directory
        working_path = os.path.join(BASE_DIR, "nvr_downloads", source_name)
        print(f"ğŸ”— NVR Path Mapping: {source_path} â†’ {working_path}")
        
        # Create directory if it doesn't exist
        try:
            os.makedirs(working_path, exist_ok=True)
            print(f"ğŸ“ Created/verified NVR directory: {working_path}")
        except Exception as e:
            print(f"âŒ Failed to create NVR directory {working_path}: {e}")
            
        return working_path
        
    elif source_type == 'local':
        # Local: source_path is actual file system path
        working_path = source_path
        print(f"ğŸ“ Local Path Mapping: {source_path} â†’ {working_path}")
        return working_path
        
    elif source_type == 'cloud':
        # Cloud: source_path is cloud URL, working path is sync directory
        working_path = os.path.join(BASE_DIR, "cloud_sync", source_name)
        print(f"â˜ï¸ Cloud Path Mapping: {source_path} â†’ {working_path}")
        
        # Create directory if it doesn't exist
        try:
            os.makedirs(working_path, exist_ok=True)
            print(f"ğŸ“ Created/verified Cloud directory: {working_path}")
        except Exception as e:
            print(f"âŒ Failed to create Cloud directory {working_path}: {e}")
            
        return working_path
        
    else:
        # Unknown source type: use as-is
        print(f"â“ Unknown source type '{source_type}', using path as-is: {source_path}")
        return source_path

@config_bp.route('/detect-cameras', methods=['POST'])
def detect_cameras():
    """Detect camera folders in the specified path"""
    try:
        data = request.json
        path = data.get('path')
        
        if not path:
            return jsonify({"error": "Path is required"}), 400
        
        if not os.path.exists(path):
            return jsonify({"error": f"Path does not exist: {path}"}), 400
        
        # Detect camera folders
        cameras = detect_camera_folders(path)
        
        # Get current selected cameras from processing_config
        selected_cameras = []
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT selected_cameras FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            if result and result[0]:
                selected_cameras = json.loads(result[0])
            conn.close()
        except Exception as e:
            print(f"Error getting selected cameras: {e}")
        
        return jsonify({
            "cameras": cameras,
            "selected_cameras": selected_cameras,
            "path": path,
            "count": len(cameras)
        }), 200
        
    except Exception as e:
        return jsonify({"error": f"Failed to detect cameras: {str(e)}"}), 500

@config_bp.route('/update-source-cameras', methods=['POST'])
def update_source_cameras():
    """ğŸ”§ Update selected cameras for a source in processing_config (Simple Update)"""
    try:
        data = request.json
        source_id = data.get('source_id')
        selected_cameras = data.get('selected_cameras', [])
        
        # Update processing_config with selected cameras
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            UPDATE processing_config 
            SET selected_cameras = ? 
            WHERE id = 1
        """, (json.dumps(selected_cameras),))
        
        conn.commit()
        conn.close()
        
        return jsonify({
            "message": "Camera selection updated successfully",
            "selected_cameras": selected_cameras,
            "count": len(selected_cameras)
        }), 200
        
    except Exception as e:
        return jsonify({"error": f"Failed to update camera selection: {str(e)}"}), 500

@config_bp.route('/get-cameras', methods=['GET'])
def get_cameras():
    try:
        path_manager = PathManager()
        sources = path_manager.get_all_active_sources()
        
        cameras = []
        
        if sources:
            # Use active source
            active_source = sources[0]  # Single active source
            source_type = active_source['source_type']
            
            if source_type == 'local':
                # Local directory scanning
                video_root = active_source['path']
                if not os.path.exists(video_root):
                    return jsonify({"error": f"Directory {video_root} does not exist. Ensure the path is correct or create the directory."}), 400
                
                # Detect camera folders
                detected_cameras = detect_camera_folders(video_root)
                for camera in detected_cameras:
                    cameras.append({"name": camera, "path": os.path.join(video_root, camera)})
            
            elif source_type in ['network', 'cloud', 'camera']:
                # For other source types, use source name as camera
                cameras.append({"name": active_source['name'], "path": active_source['path']})
        else:
            # Fallback to legacy behavior
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT input_path FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            conn.close()

            if not result:
                return jsonify({"error": "video_root not found in configuration. Please update via /save-config endpoint."}), 400

            video_root = result[0]
            if not os.path.exists(video_root):
                return jsonify({"error": f"Directory {video_root} does not exist. Ensure the path is correct or create the directory."}), 400

            detected_cameras = detect_camera_folders(video_root)
            for camera in detected_cameras:
                cameras.append({"name": camera, "path": os.path.join(video_root, camera)})

        return jsonify({"cameras": cameras}), 200
    except Exception as e:
        return jsonify({"error": f"Failed to retrieve cameras: {str(e)}"}), 500

@config_bp.route('/save-config', methods=['POST'])
def save_config():
    """Fixed save_config without frame_settings table dependency"""
    try:
        data = request.json
        if not data:
            return jsonify({"error": "No data provided"}), 400
            
        video_root = data.get('video_root')
        output_path = data.get('output_path', config.get("output_path", "/default/output"))
        default_days = data.get('default_days', 30)
        min_packing_time = data.get('min_packing_time', 10)
        max_packing_time = data.get('max_packing_time', 120)
        frame_rate = data.get('frame_rate', 30)
        frame_interval = data.get('frame_interval', 5)
        video_buffer = data.get('video_buffer', 2)
        selected_cameras = data.get('selected_cameras', [])
        run_default_on_start = data.get('run_default_on_start', 1)

        print(f"=== SAVE CONFIG REQUEST ===")
        print(f"Raw video_root from frontend: {video_root}")
        print(f"Selected cameras: {selected_cameras}")

        # âœ… FIX: Enhanced path mapping with better error handling
        try:
            # Try to get active source for path mapping
            try:
                from modules.sources.path_manager import PathManager
                path_manager = PathManager()
                active_sources = path_manager.get_all_active_sources()
                
                if active_sources:
                    active_source = active_sources[0]  # Single active source
                    source_type = active_source['source_type']
                    source_name = active_source['name']
                    source_path = active_source['path']
                    
                    print(f"Found active source: {source_name} ({source_type})")
                    
                    # Apply proper path mapping
                    correct_working_path = get_working_path_for_source(source_type, source_name, source_path)
                    
                    if video_root != correct_working_path:
                        print(f"ğŸ”„ Correcting video_root: {video_root} â†’ {correct_working_path}")
                        video_root = correct_working_path
                    else:
                        print(f"âœ… video_root already correct: {video_root}")
                else:
                    print("âš ï¸ No active video source found, using video_root as-is")
                    
            except ImportError:
                print("âš ï¸ PathManager not available, using video_root as-is")
            except Exception as pm_error:
                print(f"âš ï¸ PathManager error: {pm_error}, using video_root as-is")
                
        except Exception as path_error:
            print(f"âŒ Error in path mapping: {path_error}")

        # âœ… Final validation
        if not video_root or video_root.strip() == "":
            error_msg = "âŒ video_root cannot be empty"
            print(error_msg)
            return jsonify({"error": error_msg}), 400

        # Basic URL validation
        if '://' in video_root or 'localhost:' in video_root:
            error_msg = f"âŒ video_root cannot be a URL: {video_root}"
            print(error_msg)
            return jsonify({"error": error_msg}), 400

        print(f"ğŸ“ Final video_root for database: {video_root}")

        # âœ… FIXED: Database operations - only processing_config table
        try:
            conn = get_db_connection()
            if not conn:
                return jsonify({"error": "Database connection failed"}), 500
                
            cursor = conn.cursor()
            
            # Check if processing_config table exists
            cursor.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name='processing_config'
            """)
            if not cursor.fetchone():
                return jsonify({"error": "processing_config table not found"}), 500
            
            # Add column if not exists (safe operation)
            try:
                cursor.execute("ALTER TABLE processing_config ADD COLUMN run_default_on_start INTEGER DEFAULT 1")
                print("âœ… Added run_default_on_start column")
            except sqlite3.OperationalError:
                pass  # Column already exists
            
            # âœ… MAIN FIX: Only insert into processing_config (no frame_settings)
            cursor.execute("""
                INSERT OR REPLACE INTO processing_config (
                    id, input_path, output_path, storage_duration, min_packing_time, 
                    max_packing_time, frame_rate, frame_interval, video_buffer, default_frame_mode, 
                    selected_cameras, db_path, run_default_on_start
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (1, video_root, output_path, default_days, min_packing_time, max_packing_time, 
                  frame_rate, frame_interval, video_buffer, "default", json.dumps(selected_cameras), 
                  DB_PATH, run_default_on_start))

            print("âœ… processing_config updated successfully")

            # âœ… REMOVED: No more frame_settings table insert
            # Frame data is now stored in processing_config table only

            conn.commit()
            
            # âœ… Verify what was saved
            cursor.execute("SELECT input_path, selected_cameras, frame_rate, frame_interval FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            if result:
                saved_path, saved_cameras, saved_fr, saved_fi = result
                print(f"âœ… Verified saved input_path: {saved_path}")
                print(f"âœ… Verified saved cameras: {saved_cameras}")
                print(f"âœ… Verified saved frame_rate: {saved_fr}, frame_interval: {saved_fi}")
            
            conn.close()
            
            print("âœ… Config saved successfully")
            return jsonify({
                "message": "Configuration saved successfully",
                "saved_path": video_root,
                "saved_cameras": selected_cameras,
                "frame_settings": {
                    "frame_rate": frame_rate,
                    "frame_interval": frame_interval
                }
            }), 200
            
        except sqlite3.PermissionError as e:
            error_msg = f"Database permission denied: {str(e)}"
            print(f"âŒ {error_msg}")
            return jsonify({"error": error_msg}), 403
        except sqlite3.Error as e:
            error_msg = f"Database error: {str(e)}"
            print(f"âŒ {error_msg}")
            return jsonify({"error": error_msg}), 500
        except Exception as e:
            error_msg = f"Database operation failed: {str(e)}"
            print(f"âŒ {error_msg}")
            return jsonify({"error": error_msg}), 500

    except Exception as e:
        # âœ… Catch-all error handler to ensure JSON response
        error_msg = f"Unexpected error: {str(e)}"
        print(f"âŒ CRITICAL ERROR in save_config: {error_msg}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": error_msg}), 500

@config_bp.route('/save-general-info', methods=['POST'])
def save_general_info():
    data = request.json
    country = data.get('country')
    timezone = data.get('timezone')
    brand_name = data.get('brand_name')
    working_days = data.get('working_days', ["Thá»© Hai", "Thá»© Ba", "Thá»© TÆ°", "Thá»© NÄƒm", "Thá»© SÃ¡u", "Thá»© Báº£y", "Chá»§ Nháº­t"])
    
    # Báº£n Ä‘á»“ ngÃ y tiáº¿ng Viá»‡t sang tiáº¿ng Anh
    day_map = {
        'Thá»© Hai': 'Monday', 'Thá»© Ba': 'Tuesday', 'Thá»© TÆ°': 'Wednesday',
        'Thá»© NÄƒm': 'Thursday', 'Thá»© SÃ¡u': 'Friday', 'Thá»© Báº£y': 'Saturday',
        'Chá»§ Nháº­t': 'Sunday'
    }
    
    # Chuyá»ƒn Ä‘á»•i working_days sang tiáº¿ng Anh
    working_days_en = [day_map.get(day, day) for day in working_days]
    
    from_time = data.get('from_time', "07:00")
    to_time = data.get('to_time', "23:00")

    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            INSERT OR REPLACE INTO general_info (
                id, country, timezone, brand_name, working_days, from_time, to_time
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (1, country, timezone, brand_name, json.dumps(working_days_en), from_time, to_time))

        conn.commit()
        conn.close()
    except PermissionError as e:
        return jsonify({"error": f"Permission denied: {str(e)}. Check database file permissions."}), 403
    except Exception as e:
        return jsonify({"error": f"Database error: {str(e)}. Ensure the database is accessible."}), 500

    print("General info saved:", data)
    return jsonify({"message": "General info saved"}), 200

@config_bp.route('/save-sources', methods=['POST'])
def save_video_sources():
    """Save single active video source - Enhanced with auto-sync for NVR"""
    data = request.json
    sources = data.get('sources', [])
    
    if not sources:
        return jsonify({"error": "No sources provided"}), 400
    
    # Single Active Source: only process the first source
    source = sources[0]
    source_type = source.get('source_type')
    name = source.get('name')
    path = source.get('path')
    config_data = source.get('config', {})
    
    print(f"=== SAVE SOURCE: {name} ({source_type}) ===")
    print(f"Connection path: {path}")
    print(f"Config data: {config_data}")
    
    if not all([source_type, name, path]):
        return jsonify({"error": "Source missing required fields"}), 400
    
    path_manager = PathManager()
    
    try:
        # Disable all existing sources first
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("UPDATE video_sources SET active = 0")
        conn.commit()
        conn.close()
        
        # Add new source as active
        success, message = path_manager.add_source(source_type, name, path, config_data)
        
        if success:
            # Get source ID for database operations
            source_id = path_manager.get_source_id_by_name(name)
            
            # âœ… FIX: Calculate correct working path and update processing_config
            working_path = get_working_path_for_source(source_type, name, path)
            
            # Update processing_config.input_path to point to working path
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE processing_config 
                SET input_path = ? 
                WHERE id = 1
            """, (working_path,))
            
            print(f"âœ… Updated processing_config.input_path to: {working_path}")
            
            # ğŸ†• ENHANCED NVR PROCESSING WITH AUTO-SYNC
            if source_type == 'nvr':
                print(f"ğŸ¯ PROCESSING NVR SOURCE WITH AUTO-SYNC...")
                
                selected_cameras = config_data.get('selected_cameras', [])
                
                if selected_cameras:
                    # 1. Path validation and directory creation
                    print(f"ğŸ“ Creating directories for {len(selected_cameras)} cameras...")
                    try:
                        from modules.utils.path_validator import PathValidator
                        path_validator = PathValidator()
                        
                        # Validate source path
                        path_result = path_validator.validate_source_path(source_type, working_path)
                        if not path_result['success']:
                            print(f"âš ï¸ Path validation warning: {path_result['message']}")
                        
                        # Create camera directories
                        camera_result = path_validator.create_camera_directories(working_path, selected_cameras)
                        if camera_result['success']:
                            print(f"âœ… Created {len(camera_result['created_directories'])} camera directories")
                        else:
                            print(f"âŒ Directory creation failed: {camera_result['message']}")
                            
                    except ImportError:
                        print("âš ï¸ PathValidator not available, using basic directory creation")
                        for camera in selected_cameras:
                            camera_dir = os.path.join(working_path, camera.replace(' ', '_'))
                            os.makedirs(camera_dir, exist_ok=True)
                            print(f"ğŸ“ Created: {camera_dir}")
                    
                    # 2. Initialize sync status (always enabled for NVR)
                    print(f"ğŸ”„ Initializing auto-sync for source ID: {source_id}")
                    #try:
                    #    sync_result = initialize_sync_status(
                    #       source_id, 
                    #        sync_enabled=True, 
                    #        interval_minutes=2  # 2 minutes for testing
                    #   )
                    #    if sync_result:
                    #        print(f"âœ… Auto-sync enabled (2-minute intervals)")
                    #    else:
                    #        print(f"âš ï¸ Auto-sync initialization failed")
                    #except Exception as sync_error:
                    #    print(f"âŒ Auto-sync initialization error: {sync_error}")
                    
                    # 3. Update camera paths in processing_config
                    print(f"ğŸ—ºï¸ Updating camera paths mapping...")
                    #try:
                    #    camera_paths = {}
                    #    for camera in selected_cameras:
                    #        camera_dir = os.path.join(working_path, camera.replace(' ', '_'))
                    #        camera_paths[camera] = camera_dir
                        
                    #    update_camera_paths(camera_paths)
                    #    print(f"âœ… Updated camera paths for {len(camera_paths)} cameras")
                        
                    #except Exception as path_error:
                    #    print(f"âŒ Camera paths update error: {path_error}")
                    
                    # 4. ğŸ¬ INITIAL MOCK DOWNLOAD - Key Feature!
                    print(f"ğŸ¬ Starting initial mock download...")
                    try:
                        # Initialize NVRDownloader in mock mode with testing intervals
                        downloader = NVRDownloader(mock_mode=True, testing_intervals=True)
                        
                        download_config = {
                            'source_id': source_id,
                            'name': name,
                            'selected_cameras': selected_cameras,
                            'working_path': working_path
                        }
                        
                        # Perform initial download
                        download_results = downloader.download_recordings(download_config)
                        
                        if download_results['success']:
                            total_files = len(download_results['downloaded_files'])
                            total_size_kb = download_results['total_size'] / 1024
                            cameras_count = len(download_results['cameras_processed'])
                            
                            print(f"ğŸ‰ INITIAL DOWNLOAD SUCCESS:")
                            print(f"   ğŸ“Š Files created: {total_files}")
                            print(f"   ğŸ“ Total size: {total_size_kb:.1f} KB")
                            print(f"   ğŸ¥ Cameras processed: {cameras_count}")
                            
                            # Store download stats for response
                            initial_download_stats = {
                                'files_created': total_files,
                                'total_size_kb': round(total_size_kb, 1),
                                'cameras_processed': cameras_count
                            }
                        else:
                            print(f"âŒ Initial download failed: {download_results.get('error', 'Unknown error')}")
                            initial_download_stats = {
                                'files_created': 0,
                                'total_size_kb': 0,
                                'cameras_processed': 0,
                                'error': download_results.get('error')
                            }
                            
                    except Exception as download_error:
                        print(f"âŒ Initial download error: {download_error}")
                        initial_download_stats = {
                            'files_created': 0,
                            'total_size_kb': 0,
                            'cameras_processed': 0,
                            'error': str(download_error)
                        }
                    
                    # 5. ğŸš€ START BACKGROUND AUTO-SYNC SERVICE
                    print(f"ğŸš€ Starting background auto-sync service...")
                    try:
                        # Note: AutoSyncService implementation will be in next phase
                        # For now, just log that it would start
                        auto_sync_config = {
                            'source_id': source_id,
                            'source_name': name,
                            'selected_cameras': selected_cameras,
                            'working_path': working_path,
                            'sync_interval_minutes': 2,  # Testing interval
                            'mock_mode': True
                        }
                        
                        print(f"âœ… Auto-sync service ready with config: {auto_sync_config}")
                        # TODO: auto_sync_service.start_auto_sync(auto_sync_config)
                        
                    except Exception as service_error:
                        print(f"âŒ Auto-sync service error: {service_error}")
                    
                    # Sync cameras to processing_config
                    cursor.execute("""
                        UPDATE processing_config 
                        SET selected_cameras = ? 
                        WHERE id = 1
                    """, (json.dumps(selected_cameras),))
                    print(f"âœ… Synced {len(selected_cameras)} cameras to processing_config")
                    
                else:
                    print("âš ï¸ No cameras selected for NVR source")
                    initial_download_stats = {'files_created': 0, 'cameras_processed': 0}
                    cursor.execute("""
                        UPDATE processing_config 
                        SET selected_cameras = '[]' 
                        WHERE id = 1
                    """)
            
            # Handle other source types (existing logic)
            elif source_type == 'local':
                # Local source: Auto-detect cameras from file system
                try:
                    cameras = detect_camera_folders(working_path)
                    if cameras:
                        cursor.execute("""
                            UPDATE processing_config 
                            SET selected_cameras = ? 
                            WHERE id = 1
                        """, (json.dumps(cameras),))
                        print(f"âœ… Local cameras auto-selected: {cameras}")
                except Exception as camera_error:
                    print(f"Camera detection failed: {camera_error}")
                    cursor.execute("""
                        UPDATE processing_config 
                        SET selected_cameras = '[]' 
                        WHERE id = 1
                    """)
                    
            elif source_type == 'cloud':
                # Cloud source: use cameras from config (similar to NVR)
                selected_cameras = config_data.get('selected_cameras', [])
                if selected_cameras:
                    cursor.execute("""
                        UPDATE processing_config 
                        SET selected_cameras = ? 
                        WHERE id = 1
                    """, (json.dumps(selected_cameras),))
                    print(f"âœ… Cloud cameras synced to processing_config: {selected_cameras}")
                else:
                    cursor.execute("""
                        UPDATE processing_config 
                        SET selected_cameras = '[]' 
                        WHERE id = 1
                    """)
            else:
                # Unknown source type: clear cameras
                cursor.execute("""
                    UPDATE processing_config 
                    SET selected_cameras = '[]' 
                    WHERE id = 1
                """)
                print(f"âš ï¸ Unknown source type '{source_type}', cleared cameras")
            
            conn.commit()
            conn.close()
            
            # ğŸ‰ ENHANCED RESPONSE WITH DOWNLOAD STATS
            response_data = {
                "message": f"Source '{name}' set as active",
                "source_type": source_type,
                "connection_path": path,
                "working_path": working_path,
                "cameras_synced": config_data.get('selected_cameras', []) if source_type in ['nvr', 'cloud'] else detect_camera_folders(working_path) if source_type == 'local' else []
            }
            
            # Add NVR-specific response data
            if source_type == 'nvr' and 'initial_download_stats' in locals():
                response_data.update({
                    "auto_sync_enabled": True,
                    "initial_download": initial_download_stats,
                    "sync_interval_minutes": 2
                })
                
                # Enhanced success message
                if initial_download_stats['files_created'] > 0:
                    response_data["message"] = f"NVR source '{name}' activated with auto-sync. {initial_download_stats['files_created']} recordings downloaded immediately."
            
            return jsonify(response_data), 200
        else:
            return jsonify({"error": f"Failed to save source: {message}"}), 400
            
    except Exception as e:
        print(f"Failed to save sources: {str(e)}")
        return jsonify({"error": f"Failed to save sources: {str(e)}"}), 500

# ğŸ†• HELPER FUNCTIONS TO ADD TO config.py

def update_camera_paths(camera_paths_dict):
    """
    Update camera paths mapping in processing_config
    
    Args:
        camera_paths_dict (dict): Camera name -> directory path mapping
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Update camera_paths column in processing_config
        cursor.execute("""
            UPDATE processing_config 
            SET camera_paths = ? 
            WHERE id = 1
        """, (json.dumps(camera_paths_dict),))
        
        conn.commit()
        conn.close()
        
        print(f"âœ… Camera paths updated: {camera_paths_dict}")
        
    except Exception as e:
        print(f"âŒ Failed to update camera paths: {e}")

def get_nvr_download_status(source_id):
    """
    Get current download status for NVR source
    
    Args:
        source_id (int): Source database ID
        
    Returns:
        dict: Download status information
    """
    try:
        downloader = NVRDownloader(mock_mode=True)
        stats = downloader.get_download_statistics(source_id)
        
        # Get sync status
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            SELECT sync_enabled, sync_interval_minutes, last_sync_time, last_sync_status
            FROM sync_status WHERE source_id = ?
        """, (source_id,))
        
        sync_row = cursor.fetchone()
        conn.close()
        
        if sync_row:
            sync_enabled, interval, last_sync, last_status = sync_row
            stats['sync_status'] = {
                'enabled': bool(sync_enabled),
                'interval_minutes': interval,
                'last_sync_time': last_sync,
                'last_sync_status': last_status
            }
        else:
            stats['sync_status'] = {
                'enabled': False,
                'interval_minutes': None,
                'last_sync_time': None,
                'last_sync_status': None
            }
        
        return stats
        
    except Exception as e:
        print(f"âŒ Failed to get NVR download status: {e}")
        return {
            'total_files': 0,
            'total_size': 0,
            'cameras_count': 0,
            'sync_status': {'enabled': False}
        }

# ğŸ†• NEW API ENDPOINT TO ADD TO config.py

@config_bp.route('/get-nvr-status/<int:source_id>', methods=['GET'])
def get_nvr_status(source_id):
    """
    Get NVR source download status and statistics
    
    Args:
        source_id (int): Source database ID via URL parameter
        
    Returns:
        JSON: NVR status information
    """
    try:
        status = get_nvr_download_status(source_id)
        
        return jsonify({
            "source_id": source_id,
            "download_stats": status,
            "timestamp": datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        return jsonify({
            "error": f"Failed to get NVR status: {str(e)}",
            "source_id": source_id
        }), 500

@config_bp.route('/trigger-nvr-sync/<int:source_id>', methods=['POST'])
def trigger_manual_nvr_sync(source_id):
    """
    Manually trigger NVR sync for testing/debugging
    
    Args:
        source_id (int): Source database ID via URL parameter
        
    Returns:
        JSON: Sync results
    """
    try:
        # Get source info
        path_manager = PathManager()
        source = path_manager.get_source_by_id(source_id)
        
        if not source or source['source_type'] != 'nvr':
            return jsonify({
                "error": f"NVR source {source_id} not found"
            }), 404
        
        # Get working path and cameras
        working_path = get_working_path_for_source(
            source['source_type'], 
            source['name'], 
            source['path']
        )
        
        selected_cameras = source['config'].get('selected_cameras', [])
        
        # Trigger download
        downloader = NVRDownloader(mock_mode=True, testing_intervals=True)
        
        download_config = {
            'source_id': source_id,
            'name': source['name'],
            'selected_cameras': selected_cameras,
            'working_path': working_path
        }
        
        results = downloader.download_recordings(download_config)
        
        # Update sync status
        if results['success']:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE sync_status 
                SET last_sync_time = ?, last_sync_status = ?
                WHERE source_id = ?
            """, (datetime.now().isoformat(), 'success', source_id))
            conn.commit()
            conn.close()
        
        return jsonify({
            "message": "Manual sync completed",
            "source_id": source_id,
            "results": results
        }), 200
        
    except Exception as e:
        return jsonify({
            "error": f"Manual sync failed: {str(e)}",
            "source_id": source_id
        }), 500

@config_bp.route('/test-source', methods=['POST'])  
def test_source_connection():
    """Test connectivity for all source types including NVR/DVR"""
    try:
        # Ensure we have valid JSON request
        if not request.is_json:
            return jsonify({
                "accessible": False,
                "message": "Invalid request format - JSON required",
                "source_type": "unknown"
            }), 400
        
        data = request.get_json()
        if not data:
            return jsonify({
                "accessible": False,
                "message": "No data provided",
                "source_type": "unknown"
            }), 400
        
        source_type = data.get('source_type')
        
        if not source_type:
            return jsonify({
                "accessible": False,
                "message": "source_type is required",
                "source_type": "unknown"
            }), 400
        
        # Handle different source types
        if source_type == 'local':
            # Existing local path validation
            source_config = {
                'source_type': source_type,
                'path': data.get('path'),
                'config': data.get('config', {})
            }
            
            if not source_config['path']:
                return jsonify({
                    "accessible": False,
                    "message": "path is required for local sources",
                    "source_type": source_type
                }), 400
            
            path_manager = PathManager()
            is_accessible, message = path_manager.validate_source_accessibility(source_config)
            
            return jsonify({
                "accessible": is_accessible,
                "message": message,
                "source_type": source_type
            }), 200
            
        elif source_type == 'nvr':
            # ğŸ†• NEW: NVR connection testing + camera discovery
            from modules.sources.nvr_client import NVRClient
            
            nvr_client = NVRClient()
            result = nvr_client.test_connection_and_discover_cameras(data)
            
            return jsonify(result), 200
            
        elif source_type == 'cloud':
            # ğŸ†• NEW: Cloud connection testing + folder discovery
            from modules.sources.cloud_manager import CloudManager
            cloud_manager = CloudManager(provider='google_drive')
            result = cloud_manager.test_connection_and_discover_folders(data)
            return jsonify(result), 200
            
        else:
            return jsonify({
                "accessible": False,
                "message": f"Unknown source type: {source_type}",
                "source_type": source_type
            }), 400
        
    except ImportError as e:
        return jsonify({
            "accessible": False,
            "message": f"NVR module not available: {str(e)}",
            "source_type": data.get('source_type', 'unknown')
        }), 500
        
    except json.JSONDecodeError:
        return jsonify({
            "accessible": False,
            "message": "Invalid JSON format",
            "source_type": "unknown"
        }), 400
        
    except Exception as e:
        return jsonify({
            "accessible": False,
            "message": f"Test failed: {str(e)}",
            "source_type": data.get('source_type', 'unknown')
        }), 500

@config_bp.route('/get-sources', methods=['GET'])
def get_video_sources():
    """Get all video sources"""
    try:
        path_manager = PathManager()
        sources = path_manager.get_all_active_sources()
        
        return jsonify({"sources": sources}), 200
        
    except Exception as e:
        return jsonify({"error": f"Failed to retrieve sources: {str(e)}"}), 500

@config_bp.route('/update-source/<int:source_id>', methods=['PUT'])
def update_video_source(source_id):
    """ğŸ”§ Simple update video source - same type only, mainly for camera selection"""
    try:
        data = request.json
        path_manager = PathManager()
        
        # Get current source for validation
        current_source = path_manager.get_source_by_id(source_id)
        if not current_source:
            return jsonify({"error": f"Source with id {source_id} not found"}), 404
        
        # For now, we only support updating the config (mainly for camera selection)
        # Path and source_type changes are handled by "Change" button workflow
        new_config = data.get('config', current_source['config'])
        
        # ğŸ”„ Update source config only
        success, message = path_manager.update_source(source_id, config=new_config)
        
        if not success:
            return jsonify({"error": message}), 400
        
        return jsonify({
            "message": message,
            "source_id": source_id,
            "updated_fields": ["config"]
        }), 200
        
    except Exception as e:
        return jsonify({"error": f"Failed to update source: {str(e)}"}), 500

@config_bp.route('/delete-source/<int:source_id>', methods=['DELETE'])
def delete_video_source(source_id):
    """ğŸ”„ Delete video source (used by Change button to reset workflow)"""
    path_manager = PathManager()
    
    try:
        # Get source info before deletion for logging
        source = path_manager.get_source_by_id(source_id)
        source_name = source.get('name', 'Unknown') if source else 'Unknown'
        
        success, message = path_manager.delete_source(source_id)
        
        if success:
            # ğŸ§¹ Clean reset processing_config 
            try:
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE processing_config 
                    SET input_path = '', selected_cameras = '[]' 
                    WHERE id = 1
                """)
                conn.commit()
                conn.close()
                
                print(f"Source '{source_name}' deleted and processing_config reset")
                
            except Exception as config_error:
                print(f"Failed to reset processing_config: {config_error}")
            
            return jsonify({
                "message": f"Source '{source_name}' removed successfully. You can now add a new source.",
                "reset": True
            }), 200
        else:
            return jsonify({"error": message}), 400
            
    except Exception as e:
        return jsonify({"error": f"Failed to delete source: {str(e)}"}), 500

@config_bp.route('/toggle-source/<int:source_id>', methods=['POST'])
def toggle_source_status(source_id):
    """Toggle source active status"""
    data = request.json
    active = data.get('active', True)
    path_manager = PathManager()
    
    try:
        if active:
            # Disable all other sources first (Single Active Source)
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("UPDATE video_sources SET active = 0")
            conn.commit()
            conn.close()
        
        success, message = path_manager.toggle_source_status(source_id, active)
        
        if success and active:
            # Update input_path to this source
            source = path_manager.get_source_by_id(source_id)
            if source:
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE processing_config 
                    SET input_path = ? 
                    WHERE id = 1
                """, (source['path'],))
                
                # Auto-detect cameras for local sources
                if source['source_type'] == 'local':
                    try:
                        cameras = detect_camera_folders(source['path'])
                        if cameras:
                            cursor.execute("""
                                UPDATE processing_config 
                                SET selected_cameras = ? 
                                WHERE id = 1
                            """, (json.dumps(cameras),))
                        else:
                            cursor.execute("""
                                UPDATE processing_config 
                                SET selected_cameras = '[]' 
                                WHERE id = 1
                            """)
                    except Exception as camera_error:
                        print(f"Camera detection failed: {camera_error}")
                        cursor.execute("""
                            UPDATE processing_config 
                            SET selected_cameras = '[]' 
                            WHERE id = 1
                        """)
                else:
                    # Clear cameras for non-local sources
                    cursor.execute("""
                        UPDATE processing_config 
                        SET selected_cameras = '[]' 
                        WHERE id = 1
                    """)
                
                conn.commit()
                conn.close()
        
        if success:
            return jsonify({"message": message}), 200
        else:
            return jsonify({"error": message}), 400
            
    except Exception as e:
        return jsonify({"error": f"Failed to toggle source status: {str(e)}"}), 500
    
@config_bp.route('/get-processing-cameras', methods=['GET'])
def get_processing_cameras():
    """Get selected cameras from processing_config"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT selected_cameras FROM processing_config WHERE id = 1")
        result = cursor.fetchone()
        conn.close()
        
        if result and result[0]:
            selected_cameras = json.loads(result[0])
            return jsonify({
                "selected_cameras": selected_cameras,
                "count": len(selected_cameras)
            }), 200
        else:
            return jsonify({
                "selected_cameras": [],
                "count": 0
            }), 200
            
    except Exception as e:
        return jsonify({"error": f"Failed to get processing cameras: {str(e)}"}), 500    
```
## ğŸ“„ File: `logging_config.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/config/logging_config.py`

```python
import logging
import os
import sys
from datetime import datetime
from logging.handlers import RotatingFileHandler

class LogSizeFilter(logging.Filter):
    def __init__(self, log_file, max_size=10*1024*1024):
        super().__init__()
        self.log_file = log_file
        self.max_size = max_size
    
    def filter(self, record):
        if os.path.exists(self.log_file) and os.path.getsize(self.log_file) > self.max_size:
            if record.levelno < logging.INFO:
                return False
            print("Log file size exceeds 10MB, switching to INFO level", file=sys.stderr)
            return True
        return True

class ContextAdapter(logging.LoggerAdapter):
    def process(self, msg, kwargs):
        context = " ".join(f"{k}={v}" for k, v in self.extra.items())
        return f"[{context}] {msg}", kwargs

def setup_logging(base_dir, app_name="app", log_level=logging.INFO):
    log_dir = os.path.join(base_dir, "resources", "output_clips", "LOG")
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f"{app_name}_{datetime.now().strftime('%Y-%m-%d')}.log")
    
    handler = RotatingFileHandler(log_file, maxBytes=10*1024*1024, backupCount=5)
    handler.setFormatter(logging.Formatter(
        '%(asctime)sZ [%(levelname)s] %(name)s: %(message)s',
        datefmt='%Y-%m-%dT%H:%M:%S'
    ))
    handler.addFilter(LogSizeFilter(log_file))
    
    logging.basicConfig(level=log_level, handlers=[handler])

def get_logger(module_name, context=None, separate_log=None):
    logger = logging.getLogger("app")
    if separate_log:
        log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "resources", "output_clips", "LOG")
        os.makedirs(log_dir, exist_ok=True)
        log_file = os.path.join(log_dir, f"{separate_log}_{datetime.now().strftime('%Y-%m-%d')}.log")
        file_handler = RotatingFileHandler(log_file, maxBytes=10*1024*1024, backupCount=5)
        file_handler.setFormatter(logging.Formatter(
            '%(asctime)s,%(msecs)03d - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        ))
        file_handler.setLevel(logging.INFO)
        logger.addHandler(file_handler)
    return ContextAdapter(logger, context or {})
```
## ğŸ“„ File: `__init__.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/config/__init__.py`

```python

```
## ğŸ“„ File: `file_lister.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/scheduler/file_lister.py`

```python
import os
import sqlite3
import json
import logging
from datetime import datetime, timedelta
from statistics import median
from modules.db_utils import find_project_root, get_db_connection
from .db_sync import db_rwlock
import subprocess
import pytz

# Cáº¥u hÃ¬nh mÃºi giá» Viá»‡t Nam
VIETNAM_TZ = pytz.timezone('Asia/Ho_Chi_Minh')

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

logger = logging.getLogger("app")
logger.info("Logging initialized")

# Háº±ng sá»‘ cho quÃ©t Ä‘á»™ng
BUFFER_SECONDS = 6 * 60
N_FILES_FOR_ESTIMATE = 3
DEFAULT_DAYS = 7

DB_PATH = os.path.join(BASE_DIR, "database", "events.db")
os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)

def get_db_path():
    try:
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT db_path FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            conn.close()
            return result[0] if result else DB_PATH
    except Exception as e:
        logger.error(f"Lá»—i khi láº¥y DB_PATH: {e}")
        return DB_PATH

DB_PATH = get_db_path()
logger.info(f"Sá»­ dá»¥ng DB_PATH: {DB_PATH}")

def get_file_creation_time(file_path):
    """Láº¥y thá»i gian táº¡o tá»‡p báº±ng FFmpeg, chuáº©n hÃ³a theo mÃºi giá» Viá»‡t Nam."""
    if not os.path.isfile(file_path) or not file_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv')):
        return datetime.fromtimestamp(os.path.getctime(file_path), tz=VIETNAM_TZ)
    try:
        cmd = [
            "ffprobe",
            "-v", "quiet",
            "-print_format", "json",
            "-show_entries", "format_tags=creation_time:format=creation_time",
            file_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        metadata = json.loads(result.stdout)
        creation_time = (
            metadata.get("format", {})
                    .get("tags", {})
                    .get("creation_time")
            or metadata.get("format", {}).get("creation_time")  
        )  
        if creation_time:
            utc_time = datetime.strptime(creation_time, "%Y-%m-%dT%H:%M:%S.%fZ")
            utc_time = pytz.utc.localize(utc_time)
            return utc_time.astimezone(VIETNAM_TZ)
        else:
            logger.warning(f"KhÃ´ng tÃ¬m tháº¥y creation_time cho {file_path}, dÃ¹ng giá» há»‡ thá»‘ng")
            return datetime.fromtimestamp(os.path.getctime(file_path), tz=VIETNAM_TZ)
    except (subprocess.CalledProcessError, json.JSONDecodeError, ValueError) as e:
        logger.error(f"Lá»—i khi láº¥y creation_time cho {file_path}: {e}")
        return datetime.fromtimestamp(os.path.getctime(file_path), tz=VIETNAM_TZ)

def compute_chunk_interval(ctimes):
    ctimes = sorted(ctimes)[-N_FILES_FOR_ESTIMATE:]
    if len(ctimes) < 2:
        return 30
    intervals = [(ctimes[i+1] - ctimes[i]) / 60 for i in range(len(ctimes)-1)]
    return round(median(intervals))

def scan_files(root_path, video_root, time_threshold, max_ctime, restrict_to_current_date=False, camera_ctime_map=None, working_days=None, from_time=None, to_time=None, selected_cameras=None, strict_date_match=False):
    video_files = []
    file_ctimes = []
    video_extensions = ('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv')
    current_date = datetime.now(VIETNAM_TZ).date()
    skipped_by_ctime = 0
    skipped_by_camera = 0
    ffprobe_errors = 0

    for root, dirs, files in os.walk(root_path):
        relative_path = os.path.relpath(root, video_root)
        camera_name = relative_path.split(os.sep)[0] if relative_path != "." else os.path.basename(video_root)
        if selected_cameras and camera_name not in selected_cameras:
            skipped_by_camera += len([f for f in files if f.lower().endswith(video_extensions)])
            continue

        for file in files:
            if file.lower().endswith(video_extensions):
                file_path = os.path.join(root, file)
                try:
                    file_ctime = get_file_creation_time(file_path)
                except Exception:
                    ffprobe_errors += 1
                    file_ctime = datetime.fromtimestamp(os.path.getctime(file_path), tz=VIETNAM_TZ)

                logger.debug(f"Checking file {file_path}, ctime={file_ctime}, max_ctime={max_ctime}")
                if time_threshold and file_ctime < time_threshold:
                    skipped_by_ctime += 1
                    continue

                if max_ctime and file_ctime <= max_ctime:
                    skipped_by_ctime += 1
                    continue

                weekday = file_ctime.strftime('%A')
                if working_days and weekday not in working_days:
                    skipped_by_ctime += 1
                    logger.debug(f"Skipped file {file_path} due to non-working day: {weekday}")
                    continue

                file_time = file_ctime.time()
                if from_time and to_time and not (from_time <= file_time <= to_time):
                    skipped_by_ctime += 1
                    logger.debug(f"Skipped file {file_path} due to time outside working hours: {file_time}")
                    continue

                relative_path = os.path.relpath(file_path, video_root)
                video_files.append(relative_path)
                file_ctimes.append(file_ctime.timestamp())
                logger.info(f"TÃ¬m tháº¥y tá»‡p: {file_path}")

        if camera_ctime_map is not None:
            dir_ctime = datetime.fromtimestamp(os.path.getctime(root), tz=VIETNAM_TZ)
            camera_ctime_map[camera_name] = max(camera_ctime_map.get(camera_name, 0), dir_ctime.timestamp())

    logger.info(f"Thá»‘ng kÃª quÃ©t: {skipped_by_ctime} tá»‡p bá» qua do ctime, {skipped_by_camera} tá»‡p bá» qua do camera, {ffprobe_errors} lá»—i ffprobe")
    return video_files, file_ctimes

def save_files_to_db(conn, video_files, file_ctimes, scan_action, days, custom_path, video_root):
    if not video_files:
        return

    insert_data = []
    days_val = len(days) if isinstance(days, list) else days if days is not None else None
    for file_path, file_ctime in zip(video_files, file_ctimes):
        absolute_path = os.path.join(video_root, file_path) if scan_action != "custom" or not os.path.isfile(custom_path) else custom_path
        file_ctime_dt = datetime.fromtimestamp(file_ctime, tz=VIETNAM_TZ)
        priority = 1 if scan_action == "custom" else 0
        relative_path = os.path.relpath(absolute_path, video_root) if scan_action != "custom" else os.path.dirname(absolute_path)
        camera_name = relative_path.split(os.sep)[0] if relative_path != "." else os.path.basename(video_root)
        insert_data.append((
            scan_action, days_val, custom_path, absolute_path, datetime.now(VIETNAM_TZ), file_ctime_dt, priority, camera_name, 'pending', 0
        ))

    with conn:
        cursor = conn.cursor()
        cursor.executemany('''
            INSERT INTO file_list (program_type, days, custom_path, file_path, created_at, ctime, priority, camera_name, status, is_processed)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', insert_data)
        logger.info(f"ÄÃ£ chÃ¨n {len(insert_data)} tá»‡p vÃ o file_list")

def list_files(video_root, scan_action, custom_path, days, db_path, default_scan_days=7, camera_ctime_map=None, is_initial_scan=False):
    try:
        with db_rwlock.gen_wlock():
            conn = get_db_connection()
            cursor = conn.cursor()

            if not os.path.exists(video_root):
                try:
                    os.makedirs(video_root, exist_ok=True)
                    logger.info(f"ÄÃ£ táº¡o thÆ° má»¥c video root: {video_root}")
                except Exception as e:
                    logger.error(f"KhÃ´ng thá»ƒ táº¡o thÆ° má»¥c video root: {video_root}, lá»—i: {str(e)}")
                    raise Exception(f"KhÃ´ng thá»ƒ táº¡o thÆ° má»¥c video root: {str(e)}")

            cursor.execute('SELECT MAX(ctime) FROM file_list')
            last_ctime = cursor.fetchone()[0]
            max_ctime = datetime.fromisoformat(last_ctime.replace('Z', '+00:00')) if last_ctime else datetime.min.replace(tzinfo=VIETNAM_TZ)

            cursor.execute("SELECT input_path, selected_cameras FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            if result:
                video_root = result[0]
                selected_cameras = json.loads(result[1]) if result[1] else []
            else:
                selected_cameras = []
            logger.info(f"Sá»­ dá»¥ng video_root: {video_root}, Camera Ä‘Æ°á»£c chá»n: {selected_cameras}")

            cursor.execute("SELECT working_days, from_time, to_time FROM general_info WHERE id = 1")
            general_info = cursor.fetchone()
            if general_info:
                try:
                    working_days_raw = general_info[0].encode('utf-8').decode('utf-8') if general_info[0] else ''
                    working_days = json.loads(working_days_raw) if working_days_raw else []
                except json.JSONDecodeError as e:
                    logger.error(f"JSON khÃ´ng há»£p lá»‡ trong working_days: {general_info[0]}, lá»—i: {e}")
                    working_days = []
                from_time = datetime.strptime(general_info[1], '%H:%M').time() if general_info[1] else None
                to_time = datetime.strptime(general_info[2], '%H:%M').time() if general_info[2] else None
            else:
                working_days, from_time, to_time = [], None, None
            logger.info(f"NgÃ y lÃ m viá»‡c: {working_days}, from_time: {from_time}, to_time: {to_time}")

            video_files = []
            file_ctimes = []

            if scan_action == "custom" and custom_path:
                if not os.path.exists(custom_path):
                    raise Exception(f"ÄÆ°á»ng dáº«n khÃ´ng tá»“n táº¡i: {custom_path}")
                if os.path.isfile(custom_path) and custom_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv')):
                    file_name = os.path.basename(custom_path)
                    file_ctime = get_file_creation_time(custom_path)
                    video_files.append(file_name)
                    file_ctimes.append(file_ctime.timestamp())
                    logger.info(f"TÃ¬m tháº¥y tá»‡p: {custom_path}")
                else:
                    video_files, file_ctimes = scan_files(
                        custom_path, video_root, None, None, False, None,
                        working_days, from_time, to_time, selected_cameras, strict_date_match=False
                    )
            elif scan_action == "first" and days:
                time_threshold = datetime.now(VIETNAM_TZ) - timedelta(days=days)
                video_files, file_ctimes = scan_files(
                    video_root, video_root, time_threshold, None, False, None,
                    working_days, from_time, to_time, selected_cameras, strict_date_match=False
                )
                cursor.execute('''
                    INSERT OR REPLACE INTO program_status (id, key, value)
                    VALUES ((SELECT id FROM program_status WHERE key = 'first_run_completed'), 'first_run_completed', 'true')
                ''')
                conn.commit()
            else:  # default
                time_threshold = datetime.now(VIETNAM_TZ) - timedelta(days=default_scan_days) if is_initial_scan else datetime.now(VIETNAM_TZ) - timedelta(days=1)
                restrict_to_current_date = not is_initial_scan
                video_files, file_ctimes = scan_files(
                    video_root, video_root, time_threshold, max_ctime, restrict_to_current_date, camera_ctime_map,
                    working_days, from_time, to_time, selected_cameras, strict_date_match=True
                )

            save_files_to_db(conn, video_files, file_ctimes, scan_action, days, custom_path, video_root)
            conn.close()
        logger.info(f"TÃ¬m tháº¥y {len(video_files)} tá»‡p video")
        return video_files, file_ctimes
    except Exception as e:
        logger.error(f"Lá»—i trong list_files: {e}")
        raise Exception(f"Lá»—i trong list_files: {str(e)}")

def cleanup_stale_jobs():
    try:
        with db_rwlock.gen_wlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE file_list 
                SET status = 'pending'
                WHERE status = 'Ä‘ang frame sampler ...'
                AND created_at < datetime('now', '-59 minutes')
            """)
            affected = cursor.rowcount
            conn.commit()
            conn.close()
            if affected > 0:
                logger.info(f"Reset {affected} stale jobs to pending")
    except Exception as e:
        logger.error(f"Error cleaning up stale jobs: {e}")

def run_file_scan(scan_action="default", days=None, custom_path=None):
    db_path = get_db_path()
    cleanup_stale_jobs()
    try:
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT input_path FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            conn.close()
            video_root = result[0] if result else ""
        if not video_root:
            raise Exception("KhÃ´ng tÃ¬m tháº¥y video_root trong processing_config")
        camera_ctime_map = {}
        is_initial_scan = scan_action == "default" and days is not None  # Äáº·t is_initial_scan=True cho láº§n quÃ©t Ä‘áº§u tiÃªn
        files, _ = list_files(video_root, scan_action, custom_path, days, db_path, camera_ctime_map=camera_ctime_map, is_initial_scan=is_initial_scan)
        return files
    except Exception as e:
        logger.error(f"Lá»—i trong run_file_scan: {e}")
        raise

```
## ğŸ“„ File: `db_sync.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/scheduler/db_sync.py`

```python
from readerwriterlock import rwlock
import threading

db_rwlock = rwlock.RWLockFairD()  # Sá»­ dá»¥ng RWLockFairD Ä‘á»ƒ trÃ¡nh deadlock
frame_sampler_event = threading.Event()
event_detector_event = threading.Event()
event_detector_done = threading.Event()
event_detector_done.set()  # Ban Ä‘áº§u cho phÃ©p Frame Sampler cháº¡y

```
## ğŸ“„ File: `system_monitor.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/scheduler/system_monitor.py`

```python
import psutil
import logging
import os
from datetime import datetime
from modules.config.logging_config import get_logger


# ÄÆ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i tá»« project root
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class SystemMonitor:
    def __init__(self):
        self.min_batch_size = 2
        self.max_batch_size = 6
        self.cpu_threshold_low = 70  # TÄƒng batch_size náº¿u CPU < 70%
        self.cpu_threshold_high = 90  # Giáº£m batch_size náº¿u CPU > 90%
        self.setup_logging()

    def setup_logging(self):
        self.logger = get_logger(__name__, {"module": "system_monitor"})
        self.logger.info("SystemMonitor logging initialized")

    def get_system_metrics(self):
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            logging.info(f"System metrics retrieved: CPU={cpu_percent}%, Memory={memory_percent}%")
            return cpu_percent, memory_percent
        except Exception as e:
            logging.error(f"Error getting system metrics: {str(e)}")
            return 50.0, 50.0  # GiÃ¡ trá»‹ máº·c Ä‘á»‹nh náº¿u lá»—i

    def get_batch_size(self, current_batch_size=2):
        logging.info(f"Calculating batch size, current={current_batch_size}")
        cpu_percent, memory_percent = self.get_system_metrics()
        if cpu_percent < self.cpu_threshold_low and memory_percent < self.cpu_threshold_low:
            new_batch_size = min(current_batch_size + 1, self.max_batch_size)
        elif cpu_percent > self.cpu_threshold_high or memory_percent > self.cpu_threshold_high:
            new_batch_size = max(current_batch_size - 1, self.min_batch_size)
        else:
            new_batch_size = current_batch_size
        logging.info(f"Calculated batch_size: {new_batch_size}")
        return new_batch_size

    def log_timeout_warning(self, timeout_files, total_files):
        logging.info(f"Checking timeout: {timeout_files}/{total_files} files")
        if timeout_files > total_files * 0.1:
            logging.warning(f"Warning: {timeout_files}/{total_files} files timed out, consider increasing resources")
        else:
            logging.info("No timeout warning triggered")

```
## ğŸ“„ File: `program_runner.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/scheduler/program_runner.py`

```python
import threading
import time
import os
import logging
from datetime import datetime
from modules.db_utils.db_utils import get_db_connection
from modules.technician.frame_sampler_trigger import FrameSamplerTrigger
from modules.technician.frame_sampler_no_trigger import FrameSamplerNoTrigger
from modules.technician.IdleMonitor import IdleMonitor
from modules.technician.event_detector import process_single_log
from .db_sync import db_rwlock, frame_sampler_event, event_detector_event, event_detector_done
import json
from modules.config.logging_config import  get_logger
import logging

logging.info("Logging initialized for program_runner")

# Biáº¿n táº¡m Ä‘á»ƒ lÆ°u tráº¡ng thÃ¡i cháº¡y vÃ  sá»‘ ngÃ y
running_state = {"current_running": None, "days": None, "custom_path": None, "files": []}
# Dictionary lÆ°u khÃ³a cho tá»«ng nhÃ³m video
video_locks = {}

def start_frame_sampler_thread(batch_size=1):
    logging.info(f"Starting {batch_size} frame sampler threads")
    threads = []
    for _ in range(batch_size):
        frame_sampler_thread = threading.Thread(target=run_frame_sampler)
        frame_sampler_thread.start()
        threads.append(frame_sampler_thread)
    return threads

def run_frame_sampler():
    logging.info("Frame sampler thread started")
    while True:  # VÃ²ng láº·p vÃ´ háº¡n Ä‘á»ƒ thread luÃ´n cháº¡y
        frame_sampler_event.wait()  # Chá» thÃ´ng bÃ¡o tá»« event
        logging.debug("Frame sampler event received")
        try:
            with db_rwlock.gen_rlock():  # Äá»“ng bá»™ hÃ³a truy cáº­p database
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("SELECT file_path, camera_name FROM file_list WHERE is_processed = 0 ORDER BY priority DESC, created_at ASC")
                video_files = [(row[0], row[1]) for row in cursor.fetchall()]
                conn.close()
                logging.info(f"Found {len(video_files)} unprocessed video files")

            if not video_files:
                logging.info("No video files to process, clearing event")
                frame_sampler_event.clear()  # XÃ³a event vÃ  tiáº¿p tá»¥c chá»
                continue

            for video_file, camera_name in video_files:
                # Kiá»ƒm tra tráº¡ng thÃ¡i video trÆ°á»›c khi xá»­ lÃ½
                with db_rwlock.gen_rlock():
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("SELECT status, is_processed FROM file_list WHERE file_path = ?", (video_file,))
                    result = cursor.fetchone()
                    conn.close()
                    if result and (result[0] == "Ä‘ang frame sampler ..." or result[1] == 1):
                        logging.info(f"Skipping video {video_file}: already being processed or completed")
                        continue

                # KhÃ³a theo video
                with db_rwlock.gen_wlock():
                    if video_file not in video_locks:
                        video_locks[video_file] = threading.Lock()
                video_lock = video_locks[video_file]
                if not video_lock.acquire(blocking=False):
                    logging.info(f"Skipping video {video_file}: locked by another thread")
                    continue

                try:
                    logging.info(f"Processing video: {video_file}")
                    # Kiá»ƒm tra qr_trigger_area vÃ  packing_area tá»« packing_profiles
                    with db_rwlock.gen_rlock():
                        conn = get_db_connection()
                        cursor = conn.cursor()
                        search_name = camera_name if camera_name else "CamTest"
                        if not camera_name:
                            logging.warning(f"No camera_name for {video_file}, falling back to CamTest")
                        cursor.execute("SELECT id, profile_name, qr_trigger_area, packing_area FROM packing_profiles WHERE profile_name LIKE ?", (f'%{search_name}%',))
                        profiles = cursor.fetchall()
                        conn.close()
                    
                    # Chá»n profile cÃ³ id lá»›n nháº¥t
                    trigger = [0, 0, 0, 0]
                    packing_area = None
                    selected_profile = None
                    if profiles:
                        selected_profile = max(profiles, key=lambda x: x[0])  # Chá»n id lá»›n nháº¥t
                        profile_id, profile_name, qr_trigger_area, packing_area_raw = selected_profile
                        # Parse qr_trigger_area
                        try:
                            trigger = json.loads(qr_trigger_area) if qr_trigger_area else [0, 0, 0, 0]
                            if not isinstance(trigger, list) or len(trigger) != 4:
                                logging.error(f"Invalid qr_trigger_area for {profile_name}: {qr_trigger_area}, using default [0, 0, 0, 0]")
                                trigger = [0, 0, 0, 0]
                        except json.JSONDecodeError as e:
                            logging.error(f"Failed to parse qr_trigger_area for {profile_name}: {e}, using default [0, 0, 0, 0]")
                            trigger = [0, 0, 0, 0]
                        # Parse packing_area
                        try:
                            if packing_area_raw:
                                parsed = json.loads(packing_area_raw)
                                if isinstance(parsed, list) and len(parsed) == 4:
                                    packing_area = tuple(parsed)
                                else:
                                    logging.error(f"Invalid packing_area format for {profile_name}: {packing_area_raw}, using default None")
                                    packing_area = None
                            logging.info(f"Selected profile id={profile_id}, profile_name={profile_name}, qr_trigger_area={trigger}, packing_area={packing_area}")
                        except (ValueError, json.JSONDecodeError, KeyError, TypeError) as e:
                            logging.error(f"Failed to parse packing_area for {profile_name}: {e}, using default None")
                            packing_area = None
                    else:
                        logging.warning(f"No profile found for camera {search_name}, using default qr_trigger_area=[0, 0, 0, 0], packing_area=None")
                    
                    # Cháº¡y IdleMonitor trÆ°á»›c FrameSampler, truyá»n packing_area
                    idle_monitor = IdleMonitor()
                    logging.info(f"Running IdleMonitor for {video_file}")
                    idle_monitor.process_video(video_file, camera_name, packing_area)
                    work_block_queue = idle_monitor.get_work_block_queue()

                    # bá» qua file khÃ´ng cÃ³ work block
                    if work_block_queue.empty():
                        logging.info(f"No work blocks found for {video_file}, skipping FrameSampler and log file creation")
                        with db_rwlock.gen_wlock():
                            conn = get_db_connection()
                            cursor = conn.cursor()
                            cursor.execute("UPDATE file_list SET status = ?, is_processed = 1 WHERE file_path = ?", ("xong", video_file))
                            conn.commit()
                            conn.close()
                        continue  # Bá» qua FrameSampler vÃ  chuyá»ƒn sang video tiáº¿p theo
                    # Chá»n FrameSampler dá»±a trÃªn trigger
                    if trigger != [0, 0, 0, 0]:
                        frame_sampler = FrameSamplerTrigger()
                        logging.info(f"Using FrameSamplerTrigger for {video_file}")
                    else:
                        frame_sampler = FrameSamplerNoTrigger()
                        logging.info(f"Using FrameSamplerNoTrigger for {video_file}")

                    with db_rwlock.gen_wlock():  # KhÃ³a khi cáº­p nháº­t tráº¡ng thÃ¡i
                        conn = get_db_connection()
                        cursor = conn.cursor()
                        cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("Ä‘ang frame sampler ...", video_file))
                        conn.commit()
                        conn.close()
                        logging.debug(f"Updated status for {video_file} to 'Ä‘ang frame sampler ...'")
                    
                    # Gá»i process_video vá»›i work block tá»« queue
                    log_file = None
                    while not work_block_queue.empty():
                        work_block = work_block_queue.get()
                        start_time = work_block['start_time']
                        end_time = work_block['end_time']
                        logging.info(f"Processing video block: start_time={start_time}, end_time={end_time}")
                        log_file = frame_sampler.process_video(
                            video_file,
                            video_lock=frame_sampler.video_lock,
                            get_packing_area_func=frame_sampler.get_packing_area,
                            process_frame_func=frame_sampler.process_frame,
                            frame_interval=frame_sampler.frame_interval,
                            start_time=start_time,
                            end_time=end_time
                        )
                    
                    with db_rwlock.gen_wlock():  # KhÃ³a khi cáº­p nháº­t tráº¡ng thÃ¡i cuá»‘i
                        conn = get_db_connection()
                        cursor = conn.cursor()
                        if log_file:
                            cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("xong", video_file))
                            event_detector_event.set()  # KÃ­ch hoáº¡t Event Detector sau má»—i video
                            logging.info(f"Video {video_file} processed successfully, log file: {log_file}")
                        else:
                            cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("lá»—i", video_file))
                            logging.error(f"Failed to process video {video_file}")
                        conn.commit()
                        conn.close()
                    
                    # Táº¡m dá»«ng sau khi xá»­ lÃ½ xong má»™t video
                    logging.info(f"Frame Sampler pausing after processing {video_file}, waiting for Event Detector...")
                    while not event_detector_done.is_set():
                        time.sleep(1)  # Chá» Event Detector hoÃ n táº¥t

                finally:
                    video_lock.release()
                    with db_rwlock.gen_wlock():
                        video_locks.pop(video_file, None)  # XÃ³a khÃ³a sau khi xá»­ lÃ½ xong
                    logging.debug(f"Released lock for {video_file}")

            frame_sampler_event.clear()  # XÃ³a event sau khi xá»­ lÃ½ háº¿t file
            logging.info("All video files processed, clearing frame sampler event")
        except Exception as e:
            logging.error(f"Error in Frame Sampler thread: {str(e)}")
            frame_sampler_event.clear()  # Äáº£m báº£o thread "ngá»§" láº¡i náº¿u cÃ³ lá»—i

def start_event_detector_thread():
    logging.info("Starting event detector thread")
    event_detector_thread = threading.Thread(target=run_event_detector)
    event_detector_thread.start()
    return event_detector_thread

def run_event_detector():
    logging.info("Event detector thread started")
    while True:
        event_detector_event.wait()
        logging.debug("Event detector event received")
        try:
            with db_rwlock.gen_rlock():  # Äá»“ng bá»™ hÃ³a truy cáº­p database
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("SELECT log_file FROM processed_logs WHERE is_processed = 0")
                log_files = [row[0] for row in cursor.fetchall()]
                conn.close()
                logging.info(f"Found {len(log_files)} unprocessed log files")

            if not log_files:
                event_detector_event.clear()
                event_detector_done.set()  # BÃ¡o hiá»‡u Frame Sampler tiáº¿p tá»¥c
                logging.info("No log files to process, clearing event and signaling done")
                continue

            for log_file in log_files:
                logging.info(f"Event Detector processing: {log_file}")
                process_single_log(log_file)
            event_detector_event.clear()
            event_detector_done.set()  # BÃ¡o hiá»‡u Frame Sampler tiáº¿p tá»¥c sau khi xá»­ lÃ½ háº¿t log
            logging.info("All log files processed, clearing event and signaling done")
        except Exception as e:
            logging.error(f"Error in Event Detector thread: {str(e)}")
            event_detector_event.clear()
            event_detector_done.set()  # Váº«n bÃ¡o hiá»‡u Frame Sampler tiáº¿p tá»¥c náº¿u cÃ³ lá»—i

```
## ğŸ“„ File: `program.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/scheduler/program.py`

```python
from flask import Blueprint, request, jsonify
import os
import json
import threading
import pytz
from datetime import datetime, timedelta
import logging
from modules.db_utils import find_project_root, get_db_connection
from .file_lister import run_file_scan, get_db_path
from .batch_scheduler import BatchScheduler
from .db_sync import frame_sampler_event, event_detector_event
import logging

VIETNAM_TZ = pytz.timezone('Asia/Ho_Chi_Minh')

program_bp = Blueprint('program', __name__)

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DB_PATH = os.path.join(BASE_DIR, "database", "events.db")
os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)

DB_PATH = get_db_path()
LOG_DIR = os.path.join(BASE_DIR, "../../resources/output_clips/LOG")
os.makedirs(LOG_DIR, exist_ok=True)

logger = logging.getLogger("app")
logger.info("Program logging initialized")

db_rwlock = threading.RLock()
running_state = {
    "days": None,
    "custom_path": None,
    "current_running": None,
    "files": []
}

scheduler = BatchScheduler()

def init_default_program():
    logger.info("Initializing default program")
    try:
        with db_rwlock:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('SELECT value FROM program_status WHERE key = "first_run_completed"')
            result = cursor.fetchone()
            conn.close()
        first_run_completed = result[0] == 'true' if result else False
        logger.info(f"First run completed: {first_run_completed}, Scheduler running: {scheduler.running}")
        if first_run_completed and not scheduler.running:
            logger.info("Chuyá»ƒn sang cháº¿ Ä‘á»™ cháº¡y máº·c Ä‘á»‹nh (quÃ©t láº·p)")
            running_state["current_running"] = "Máº·c Ä‘á»‹nh"
            scheduler.start()
    except Exception as e:
        logger.error(f"Error initializing default program: {e}")

init_default_program()

@program_bp.route('/program', methods=['POST'])
def program():
    logger.info(f"POST /program called, Current state before action: scheduler_running={scheduler.running}, running_state={running_state}")
    data = request.json
    card = data.get('card')
    action = data.get('action')
    custom_path = data.get('custom_path', '')
    days = data.get('days')

    if card == "Láº§n Ä‘áº§u" and action == "run":
        try:
            with db_rwlock:
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute('SELECT value FROM program_status WHERE key = "first_run_completed"')
                result = cursor.fetchone()
                first_run_completed = result[0] == 'true' if result else False
                conn.close()
            if first_run_completed:
                logger.warning("First run already completed")
                return jsonify({"error": "Láº§n Ä‘áº§u Ä‘Ã£ cháº¡y trÆ°á»›c Ä‘Ã³, khÃ´ng thá»ƒ cháº¡y láº¡i"}), 400
        except Exception as e:
            logger.error(f"Failed to check first run status: {str(e)}")
            return jsonify({"error": f"Failed to check first run status: {str(e)}"}), 500

    if action == "run":
        logger.info(f"Action run for card: {card}, scheduler_running={scheduler.running}")
        if scheduler.running and card == "Chá»‰ Ä‘á»‹nh":
            scheduler.pause()
            running_state["current_running"] = None
            running_state["files"] = []
        if card == "Láº§n Ä‘áº§u":
            if not days:
                logger.error("Days required for Láº§n Ä‘áº§u")
                return jsonify({"error": "Days required for Láº§n Ä‘áº§u"}), 400
            running_state["days"] = days
            running_state["custom_path"] = None
            try:
                run_file_scan(scan_action="first", days=days)
            except Exception as e:
                logger.error(f"Failed to run first scan: {str(e)}")
                return jsonify({"error": f"Failed to run first scan: {str(e)}"}), 500
        elif card == "Chá»‰ Ä‘á»‹nh":
            if not custom_path:
                logger.error("Custom path required for Chá»‰ Ä‘á»‹nh")
                return jsonify({"error": "Custom path required cho Chá»‰ Ä‘á»‹nh"}), 400
            abs_path = os.path.abspath(custom_path)
            if not os.path.exists(abs_path):
                logger.error(f"Custom path {abs_path} does not exist")
                return jsonify({"error": f"Custom path {abs_path} does not exist"}), 400
            try:
                with db_rwlock:
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("SELECT status, is_processed FROM file_list WHERE file_path = ? AND (status = 'xong' OR is_processed = 1)", (abs_path,))
                    result = cursor.fetchone()
                    conn.close()
                    if result:
                        logger.warning(f"File {abs_path} already processed with status {result[0]}")
                        return jsonify({"error": "File Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½"}), 400
            except Exception as e:
                logger.error(f"Error checking file status: {str(e)}")
                return jsonify({"error": f"Error checking file status: {str(e)}"}), 500
            running_state["custom_path"] = abs_path
            running_state["days"] = None
            try:
                scheduler.pause()
                run_file_scan(scan_action="custom", custom_path=abs_path)
                with db_rwlock:
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("SELECT file_path FROM file_list WHERE custom_path = ? AND status = 'pending' ORDER BY created_at DESC LIMIT 1", (abs_path,))
                    result = cursor.fetchone()
                    conn.close()
                if result:
                    from .program_runner import start_frame_sampler_thread, start_event_detector_thread
                    frame_sampler_event.set()
                    start_frame_sampler_thread()
                    start_event_detector_thread()
                    logger.info(f"[Chá»‰ Ä‘á»‹nh] Processing started: {result[0]}")
                    import time
                    while True:
                        with db_rwlock:
                            conn = get_db_connection()
                            cursor = conn.cursor()
                            cursor.execute("SELECT status FROM file_list WHERE file_path = ?", (result[0],))
                            status_result = cursor.fetchone()
                            conn.close()
                        if status_result and status_result[0] == 'xong':
                            break
                        time.sleep(2)
                    logger.info(f"[Chá»‰ Ä‘á»‹nh] Processing completed: {result[0]}")
                    scheduler.resume()
                    try:
                        if not scheduler.running:
                            scheduler.start()
                            logger.info("Restarted scheduler for default mode")
                        run_file_scan(scan_action="default")
                        frame_sampler_event.set()
                        event_detector_event.set()
                        logger.info(f"Completed Chá»‰ Ä‘á»‹nh, transitioning to default: scheduler_running={scheduler.running}, running_state={running_state}")
                    except Exception as e:
                        logger.error(f"Error triggering default scan: {str(e)}")
                else:
                    logger.error(f"[Chá»‰ Ä‘á»‹nh] No pending file found at: {abs_path}")
                    return jsonify({"error": "KhÃ´ng tÃ¬m tháº¥y file pending Ä‘á»ƒ xá»­ lÃ½"}), 404
            except Exception as e:
                logger.error(f"[Chá»‰ Ä‘á»‹nh] Error: {str(e)}")
                scheduler.resume()
                return jsonify({"error": f"Xá»­ lÃ½ chá»‰ Ä‘á»‹nh tháº¥t báº¡i: {str(e)}"}), 500
        else:
            running_state["days"] = None
            running_state["custom_path"] = None

        running_state["current_running"] = card
        if not scheduler.running:
            running_state["current_running"] = "Máº·c Ä‘á»‹nh"
            scheduler.start()

        if card == "Láº§n Ä‘áº§u":
            try:
                with db_rwlock:
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("UPDATE program_status SET value = ? WHERE key = ?", ("true", "first_run_completed"))
                    conn.commit()
                    conn.close()
                logger.info("Chuyá»ƒn sang cháº¿ Ä‘á»™ cháº¡y máº·c Ä‘á»‹nh (quÃ©t láº·p) sau khi hoÃ n thÃ nh Láº§n Ä‘áº§u")
            except Exception as e:
                logger.error(f"Error updating first_run_completed: {e}")

    elif action == "stop":
        logger.info(f"Action stop called, current_state={running_state}, scheduler_running={scheduler.running}")
        running_state["current_running"] = None
        running_state["days"] = None
        running_state["custom_path"] = None
        running_state["files"] = []
        if not scheduler.running:
            scheduler.start()
            logger.info("Scheduler restarted for default mode")
        logger.info(f"State after stop: running_state={running_state}, scheduler_running={scheduler.running}")

    logger.info(f"Program action completed: {card} {action}, final_state={running_state}, scheduler_running={scheduler.running}")
    return jsonify({
        "current_running": running_state["current_running"],
        "days": running_state.get("days"),
        "custom_path": running_state.get("custom_path")
    }), 200

@program_bp.route('/program', methods=['GET'])
def get_program_status():
    logger.info("GET /program called")
    return jsonify({
        "current_running": running_state["current_running"],
        "days": running_state.get("days"),
        "custom_path": running_state.get("custom_path")
    }), 200

@program_bp.route('/confirm-run', methods=['POST'])
def confirm_run():
    logger.info("POST /confirm-run called")
    data = request.json
    card = data.get('card')

    scan_action = "first" if card == "Láº§n Ä‘áº§u" else "default" if card == "Máº·c Ä‘á»‹nh" else "custom"
    days = running_state.get("days") if card == "Láº§n Ä‘áº§u" else None
    custom_path = running_state.get("custom_path", '')
    try:
        run_file_scan(scan_action=scan_action, days=days, custom_path=custom_path)
        logger.info(f"Files queued for {scan_action}")
    except Exception as e:
        logger.error(f"Failed to list files: {str(e)}")
        return jsonify({"error": f"Failed to list files: {str(e)}"}), 500

    return jsonify({
        "program_type": scan_action
    }), 200

@program_bp.route('/program-progress', methods=['GET'])
def get_program_progress():
    logger.info("GET /program-progress called")
    try:
        with db_rwlock:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT file_path, status FROM file_list WHERE is_processed = 0 ORDER BY created_at DESC")
            files_status = [{"file": row[0], "status": row[1]} for row in cursor.fetchall()]
            conn.close()
        logger.info(f"Retrieved {len(files_status)} files for status")
        return jsonify({"files": files_status}), 200
    except Exception as e:
        logger.error(f"Failed to retrieve program progress: {str(e)}")
        return jsonify({"error": f"Failed to retrieve program progress: {str(e)}"}), 500

@program_bp.route('/check-first-run', methods=['GET'])
def check_first_run():
    logger.info("GET /check-first-run called")
    try:
        with db_rwlock:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('SELECT value FROM program_status WHERE key = "first_run_completed"')
            result = cursor.fetchone()
            conn.close()
            first_run_completed = result[0] == 'true' if result else False
        logger.info(f"First run completed: {first_run_completed}")
        return jsonify({"first_run_completed": first_run_completed}), 200
    except Exception as e:
        logger.error(f"Failed to check first run status: {str(e)}")
        return jsonify({"error": f"Failed to check first run status: {str(e)}"}), 500

@program_bp.route('/get-cameras', methods=['GET'])
def get_cameras():
    logger.info("GET /get-cameras called")
    try:
        with db_rwlock:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT selected_cameras FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            conn.close()
            cameras = result[0] if result else "[]"
            cameras_list = json.loads(cameras) if cameras else []
        logger.info(f"Retrieved {len(cameras_list)} cameras")
        return jsonify({"cameras": cameras_list}), 200
    except Exception as e:
        logger.error(f"Failed to retrieve cameras: {str(e)}")
        return jsonify({"error": f"Failed to retrieve cameras: {str(e)}"}), 500

@program_bp.route('/get-camera-folders', methods=['GET'])
def get_camera_folders():
    logger.info("GET /get-camera-folders called")
    try:
        with db_rwlock:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT input_path FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            video_root = result[0] if result else os.path.join(BASE_DIR, "Inputvideo")
            conn.close()

        if not os.path.exists(video_root):
            logger.error(f"Directory {video_root} does not exist")
            return jsonify({"error": f"Directory {video_root} does not exist. Ensure the path is correct or create the directory."}), 400

        folders = []
        for folder_name in os.listdir(video_root):
            folder_path = os.path.join(video_root, folder_name)
            if os.path.isdir(folder_path):
                folders.append({"name": folder_name, "path": folder_path})
        logger.info(f"Retrieved {len(folders)} camera folders")
        return jsonify({"folders": folders}), 200
    except Exception as e:
        logger.error(f"Failed to retrieve camera folders: {str(e)}")
        return jsonify({"error": f"Failed to retrieve camera folders: {str(e)}"}), 500

if __name__ == "__main__":
    logger.info("Main program started")
    if not scheduler.running:
        running_state["current_running"] = "Máº·c Ä‘á»‹nh"
        scheduler.start()

```
## ğŸ“„ File: `batch_scheduler.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/scheduler/batch_scheduler.py`

```python
import os
import threading
import time
import logging
import sqlite3
import pytz
import psutil  # THÃŠM IMPORT Má»šI
from datetime import datetime, timedelta
from modules.db_utils import get_db_connection
from .db_sync import db_rwlock, frame_sampler_event, event_detector_event
from .file_lister import run_file_scan
from .program_runner import start_frame_sampler_thread, start_event_detector_thread
import logging

logger = logging.getLogger("app")
logger.info("BatchScheduler logging initialized")

# Cáº¥u hÃ¬nh mÃºi giá» Viá»‡t Nam
VIETNAM_TZ = pytz.timezone('Asia/Ho_Chi_Minh')

class SystemMonitor:
    """Theo dÃµi tÃ i nguyÃªn há»‡ thá»‘ng vÃ  tÃ­nh batch_size Ä‘á»™ng."""
    def __init__(self):
        self.cpu_threshold_low = 70
        self.cpu_threshold_high = 90
        self.base_batch_size = 2
        self.max_batch_size = 6

    def get_cpu_usage(self):
        """Láº¥y CPU usage thá»±c táº¿ tá»« há»‡ thá»‘ng"""
        try:
            # Láº¥y CPU usage trung bÃ¬nh trong 1 giÃ¢y
            cpu_percent = psutil.cpu_percent(interval=1)
            logger.debug(f"Current CPU usage: {cpu_percent}%")
            return cpu_percent
        except Exception as e:
            logger.error(f"Error getting CPU usage: {e}")
            return 50  # Fallback value náº¿u cÃ³ lá»—i

    def get_memory_usage(self):
        """Láº¥y memory usage thá»±c táº¿"""
        try:
            memory_percent = psutil.virtual_memory().percent
            logger.debug(f"Current memory usage: {memory_percent}%")
            return memory_percent
        except Exception as e:
            logger.error(f"Error getting memory usage: {e}")
            return 50  # Fallback value

    def get_batch_size(self, current_batch_size):
        cpu_usage = self.get_cpu_usage()
        memory_usage = self.get_memory_usage()
        
        logger.debug(f"Resource usage - CPU: {cpu_usage}%, Memory: {memory_usage}%")
        
        # Kiá»ƒm tra náº¿u tÃ i nguyÃªn quÃ¡ táº£i
        if cpu_usage > self.cpu_threshold_high or memory_usage > 85:
            if current_batch_size > self.base_batch_size:
                new_batch_size = current_batch_size - 1
                logger.warning(f"High resource usage (CPU: {cpu_usage}%, RAM: {memory_usage}%), reducing batch size: {current_batch_size} -> {new_batch_size}")
                return new_batch_size
        
        # Kiá»ƒm tra náº¿u tÃ i nguyÃªn nhÃ n rá»—i
        elif cpu_usage < self.cpu_threshold_low and memory_usage < 70:
            if current_batch_size < self.max_batch_size:
                new_batch_size = current_batch_size + 1
                logger.info(f"Low resource usage (CPU: {cpu_usage}%, RAM: {memory_usage}%), increasing batch size: {current_batch_size} -> {new_batch_size}")
                return new_batch_size
        
        # Giá»¯ nguyÃªn náº¿u tÃ i nguyÃªn á»•n Ä‘á»‹nh
        return current_batch_size

    def log_system_info(self):
        """Log thÃ´ng tin há»‡ thá»‘ng khi khá»Ÿi Ä‘á»™ng"""
        try:
            cpu_count = psutil.cpu_count()
            memory_total = psutil.virtual_memory().total / (1024**3)  # GB
            logger.info(f"System info - CPU cores: {cpu_count}, Total RAM: {memory_total:.1f}GB")
            logger.info(f"Batch size config - Base: {self.base_batch_size}, Max: {self.max_batch_size}")
            logger.info(f"CPU thresholds - Low: {self.cpu_threshold_low}%, High: {self.cpu_threshold_high}%")
        except Exception as e:
            logger.error(f"Error logging system info: {e}")

class BatchScheduler:
    def __init__(self):
        self.batch_size = 2
        self.sys_monitor = SystemMonitor()
        self.scan_interval = 60
        self.timeout_seconds = 3600
        self.running = False
        self.queue_limit = 5
        self.sampler_threads = []
        self.detector_thread = None
        self.pause_event = threading.Event()
        self.pause_event.set()

    def pause(self):
        logger.info(f"BatchScheduler paused, current_batch_size={self.batch_size}")
        self.pause_event.clear()

    def resume(self):
        logger.info(f"BatchScheduler resumed, current_batch_size={self.batch_size}")
        self.pause_event.set()

    def get_pending_files(self):
        """Láº¥y danh sÃ¡ch file chÆ°a xá»­ lÃ½, giá»›i háº¡n queue_limit."""
        try:
            with db_rwlock.gen_rlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("SELECT file_path, camera_name FROM file_list WHERE status = 'pending' AND is_processed = 0 ORDER BY priority DESC, created_at ASC LIMIT ?", 
                             (self.queue_limit,))
                files = [(row[0], row[1]) for row in cursor.fetchall()]
                conn.close()
            return files
        except Exception as e:
            logger.error(f"Error retrieving pending files: {e}")
            return []

    def update_file_status(self, file_path, status):
        """Cáº­p nháº­t tráº¡ng thÃ¡i file trong file_list."""
        try:
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("UPDATE file_list SET status = ?, is_processed = ? WHERE file_path = ?",
                             (status, 1 if status in ['xong', 'lá»—i', 'timeout'] else 0, file_path))
                conn.commit()
                conn.close()
        except Exception as e:
            logger.error(f"Error updating file status for {file_path}: {e}")

    def check_timeout(self):
        """Kiá»ƒm tra vÃ  cáº­p nháº­t tráº¡ng thÃ¡i timeout cho file quÃ¡ 900s."""
        try:
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("SELECT file_path, created_at FROM file_list WHERE status = 'Ä‘ang frame sampler ...'")
                for row in cursor.fetchall():
                    created_at = datetime.fromisoformat(row[1].replace('Z', '+00:00')) if row[1] else datetime.min.replace(tzinfo=VIETNAM_TZ)
                    if (datetime.now(VIETNAM_TZ) - created_at).total_seconds() > self.timeout_seconds:
                        cursor.execute("UPDATE file_list SET status = ?, is_processed = 1 WHERE file_path = ?", 
                                     ('timeout', row[0]))
                        logger.warning(f"Timeout processing {row[0]} after {self.timeout_seconds}s")
                conn.commit()
                conn.close()
        except Exception as e:
            logger.error(f"Error checking timeout: {e}")

    def scan_files(self):
        """QuÃ©t file má»›i Ä‘á»‹nh ká»³ (15 phÃºt)."""
        logger.info("Báº¯t Ä‘áº§u quÃ©t láº·p")
        while self.running:
            try:
                logger.debug("Kiá»ƒm tra quÃ©t láº·p, running=%s, paused=%s", self.running, not self.pause_event.is_set())
                self.pause_event.wait()
                with db_rwlock.gen_rlock():
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("SELECT COUNT(*) FROM file_list WHERE status = 'pending' AND is_processed = 0")
                    pending_count = cursor.fetchone()[0]
                    conn.close()

                if pending_count >= self.queue_limit:
                    logger.warning(f"Queue full ({pending_count}/{self.queue_limit}), skipping file scan")
                else:
                    run_file_scan("default")
                    frame_sampler_event.set()
                time.sleep(self.scan_interval)
            except Exception as e:
                logger.error(f"Error in file scan: {e}")

    def run_batch(self):
        """Cháº¡y batch xá»­ lÃ½ file, sá»­ dá»¥ng run_frame_sampler."""
        while self.running:
            try:
                self.pause_event.wait()
                self.batch_size = self.sys_monitor.get_batch_size(self.batch_size)

                if not self.sampler_threads or len(self.sampler_threads) != self.batch_size:
                    for thread in self.sampler_threads:
                        if thread.is_alive():
                            thread.join(timeout=1)
                    self.sampler_threads = start_frame_sampler_thread(self.batch_size)
                    logger.info(f"Started {self.batch_size} frame sampler threads")

                if not self.detector_thread or not self.detector_thread.is_alive():
                    self.detector_thread = start_event_detector_thread()
                    logger.info("Started event detector thread")

                self.check_timeout()

                files = self.get_pending_files()
                if not files:
                    frame_sampler_event.clear()
                    frame_sampler_event.wait()
                    continue

                frame_sampler_event.set()
                event_detector_event.set()
                time.sleep(60)
            except Exception as e:
                logger.error(f"Error in batch processing: {e}")

    def start(self):
        """Khá»Ÿi Ä‘á»™ng BatchScheduler."""
        if not self.running:
            # Log system info khi khá»Ÿi Ä‘á»™ng
            self.sys_monitor.log_system_info()
            
            self.running = True
            days = [(datetime.now(VIETNAM_TZ) - timedelta(days=i)).date().isoformat() for i in range(6, -1, -1)]
            logger.info(f"Initial scan for days: {days}")
            try:
                run_file_scan("default", days=days)
                frame_sampler_event.set()
            except Exception as e:
                logger.error(f"Initial scan failed: {e}")

            scan_thread = threading.Thread(target=self.scan_files)
            batch_thread = threading.Thread(target=self.run_batch)
            scan_thread.start()
            batch_thread.start()
            logger.info(f"BatchScheduler started, batch_size={self.batch_size}, scan_interval={self.scan_interval}")

    def stop(self):
        """Dá»«ng BatchScheduler má»™t cÃ¡ch an toÃ n."""
        if not self.running:
            return  # ÄÃ£ dá»«ng rá»“i
            
        logger.info("Stopping BatchScheduler...")
        self.running = False
        
        # Clear events Ä‘á»ƒ cÃ¡c thread cÃ³ thá»ƒ thoÃ¡t
        frame_sampler_event.clear()
        event_detector_event.clear()
        
        # Äáº·t pause_event Ä‘á»ƒ cÃ¡c thread khÃ´ng bá»‹ block
        self.pause_event.set()
        
        # Dá»«ng sampler threads vá»›i timeout ngáº¯n
        for i, thread in enumerate(self.sampler_threads):
            if thread and thread.is_alive():
                try:
                    thread.join(timeout=0.5)  # Timeout ngáº¯n
                    if thread.is_alive():
                        logger.warning(f"Sampler thread {i} did not stop gracefully")
                except Exception as e:
                    logger.warning(f"Error stopping sampler thread {i}: {e}")
        
        # Dá»«ng detector thread
        if self.detector_thread and self.detector_thread.is_alive():
            try:
                self.detector_thread.join(timeout=0.5)
                if self.detector_thread.is_alive():
                    logger.warning("Detector thread did not stop gracefully")
            except Exception as e:
                logger.warning(f"Error stopping detector thread: {e}")
        
        logger.info("BatchScheduler stopped")
```
## ğŸ“„ File: `file_parser.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/utils/file_parser.py`

```python
import pandas as pd
import base64
import os
from io import BytesIO
import logging
import csv

# Thiáº¿t láº­p logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

def force_split_excel_text(df: pd.DataFrame) -> pd.DataFrame:
    if df.shape[1] == 1 and isinstance(df.columns[0], str) and "\t" in df.columns[0]:
        raw = df.to_csv(index=False, header=False)
        reader = csv.reader(raw.splitlines(), delimiter='\t')
        rows = list(reader)
        df_fixed = pd.DataFrame(rows[1:], columns=rows[0])
        logger.debug(f"[DEBUG] After force_split_excel_text - df.columns: {df_fixed.columns.tolist()}")
        logger.debug(f"[DEBUG] After force_split_excel_text - df.shape: {df_fixed.shape}")
        return df_fixed
    return df

def parse_uploaded_file(file_content: str = None, file_path: str = None, is_excel: bool = False) -> pd.DataFrame:
    """
    Decode base64-encoded file content or read from file path and return a pandas DataFrame.
    Supports both CSV and Excel formats.
    Raises detailed exceptions instead of falling back silently.
    """
    logger.debug(f"parse_uploaded_file called with is_excel={is_excel}, file_content={'provided' if file_content else 'not provided'}, file_path={file_path}")

    if not file_content and not file_path:
        raise ValueError("Either file_content or file_path must be provided.")

    if file_content:
        try:
            file_bytes = base64.b64decode(file_content)
        except Exception as e:
            raise ValueError(f"Failed to decode base64 content: {e}")
        buffer = BytesIO(file_bytes)
    elif file_path:
        if not os.path.exists(file_path):
            raise ValueError(f"File not found at path: {file_path}")
        buffer = file_path  # pandas can read directly from file path

    try:
        if is_excel:
            logger.debug("Reading file as Excel (pd.read_excel)")
            try:
                df = pd.read_excel(buffer, header=None, engine='openpyxl')  # XÃ³a encoding
            except Exception as e:
                logger.debug(f"Failed to read Excel with pd.read_excel: {str(e)}")
                logger.debug("Falling back to pd.read_csv")
                buffer.seek(0)  # Reset con trá» file Ä‘á»ƒ Ä‘á»c láº¡i
                df = pd.read_csv(buffer, sep=",", encoding='utf-8-sig', engine='python')
            logger.debug(f"[DEBUG] Raw DataFrame before processing: {df.to_dict()}")
            logger.debug(f"[DEBUG] Raw first row (potential header): {df.iloc[0].tolist()}")
            if df.shape[0] > 1:
                df.columns = df.iloc[0].values.tolist()
                df = df[1:]
                df = force_split_excel_text(df)  # Fix náº¿u dÃ­nh lá»—i tab
            else:
                raise ValueError("Excel file missing header/data")
            logger.debug(f"[DEBUG] df.columns: {df.columns.tolist()}")
            logger.debug(f"[DEBUG] df.shape: {df.shape}")
            logger.debug(f"[DEBUG] df.head(2): {df.head(2).to_dict()}")
            return df
        else:
            logger.debug("Reading file as CSV (pd.read_csv)")
            try:
                df = pd.read_csv(buffer, sep=",", encoding='latin1', engine='python')  # Thá»­ dáº¥u pháº©y trÆ°á»›c
                # Kiá»ƒm tra náº¿u chá»‰ cÃ³ 1 cá»™t vÃ  cá»™t Ä‘Ã³ chá»©a dáº¥u ;
                if len(df.columns) == 1 and df.columns[0] and isinstance(df.columns[0], str) and ";" in df.columns[0]:
                    logger.debug("Detected single column with semicolon, retrying with sep=';'")
                    if file_content:
                        buffer.seek(0)  # Reset con trá» file Ä‘á»ƒ Ä‘á»c láº¡i náº¿u dÃ¹ng file_content
                    df = pd.read_csv(buffer, sep=";", encoding='latin1', engine='python')
                return df
            except pd.errors.ParserError:
                if file_content:
                    buffer.seek(0)  # Reset con trá» file Ä‘á»ƒ Ä‘á»c láº¡i náº¿u dÃ¹ng file_content
                return pd.read_csv(buffer, sep=";", encoding='latin1', engine='python')  # Náº¿u lá»—i, thá»­ dáº¥u cháº¥m pháº©y
    except Exception as e:
        file_type = 'Excel' if is_excel else 'CSV'
        raise ValueError(f"Failed to read {file_type} file: {e}")
```
## ğŸ“„ File: `__init__.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/utils/__init__.py`

```python

```
## ğŸ“„ File: `path_validator.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/utils/path_validator.py`

```python
# backend/modules/utils/path_validator.py
import os
import shutil
import json
import logging
from typing import Dict, List, Tuple
from pathlib import Path
import stat

logger = logging.getLogger(__name__)

class PathValidator:
    """
    Path validation and directory management for VTrack video sources
    Handles directory creation, permission checks, and disk space validation
    """
    
    def __init__(self, base_path: str = None):
        """
        Initialize PathValidator with base path for video storage
        
        Args:
            base_path: Base directory for all video sources (default: project root)
        """
        if base_path is None:
            # Default to project root + storage directories
            from modules.db_utils import find_project_root
            project_root = find_project_root(os.path.abspath(__file__))
            base_path = project_root
        
        self.base_path = Path(base_path)
        self.logger = logging.getLogger(__name__)
        
        # Default storage directories
        self.nvr_downloads_dir = self.base_path / "nvr_downloads"
        self.cloud_sync_dir = self.base_path / "cloud_sync"
        self.output_clips_dir = self.base_path / "output_clips"
        
        self.logger.info(f"PathValidator initialized with base_path: {self.base_path}")
    
    def validate_source_path(self, source_type: str, source_name: str) -> Dict:
        """
        Validate and prepare working path for a video source
        
        Args:
            source_type: Type of source ('nvr', 'local', 'cloud')
            source_name: Unique name for the source
            
        Returns:
            Dict with validation results and working path
        """
        try:
            self.logger.info(f"ğŸ” Validating source path: {source_type}/{source_name}")
            
            # Determine working directory based on source type
            if source_type == 'nvr':
                working_path = self.nvr_downloads_dir / source_name
            elif source_type == 'cloud':
                working_path = self.cloud_sync_dir / source_name
            elif source_type == 'local':
                # Local sources use their own paths, no validation needed
                return {
                    'success': True,
                    'working_path': None,
                    'message': 'Local source uses existing path',
                    'disk_space_gb': self._get_disk_space_gb(str(self.base_path)),
                    'permissions': 'read-only'
                }
            else:
                return {
                    'success': False,
                    'message': f'Unknown source type: {source_type}',
                    'working_path': None
                }
            
            # Create working directory if it doesn't exist
            working_path.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"âœ… Working directory ready: {working_path}")
            
            # Validate permissions and disk space
            permissions_check = self.check_permissions(str(working_path))
            disk_check = self.check_disk_space(str(working_path), required_gb=1.0)
            
            if not permissions_check['writable']:
                return {
                    'success': False,
                    'message': f'Directory not writable: {working_path}',
                    'working_path': str(working_path),
                    'permissions': permissions_check
                }
            
            if not disk_check['sufficient']:
                return {
                    'success': False,
                    'message': f'Insufficient disk space: {disk_check["available_gb"]:.1f} GB available',
                    'working_path': str(working_path),
                    'disk_space': disk_check
                }
            
            return {
                'success': True,
                'working_path': str(working_path),
                'message': f'Source path validated successfully',
                'disk_space_gb': disk_check['available_gb'],
                'permissions': 'read-write',
                'created_directories': [str(working_path)]
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Path validation failed: {e}")
            return {
                'success': False,
                'message': f'Path validation error: {str(e)}',
                'working_path': None
            }
    
    def create_camera_directories(self, source_path: str, camera_names: List[str]) -> Dict:
        """
        Create individual directories for each camera under source path
        
        Args:
            source_path: Working directory for the source
            camera_names: List of camera names to create directories for
            
        Returns:
            Dict with created directories and camera path mapping
        """
        try:
            self.logger.info(f"ğŸ“ Creating camera directories in: {source_path}")
            self.logger.info(f"ğŸ“¹ Cameras: {camera_names}")
            
            if not camera_names:
                return {
                    'success': True,
                    'camera_paths': {},
                    'created_directories': [],
                    'message': 'No cameras to create directories for'
                }
            
            source_dir = Path(source_path)
            if not source_dir.exists():
                source_dir.mkdir(parents=True, exist_ok=True)
                self.logger.info(f"âœ… Created source directory: {source_dir}")
            
            camera_paths = {}
            created_directories = []
            
            for camera_name in camera_names:
                # Sanitize camera name for file system
                safe_name = self._sanitize_directory_name(camera_name)
                camera_dir = source_dir / safe_name
                
                # Create camera directory
                camera_dir.mkdir(parents=True, exist_ok=True)
                
                # Store mapping
                camera_paths[camera_name] = str(camera_dir)
                created_directories.append(str(camera_dir))
                
                self.logger.info(f"âœ… Created camera directory: {camera_name} â†’ {camera_dir}")
            
            return {
                'success': True,
                'camera_paths': camera_paths,
                'created_directories': created_directories,
                'message': f'Created {len(camera_paths)} camera directories',
                'total_cameras': len(camera_names)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Camera directory creation failed: {e}")
            return {
                'success': False,
                'message': f'Camera directory creation error: {str(e)}',
                'camera_paths': {},
                'created_directories': []
            }
    
    def check_disk_space(self, path: str, required_gb: float = 1.0) -> Dict:
        """
        Check available disk space at given path
        
        Args:
            path: Path to check disk space for
            required_gb: Minimum required space in GB
            
        Returns:
            Dict with disk space information
        """
        try:
            # Get disk usage statistics
            statvfs = shutil.disk_usage(path)
            
            # Convert to GB
            total_gb = statvfs.total / (1024**3)
            used_gb = (statvfs.total - statvfs.free) / (1024**3)
            available_gb = statvfs.free / (1024**3)
            
            usage_percent = (used_gb / total_gb) * 100
            sufficient = available_gb >= required_gb
            
            return {
                'sufficient': sufficient,
                'available_gb': available_gb,
                'used_gb': used_gb,
                'total_gb': total_gb,
                'usage_percent': usage_percent,
                'required_gb': required_gb,
                'path': path
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Disk space check failed: {e}")
            return {
                'sufficient': False,
                'available_gb': 0,
                'error': str(e),
                'path': path
            }
    
    def check_permissions(self, path: str) -> Dict:
        """
        Check read/write permissions for given path
        
        Args:
            path: Path to check permissions for
            
        Returns:
            Dict with permission information
        """
        try:
            path_obj = Path(path)
            
            # Check if path exists
            if not path_obj.exists():
                # Try to create it to test permissions
                try:
                    path_obj.mkdir(parents=True, exist_ok=True)
                    created = True
                except Exception:
                    return {
                        'readable': False,
                        'writable': False,
                        'executable': False,
                        'error': 'Cannot create directory',
                        'path': path
                    }
            else:
                created = False
            
            # Test permissions
            readable = os.access(path, os.R_OK)
            writable = os.access(path, os.W_OK)
            executable = os.access(path, os.X_OK)
            
            # Get file mode
            try:
                stat_info = path_obj.stat()
                file_mode = stat.filemode(stat_info.st_mode)
            except Exception:
                file_mode = 'unknown'
            
            return {
                'readable': readable,
                'writable': writable,
                'executable': executable,
                'file_mode': file_mode,
                'created': created,
                'path': path
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Permission check failed: {e}")
            return {
                'readable': False,
                'writable': False,
                'executable': False,
                'error': str(e),
                'path': path
            }
    
    def get_camera_paths(self, source_path: str, camera_names: List[str]) -> Dict:
        """
        Get mapping of camera names to their directory paths
        
        Args:
            source_path: Working directory for the source
            camera_names: List of camera names
            
        Returns:
            Dict mapping camera names to directory paths
        """
        try:
            source_dir = Path(source_path)
            camera_paths = {}
            
            for camera_name in camera_names:
                safe_name = self._sanitize_directory_name(camera_name)
                camera_dir = source_dir / safe_name
                camera_paths[camera_name] = str(camera_dir)
            
            return {
                'success': True,
                'camera_paths': camera_paths,
                'source_path': source_path
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Get camera paths failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'camera_paths': {}
            }
    
    def cleanup_unused_directories(self, source_path: str, active_cameras: List[str]) -> Dict:
        """
        Remove directories for cameras that are no longer active
        
        Args:
            source_path: Working directory for the source
            active_cameras: List of currently active camera names
            
        Returns:
            Dict with cleanup results
        """
        try:
            self.logger.info(f"ğŸ§¹ Cleaning up unused directories in: {source_path}")
            
            source_dir = Path(source_path)
            if not source_dir.exists():
                return {
                    'success': True,
                    'removed_directories': [],
                    'message': 'Source directory does not exist'
                }
            
            # Get all existing directories
            existing_dirs = [d for d in source_dir.iterdir() if d.is_dir()]
            
            # Sanitize active camera names
            active_safe_names = [self._sanitize_directory_name(name) for name in active_cameras]
            
            removed_directories = []
            
            for dir_path in existing_dirs:
                if dir_path.name not in active_safe_names:
                    try:
                        shutil.rmtree(dir_path)
                        removed_directories.append(str(dir_path))
                        self.logger.info(f"ğŸ—‘ï¸ Removed unused directory: {dir_path}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Failed to remove directory {dir_path}: {e}")
            
            return {
                'success': True,
                'removed_directories': removed_directories,
                'message': f'Cleaned up {len(removed_directories)} unused directories',
                'active_cameras': active_cameras,
                'remaining_directories': len(existing_dirs) - len(removed_directories)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Directory cleanup failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'removed_directories': []
            }
    
    def get_directory_health_status(self, path: str) -> Dict:
        """
        Get comprehensive health status of a directory
        
        Args:
            path: Directory path to check
            
        Returns:
            Dict with health status information
        """
        try:
            path_obj = Path(path)
            
            if not path_obj.exists():
                return {
                    'healthy': False,
                    'exists': False,
                    'message': 'Directory does not exist',
                    'path': path
                }
            
            # Check permissions
            permissions = self.check_permissions(path)
            
            # Check disk space
            disk_space = self.check_disk_space(path, required_gb=0.5)
            
            # Count files and subdirectories
            try:
                files_count = len([f for f in path_obj.rglob('*') if f.is_file()])
                dirs_count = len([d for d in path_obj.rglob('*') if d.is_dir()])
            except Exception:
                files_count = 0
                dirs_count = 0
            
            # Calculate directory size
            try:
                total_size = sum(f.stat().st_size for f in path_obj.rglob('*') if f.is_file())
                size_mb = total_size / (1024**2)
            except Exception:
                size_mb = 0
            
            # Determine overall health
            healthy = (
                permissions.get('readable', False) and
                permissions.get('writable', False) and
                disk_space.get('sufficient', False)
            )
            
            return {
                'healthy': healthy,
                'exists': True,
                'permissions': permissions,
                'disk_space': disk_space,
                'files_count': files_count,
                'directories_count': dirs_count,
                'size_mb': size_mb,
                'path': path,
                'message': 'Directory health check complete'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Directory health check failed: {e}")
            return {
                'healthy': False,
                'error': str(e),
                'path': path
            }
    
    def _sanitize_directory_name(self, name: str) -> str:
        """
        Sanitize camera name to be safe for file system directory names
        
        Args:
            name: Original camera name
            
        Returns:
            Sanitized directory name
        """
        # Replace problematic characters
        import re
        
        # Replace spaces and special chars with underscores
        sanitized = re.sub(r'[<>:"/\\|?*]', '_', name)
        sanitized = re.sub(r'\s+', '_', sanitized)
        
        # Remove leading/trailing underscores and dots
        sanitized = sanitized.strip('_.')
        
        # Ensure not empty
        if not sanitized:
            sanitized = 'camera'
        
        # Limit length to 50 characters
        if len(sanitized) > 50:
            sanitized = sanitized[:50].rstrip('_')
        
        return sanitized
    
    def _get_disk_space_gb(self, path: str) -> float:
        """
        Helper method to get available disk space in GB
        
        Args:
            path: Path to check
            
        Returns:
            Available space in GB
        """
        try:
            statvfs = shutil.disk_usage(path)
            return statvfs.free / (1024**3)
        except Exception:
            return 0.0
    
    def get_base_directories(self) -> Dict:
        """
        Get all base directories managed by PathValidator
        
        Returns:
            Dict with base directory paths and their status
        """
        directories = {
            'base_path': str(self.base_path),
            'nvr_downloads': str(self.nvr_downloads_dir),
            'cloud_sync': str(self.cloud_sync_dir),
            'output_clips': str(self.output_clips_dir)
        }
        
        status = {}
        for name, path in directories.items():
            try:
                Path(path).mkdir(parents=True, exist_ok=True)
                status[name] = {
                    'path': path,
                    'exists': True,
                    'writable': os.access(path, os.W_OK)
                }
            except Exception as e:
                status[name] = {
                    'path': path,
                    'exists': False,
                    'error': str(e)
                }
        
        return {
            'directories': directories,
            'status': status,
            'all_healthy': all(s.get('exists', False) and s.get('writable', False) 
                             for s in status.values())
        }

# Global instance for easy import
path_validator = PathValidator()
```
## ğŸ“„ File: `LicenseGuard.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/system/LicenseGuard.py`

```python
# LicenseGuard.py - Module kiá»ƒm tra báº£n quyá»n (QR watermark, MAC address)

```
## ğŸ“„ File: `SystemMonitor.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/system/SystemMonitor.py`

```python
# SystemMonitor.py - Module theo dÃµi hiá»‡u suáº¥t vÃ  tÃ i nguyÃªn há»‡ thá»‘ng

```
## ğŸ“„ File: `SystemCalendar.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/system/SystemCalendar.py`

```python
# SystemCalendar.py - Module quáº£n lÃ½ lá»‹ch lÃ m viá»‡c (ngÃ y nghá»‰, ca lÃ m viá»‡c)

```
## ğŸ“„ File: `AuditLogger.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/system/AuditLogger.py`

```python
# AuditLogger.py - Module ghi log há»‡ thá»‘ng (lá»‹ch sá»­ xá»­ lÃ½, lá»—i, ngÆ°á»i dÃ¹ng)

```
## ğŸ“„ File: `nvr_downloader.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/nvr_downloader.py`

```python
import os
import json
import sqlite3
from datetime import datetime, timedelta
from modules.db_utils import get_db_connection
from .mock_video_generator import MockVideoGenerator

class NVRDownloader:
    """
    NVR Video Downloader vá»›i support cho cáº£ mock vÃ  real ONVIF downloads
    OPTIMIZED: Sá»­ dá»¥ng last_downloaded_file tracking thay vÃ¬ full file tracking
    """
    
    def __init__(self, mock_mode=True, testing_intervals=True):
        """
        Initialize NVRDownloader
        
        Args:
            mock_mode (bool): True = use mock data, False = real ONVIF downloads
            testing_intervals (bool): True = use short intervals for testing
        """
        self.mock_mode = mock_mode
        self.testing_intervals = testing_intervals
        
        if self.mock_mode:
            self.mock_generator = MockVideoGenerator()
            print(f"ğŸ­ NVRDownloader initialized in MOCK MODE")
        else:
            print(f"ğŸ“¹ NVRDownloader initialized in REAL MODE")
    
    def download_recordings(self, source_config):
        """
        Main download method - routes to mock or real implementation
        
        Args:
            source_config (dict): Source configuration
                - source_id: Database ID
                - name: Source name
                - selected_cameras: List of camera names
                - working_path: Base download directory
                
        Returns:
            dict: Download results with success status, files, and stats
        """
        print(f"ğŸš€ Starting download for source: {source_config.get('name')}")
        print(f"ğŸ“ Working path: {source_config.get('working_path')}")
        print(f"ğŸ“¹ Cameras: {source_config.get('selected_cameras', [])}")
        
        if self.mock_mode:
            return self._mock_download(source_config)
        else:
            return self._real_onvif_download(source_config)
    
    def _mock_download(self, source_config):
        """
        Mock download implementation using MockVideoGenerator
        OPTIMIZED: Uses last_downloaded_file tracking
        
        Args:
            source_config (dict): Source configuration
            
        Returns:
            dict: Mock download results
        """
        print(f"ğŸ¬ MOCK DOWNLOAD: Generating recordings...")
        
        results = {
            'success': True,
            'downloaded_files': [],
            'total_size': 0,
            'cameras_processed': [],
            'mode': 'mock'
        }
        
        try:
            source_id = source_config.get('source_id')
            selected_cameras = source_config.get('selected_cameras', [])
            working_path = source_config.get('working_path')
            
            if not selected_cameras:
                print("âš ï¸ No cameras selected for download")
                return results
            
            # Process each camera
            for camera in selected_cameras:
                print(f"ğŸ¥ Processing camera: {camera}")
                
                # Create camera directory
                camera_dir = os.path.join(working_path, camera.replace(' ', '_'))
                os.makedirs(camera_dir, exist_ok=True)
                print(f"ğŸ“ Camera directory: {camera_dir}")
                
                # Generate mock files for this camera
                if self.testing_intervals:
                    # Testing mode: Short intervals for development
                    mock_files = self.mock_generator.generate_realtime_testing_videos(
                        camera, 
                        camera_dir, 
                        interval_seconds=120,  # 2 minutes for testing
                        count=15  # 15 files per camera
                    )
                else:
                    # Normal mode: Daily videos with realistic intervals
                    mock_files = self.mock_generator.generate_daily_videos(
                        camera,
                        camera_dir,
                        days=1,
                        schedule='security'  # 4 times per day
                    )
                
                # ğŸ†• OPTIMIZED: Track only latest file instead of all files
                tracked_count = self._track_latest_file_only(source_id, camera, mock_files)
                
                # Update results
                results['downloaded_files'].extend(mock_files)
                results['total_size'] += sum(f['size'] for f in mock_files)
                results['cameras_processed'].append(camera)
                
                print(f"âœ… Camera {camera}: {len(mock_files)} files, {tracked_count} efficient tracking")
            
            print(f"ğŸ‰ MOCK DOWNLOAD COMPLETED:")
            print(f"   ğŸ“Š Total files: {len(results['downloaded_files'])}")
            print(f"   ğŸ“ Total size: {results['total_size']} bytes ({results['total_size']/1024:.1f} KB)")
            print(f"   ğŸ¥ Cameras processed: {len(results['cameras_processed'])}")
            print(f"   ğŸ—„ï¸ Database records: {len(results['cameras_processed'])} (optimized)")
            
        except Exception as e:
            print(f"âŒ Mock download error: {e}")
            results['success'] = False
            results['error'] = str(e)
        
        return results
    
    def _real_onvif_download(self, source_config):
        """
        Real ONVIF download implementation (placeholder for future)
        
        Args:
            source_config (dict): Source configuration
            
        Returns:
            dict: Real download results
        """
        print(f"ğŸ“¹ REAL ONVIF DOWNLOAD: Not implemented yet")
        
        # TODO: Implement real ONVIF download logic
        # 1. Connect to ONVIF cameras
        # 2. Get last downloaded timestamp from last_downloaded_file table
        # 3. Query ONVIF for recordings newer than last timestamp
        # 4. Download only new files
        # 5. Update last_downloaded_file with latest info
        
        return {
            'success': False,
            'error': 'Real ONVIF download not implemented yet',
            'downloaded_files': [],
            'total_size': 0,
            'cameras_processed': [],
            'mode': 'real'
        }
    
    def _track_latest_file_only(self, source_id, camera_name, file_list):
        """
        ğŸ†• OPTIMIZED: Track only the latest file per camera in last_downloaded_file table
        TEMPORARILY DISABLED: Database tracking disabled for clean workflow
        
        Args:
            source_id (int): Source database ID
            camera_name (str): Camera name
            file_list (list): List of file info dicts
            
        Returns:
            int: Number of files successfully tracked (for compatibility)
        """
        if not file_list:
            return 0
            
        try:
            # Calculate totals for logging
            total_count = len(file_list)
            total_size_mb = sum(f['size'] for f in file_list) / (1024 * 1024)
            
            # Find latest file by timestamp for logging
            latest_file = max(file_list, key=lambda f: f['timestamp'])
            
            # ğŸ”§ TEMPORARILY DISABLED: Database tracking
            print(f"ğŸ“Š Efficient tracking DISABLED: Latest file '{latest_file['filename']}' | Total: {total_count} files, {total_size_mb:.1f} MB")
            print(f"âš ï¸ Database tracking temporarily disabled for clean workflow")
            
            # Return total count for compatibility (without database update)
            return total_count
            
            # ğŸš« DISABLED CODE BELOW:
            # # Import helper function from database
            # from database import update_last_downloaded_file
            # 
            # # Update database with only latest file info (1 DB operation instead of 15!)
            # success = update_last_downloaded_file(
            #     source_id, camera_name, latest_file, total_count, total_size_mb
            # )
            # 
            # if success:
            #     print(f"ğŸ“Š Efficient tracking: Latest file '{latest_file['filename']}' | Total: {total_count} files, {total_size_mb:.1f} MB")
            #     return total_count
            # else:
            #     print(f"âŒ Failed to track latest file for camera: {camera_name}")
            #     return 0
                
        except Exception as e:
            print(f"âŒ Latest file tracking error: {e}")
            return 0
    
    def _track_downloaded_files(self, source_id, camera_name, file_list):
        """
        âš ï¸ DEPRECATED: Legacy method for full file tracking
        Use _track_latest_file_only() instead for better performance
        
        Args:
            source_id (int): Source database ID
            camera_name (str): Camera name
            file_list (list): List of file info dicts
            
        Returns:
            int: Number of files successfully tracked
        """
        print(f"âš ï¸ Using legacy full file tracking - consider using _track_latest_file_only()")
        
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            
            tracked_count = 0
            
            for file_info in file_list:
                try:
                    cursor.execute("""
                        INSERT INTO downloaded_files (
                            source_id, camera_name, local_file_path, file_size_bytes, 
                            download_timestamp, recording_start_time, original_filename
                        ) VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (
                        source_id,
                        camera_name,
                        file_info['path'],
                        file_info['size'],
                        datetime.now().isoformat(),
                        file_info['timestamp'].isoformat(),
                        file_info['filename']
                    ))
                    tracked_count += 1
                    
                except sqlite3.IntegrityError:
                    print(f"âš ï¸ File already tracked: {file_info['filename']}")
                except Exception as e:
                    print(f"âŒ Failed to track file {file_info['filename']}: {e}")
            
            conn.commit()
            print(f"âœ… DB commit & close success for camera: {camera_name}")
            conn.close()
            
            print(f"ğŸ“Š Legacy tracking: {tracked_count}/{len(file_list)} files")
            return tracked_count
            
        except Exception as e:
            print(f"âŒ Database tracking error: {e}")
            return 0
    
    def get_download_statistics(self, source_id):
        """
        ğŸ†• OPTIMIZED: Get download statistics using last_downloaded_file table
        
        Args:
            source_id (int): Source database ID
            
        Returns:
            dict: Download statistics
        """
        try:
            # Import helper function from database
            from database import get_camera_download_stats
            
            stats = get_camera_download_stats(source_id)
            
            # Convert to expected format for compatibility
            camera_stats = {}
            for camera_name, camera_info in stats['camera_stats'].items():
                camera_stats[camera_name] = {
                    'file_count': camera_info['files_count'],
                    'total_size': camera_info['size_mb'] * 1024 * 1024  # Convert back to bytes
                }
            
            # Get latest download time
            latest_download = None
            for camera_info in stats['camera_stats'].values():
                if camera_info['last_download']:
                    if not latest_download or camera_info['last_download'] > latest_download:
                        latest_download = camera_info['last_download']
            
            return {
                'total_files': stats['total_files'],
                'total_size': int(stats['total_size_mb'] * 1024 * 1024),  # Convert to bytes
                'total_size_mb': stats['total_size_mb'],
                'camera_stats': camera_stats,
                'latest_download': latest_download,
                'cameras_count': stats['cameras_count']
            }
            
        except Exception as e:
            print(f"âŒ Statistics error: {e}")
            return {
                'total_files': 0,
                'total_size': 0,
                'total_size_mb': 0,
                'camera_stats': {},
                'latest_download': None,
                'cameras_count': 0
            }
    
    def get_last_downloaded_timestamp(self, source_id, camera_name):
        """
        ğŸ†• NEW: Get last downloaded file timestamp for incremental sync
        
        Args:
            source_id (int): Source database ID
            camera_name (str): Camera name
            
        Returns:
            str: ISO timestamp of last downloaded file
        """
        try:
            from database import get_last_downloaded_timestamp
            return get_last_downloaded_timestamp(source_id, camera_name)
        except Exception as e:
            print(f"âŒ Error getting last timestamp: {e}")
            return "1970-01-01T00:00:00"  # Default to epoch if error
    
    def cleanup_old_downloads(self, source_id, keep_days=30):
        """
        ğŸ†• OPTIMIZED: Clean up old downloaded files using filesystem + last_downloaded_file
        
        Args:
            source_id (int): Source database ID
            keep_days (int): Number of days to keep
            
        Returns:
            dict: Cleanup results
        """
        try:
            from database import get_camera_download_stats
            
            stats = get_camera_download_stats(source_id)
            cutoff_date = datetime.now() - timedelta(days=keep_days)
            
            deleted_files = 0
            deleted_size = 0
            
            # For each camera, clean up old files from filesystem
            for camera_name, camera_info in stats['camera_stats'].items():
                if camera_info['last_download']:
                    last_download_date = datetime.fromisoformat(camera_info['last_download'])
                    
                    # If last download is older than cutoff, could clean up
                    if last_download_date < cutoff_date:
                        print(f"ğŸ§¹ Camera {camera_name}: Last download {camera_info['last_download']} is old")
                        # Here you could implement filesystem cleanup logic
                        # For now, just report what would be cleaned
            
            # Update last_downloaded_file table to remove old entries
            conn = get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute("""
                DELETE FROM last_downloaded_file 
                WHERE source_id = ? AND last_download_time < ?
            """, (source_id, cutoff_date.isoformat()))
            
            db_deleted = cursor.rowcount
            conn.commit()
            conn.close()
            
            result = {
                'files_deleted': deleted_files,
                'size_freed': deleted_size,
                'size_freed_mb': round(deleted_size / (1024 * 1024), 2),
                'db_records_deleted': db_deleted
            }
            
            print(f"ğŸ§¹ Cleanup completed: {deleted_files} files, {result['size_freed_mb']} MB freed, {db_deleted} DB records removed")
            return result
            
        except Exception as e:
            print(f"âŒ Cleanup error: {e}")
            return {
                'files_deleted': 0,
                'size_freed': 0,
                'size_freed_mb': 0,
                'db_records_deleted': 0,
                'error': str(e)
            }
    
    def test_mock_download(self, test_source_config=None):
        """
        Test method Ä‘á»ƒ kiá»ƒm tra mock download functionality
        
        Args:
            test_source_config (dict): Optional test configuration
            
        Returns:
            dict: Test results
        """
        if test_source_config is None:
            test_source_config = {
                'source_id': 999,  # Test source ID
                'name': 'test_nvr',
                'selected_cameras': ['Test Camera 1', 'Test Camera 2'],
                'working_path': '/tmp/nvr_test_download'
            }
        
        print(f"ğŸ§ª TESTING OPTIMIZED MOCK DOWNLOAD...")
        print(f"ğŸ“‹ Test config: {test_source_config}")
        
        # Ensure test directory exists
        os.makedirs(test_source_config['working_path'], exist_ok=True)
        
        # Run mock download
        results = self._mock_download(test_source_config)
        
        # Verify results
        if results['success']:
            print(f"âœ… Test passed: {len(results['downloaded_files'])} files created")
            print(f"ğŸ“Š Efficient DB tracking: {len(results['cameras_processed'])} records instead of {len(results['downloaded_files'])}")
            
            # Check actual files exist
            sample_files = results['downloaded_files'][:5]  # Show first 5 files
            for file_info in sample_files:
                if os.path.exists(file_info['path']):
                    actual_size = os.path.getsize(file_info['path'])
                    print(f"   ğŸ“„ {file_info['filename']}: {actual_size} bytes")
                else:
                    print(f"   âŒ Missing: {file_info['filename']}")
            
            if len(results['downloaded_files']) > 5:
                print(f"   ... and {len(results['downloaded_files']) - 5} more files")
        else:
            print(f"âŒ Test failed: {results.get('error', 'Unknown error')}")
        
        return results

# Usage examples vÃ  test function
def test_nvr_downloader():
    """Test function Ä‘á»ƒ verify optimized NVRDownloader functionality"""
    print("=== TESTING OPTIMIZED NVRDownloader ===")
    
    # Test 1: Mock download vá»›i testing intervals
    print("\n--- Test 1: Optimized Mock Download (Testing Mode) ---")
    downloader = NVRDownloader(mock_mode=True, testing_intervals=True)
    
    test_config = {
        'source_id': 37,
        'name': 'nvr_localhost',
        'selected_cameras': ['Front Door Camera', 'Parking Lot Camera'],
        'working_path': '/tmp/test_nvr_downloads'
    }
    
    results = downloader.test_mock_download(test_config)
    
    # Test 2: Optimized Statistics
    print("\n--- Test 2: Optimized Download Statistics ---")
    stats = downloader.get_download_statistics(37)
    print(f"Statistics: {stats}")
    
    # Test 3: Last downloaded timestamp
    print("\n--- Test 3: Last Downloaded Timestamp ---")
    for camera in test_config['selected_cameras']:
        last_timestamp = downloader.get_last_downloaded_timestamp(37, camera)
        print(f"Camera '{camera}' last download: {last_timestamp}")
    
    # Test 4: Normal intervals
    print("\n--- Test 4: Normal Intervals ---")
    downloader_normal = NVRDownloader(mock_mode=True, testing_intervals=False)
    results_normal = downloader_normal.test_mock_download({
        'source_id': 38,
        'name': 'nvr_normal_test',
        'selected_cameras': ['Normal Test Camera'],
        'working_path': '/tmp/test_nvr_normal'
    })
    
    print("\nâœ… All optimized tests completed!")

if __name__ == "__main__":
    test_nvr_downloader()
```
## ğŸ“„ File: `cloud_manager.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/cloud_manager.py`

```python
#!/usr/bin/env python3
"""
Cloud Manager for VTrack - Unified Cloud Interface
Handles connection management, folder discovery, and authentication validation
Supports multiple cloud providers with Google Drive as primary implementation
"""

import os
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path

# Import existing Google Drive client
from .google_drive_client import GoogleDriveClient

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CloudManager:
    """
    Unified cloud interface for VTrack video source management
    Provides consistent API across different cloud providers
    """
    
    # Supported cloud providers
    SUPPORTED_PROVIDERS = {
        'google_drive': {
            'name': 'Google Drive',
            'client_class': GoogleDriveClient,
            'auth_type': 'oauth2',
            'supports_folders': True,
            'supports_nested': True
        }
        # Future: Dropbox, OneDrive, etc.
    }
    
    def __init__(self, provider: str = 'google_drive'):
        """
        Initialize CloudManager for specified provider
        
        Args:
            provider (str): Cloud provider name ('google_drive', etc.)
        """
        self.provider = provider
        self.client = None
        self.authenticated = False
        self.user_info = {}
        
        # Validate provider
        if provider not in self.SUPPORTED_PROVIDERS:
            raise ValueError(f"Unsupported provider: {provider}. Supported: {list(self.SUPPORTED_PROVIDERS.keys())}")
        
        # Initialize provider-specific client
        self._initialize_client()
        
        logger.info(f"CloudManager initialized for provider: {provider}")
    
    def _initialize_client(self):
        """Initialize the provider-specific client"""
        try:
            provider_config = self.SUPPORTED_PROVIDERS[self.provider]
            client_class = provider_config['client_class']
            
            if self.provider == 'google_drive':
                self.client = client_class()
            else:
                # Future provider initialization
                raise NotImplementedError(f"Provider {self.provider} not yet implemented")
                
            logger.info(f"âœ… {provider_config['name']} client initialized")
            
        except Exception as e:
            logger.error(f"âŒ Failed to initialize {self.provider} client: {e}")
            raise
    
    def test_connection_and_discover_folders(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Test cloud connection and discover available folders
        Main method called by VTrack's /test-source endpoint
        
        Args:
            config (dict): Source configuration containing provider settings
            
        Returns:
            dict: Connection test results with folder discovery
        """
        try:
            logger.info(f"ğŸ” Testing {self.provider} connection and discovering folders...")
            
            # Step 1: Test basic connection
            connection_result = self.test_connection(config)
            
            if not connection_result['success']:
                return {
                    'accessible': False,
                    'message': connection_result['message'],
                    'provider': self.provider,
                    'folders': [],
                    'cameras': []
                }
            
            # Step 2: Discover root folders
            root_folders = self.discover_root_folders()
            
            # Step 3: Analyze folder structure for camera folders
            folder_analysis = self._analyze_folder_structure(root_folders)
            
            logger.info(f"âœ… Connection successful: {len(root_folders)} root folders discovered")
            
            return {
                'accessible': True,
                'message': f"{self.provider.title()} connection successful",
                'provider': self.provider,
                'user_info': self.user_info,
                'folders': root_folders,
                'folder_analysis': folder_analysis,
                'cameras': [],  # Will be populated when specific folder is selected
                'connection_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ Connection test failed: {e}")
            return {
                'accessible': False,
                'message': f"Connection failed: {str(e)}",
                'provider': self.provider,
                'folders': [],
                'cameras': [],
                'error': str(e)
            }
    
    def test_connection(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Test basic cloud provider connection
        
        Args:
            config (dict): Provider configuration
            
        Returns:
            dict: Connection test result
        """
        try:
            if self.provider == 'google_drive':
                return self._test_google_drive_connection(config)
            else:
                return {
                    'success': False,
                    'message': f"Provider {self.provider} not implemented"
                }
                
        except Exception as e:
            logger.error(f"âŒ Connection test error: {e}")
            return {
                'success': False,
                'message': f"Connection test failed: {str(e)}"
            }
    
    def _test_google_drive_connection(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Test Google Drive specific connection"""
        try:
            # Attempt authentication
            auth_success = self.client.authenticate()
            
            if not auth_success:
                return {
                    'success': False,
                    'message': 'Google Drive authentication failed'
                }
            
            # Test API access
            connection_test = self.client.test_connection()
            
            if connection_test['success']:
                self.authenticated = True
                self.user_info = {
                    'email': connection_test.get('user_email', 'Unknown'),
                    'storage_used_gb': connection_test.get('storage_used_gb', 0),
                    'storage_total_gb': connection_test.get('storage_total_gb', 'Unknown')
                }
                
                logger.info(f"âœ… Google Drive authenticated: {self.user_info['email']}")
                return {
                    'success': True,
                    'message': f"Connected to Google Drive: {self.user_info['email']}",
                    'user_info': self.user_info
                }
            else:
                return {
                    'success': False,
                    'message': connection_test['message']
                }
                
        except Exception as e:
            logger.error(f"âŒ Google Drive connection error: {e}")
            return {
                'success': False,
                'message': f"Google Drive connection failed: {str(e)}"
            }
    
    def discover_root_folders(self) -> List[Dict[str, Any]]:
        """
        Discover root-level folders in cloud storage
        
        Returns:
            list: List of root folder information
        """
        try:
            if not self.authenticated:
                logger.warning("âš ï¸ Not authenticated - cannot discover folders")
                return []
            
            if self.provider == 'google_drive':
                return self._discover_google_drive_folders()
            else:
                logger.warning(f"âš ï¸ Folder discovery not implemented for {self.provider}")
                return []
                
        except Exception as e:
            logger.error(f"âŒ Folder discovery error: {e}")
            return []
    
    def _discover_google_drive_folders(self) -> List[Dict[str, Any]]:
        """Discover Google Drive folders"""
        try:
            # Get root-level folders from Google Drive
            folders = self.client.list_files(folder_id='root', limit=50)
            
            root_folders = []
            for folder in folders:
                if folder.get('mimeType') == 'application/vnd.google-apps.folder':
                    folder_info = {
                        'id': folder['id'],
                        'name': folder['name'],
                        'created_time': folder.get('createdTime'),
                        'size': folder.get('size', 0),
                        'provider': 'google_drive',
                        'type': 'folder',
                        'description': f"Google Drive folder: {folder['name']}"
                    }
                    root_folders.append(folder_info)
            
            logger.info(f"ğŸ“ Discovered {len(root_folders)} Google Drive root folders")
            return root_folders
            
        except Exception as e:
            logger.error(f"âŒ Google Drive folder discovery error: {e}")
            return []
    
    def discover_subfolders(self, folder_id: str, credentials: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Discover subfolders within a specific folder
        Used for camera folder detection in nested structures
        
        Args:
            folder_id (str): Parent folder ID
            credentials (dict): Optional authentication credentials
            
        Returns:
            dict: Subfolder discovery results
        """
        try:
            logger.info(f"ğŸ” Discovering subfolders in folder: {folder_id}")
            
            if self.provider == 'google_drive':
                return self._discover_google_drive_subfolders(folder_id, credentials)
            else:
                return {
                    'success': False,
                    'message': f"Subfolder discovery not implemented for {self.provider}",
                    'subfolders': []
                }
                
        except Exception as e:
            logger.error(f"âŒ Subfolder discovery error: {e}")
            return {
                'success': False,
                'message': f"Subfolder discovery failed: {str(e)}",
                'subfolders': [],
                'error': str(e)
            }
    
    def _discover_google_drive_subfolders(self, folder_id: str, credentials: Optional[Dict] = None) -> Dict[str, Any]:
        """Discover Google Drive subfolders"""
        try:
            # Re-authenticate if credentials provided
            if credentials and not self.authenticated:
                # TODO: Use provided credentials for authentication
                pass
            
            # Get subfolders
            subfolders_raw = self.client.list_files(folder_id=folder_id, limit=100)
            
            subfolders = []
            camera_folders = []
            
            for item in subfolders_raw:
                if item.get('mimeType') == 'application/vnd.google-apps.folder':
                    folder_info = {
                        'id': item['id'],
                        'name': item['name'],
                        'created_time': item.get('createdTime'),
                        'size': item.get('size', 0),
                        'parent_id': folder_id,
                        'provider': 'google_drive',
                        'type': 'folder'
                    }
                    
                    # Check if this looks like a camera folder
                    if self._is_camera_folder(item['name']):
                        folder_info['is_camera_folder'] = True
                        camera_folders.append(folder_info)
                    else:
                        folder_info['is_camera_folder'] = False
                    
                    subfolders.append(folder_info)
            
            logger.info(f"ğŸ“ Found {len(subfolders)} subfolders, {len(camera_folders)} camera folders")
            
            return {
                'success': True,
                'message': f"Found {len(subfolders)} subfolders",
                'subfolders': subfolders,
                'camera_folders': camera_folders,
                'total_folders': len(subfolders),
                'camera_count': len(camera_folders)
            }
            
        except Exception as e:
            logger.error(f"âŒ Google Drive subfolder discovery error: {e}")
            return {
                'success': False,
                'message': f"Google Drive subfolder discovery failed: {str(e)}",
                'subfolders': [],
                'error': str(e)
            }
    
    def _is_camera_folder(self, folder_name: str) -> bool:
        """
        Determine if a folder name indicates it contains camera videos
        
        Args:
            folder_name (str): Folder name to analyze
            
        Returns:
            bool: True if likely a camera folder
        """
        camera_keywords = [
            'cam', 'camera', 'channel', 'ch', 'zone', 'area',
            'front', 'back', 'door', 'entrance', 'parking',
            'office', 'lobby', 'security', 'surveillance',
            'nvr', 'dvr', 'cctv', 'ip_cam', 'ipcam'
        ]
        
        folder_lower = folder_name.lower()
        
        # Check for camera keywords
        for keyword in camera_keywords:
            if keyword in folder_lower:
                return True
        
        # Check for camera patterns (e.g., "Camera_01", "CH01", "Zone1")
        import re
        camera_patterns = [
            r'camera[_\s]*\d+',
            r'cam[_\s]*\d+',
            r'ch[_\s]*\d+',
            r'channel[_\s]*\d+',
            r'zone[_\s]*\d+',
            r'area[_\s]*\d+'
        ]
        
        for pattern in camera_patterns:
            if re.search(pattern, folder_lower):
                return True
        
        return False
    
    def _analyze_folder_structure(self, root_folders: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Analyze folder structure to provide insights for UI
        
        Args:
            root_folders (list): List of root folders
            
        Returns:
            dict: Folder structure analysis
        """
        analysis = {
            'total_folders': len(root_folders),
            'camera_folders_at_root': 0,
            'security_related_folders': 0,
            'recommended_folders': [],
            'structure_type': 'unknown'
        }
        
        security_keywords = ['security', 'surveillance', 'camera', 'nvr', 'dvr', 'cctv']
        
        for folder in root_folders:
            folder_name_lower = folder['name'].lower()
            
            # Check if root folder is camera folder
            if self._is_camera_folder(folder['name']):
                analysis['camera_folders_at_root'] += 1
            
            # Check if security-related
            if any(keyword in folder_name_lower for keyword in security_keywords):
                analysis['security_related_folders'] += 1
                analysis['recommended_folders'].append(folder)
        
        # Determine structure type
        if analysis['camera_folders_at_root'] > 0:
            analysis['structure_type'] = 'flat_cameras'
        elif analysis['security_related_folders'] > 0:
            analysis['structure_type'] = 'nested_security'
        else:
            analysis['structure_type'] = 'general'
        
        return analysis
    
    def get_authentication_status(self) -> Dict[str, Any]:
        """
        Get current authentication status
        
        Returns:
            dict: Authentication status information
        """
        return {
            'authenticated': self.authenticated,
            'provider': self.provider,
            'user_info': self.user_info if self.authenticated else {},
            'last_check': datetime.now().isoformat()
        }
    
    def disconnect(self) -> Dict[str, Any]:
        """
        Disconnect from cloud provider
        
        Returns:
            dict: Disconnection result
        """
        try:
            # Provider-specific disconnection logic
            if self.provider == 'google_drive':
                # TODO: Implement Google Drive token revocation
                pass
            
            # Reset local state
            self.authenticated = False
            self.user_info = {}
            self.client = None
            
            # Reinitialize client
            self._initialize_client()
            
            logger.info(f"ğŸ”Œ Disconnected from {self.provider}")
            
            return {
                'success': True,
                'message': f"Disconnected from {self.provider}",
                'provider': self.provider
            }
            
        except Exception as e:
            logger.error(f"âŒ Disconnection error: {e}")
            return {
                'success': False,
                'message': f"Disconnection failed: {str(e)}",
                'error': str(e)
            }
    
    def get_provider_info(self) -> Dict[str, Any]:
        """
        Get information about the current cloud provider
        
        Returns:
            dict: Provider information
        """
        if self.provider in self.SUPPORTED_PROVIDERS:
            provider_config = self.SUPPORTED_PROVIDERS[self.provider]
            return {
                'provider': self.provider,
                'name': provider_config['name'],
                'auth_type': provider_config['auth_type'],
                'supports_folders': provider_config['supports_folders'],
                'supports_nested': provider_config['supports_nested'],
                'authenticated': self.authenticated,
                'available': True
            }
        else:
            return {
                'provider': self.provider,
                'available': False,
                'error': 'Provider not supported'
            }


def test_cloud_manager():
    """
    Test function for CloudManager functionality
    """
    print("ğŸ”§ Testing CloudManager...")
    
    try:
        # Initialize CloudManager
        print("\n1. Initializing CloudManager...")
        cloud_manager = CloudManager('google_drive')
        print(f"âœ… CloudManager initialized for: {cloud_manager.provider}")
        
        # Get provider info
        print("\n2. Getting provider info...")
        provider_info = cloud_manager.get_provider_info()
        print(f"âœ… Provider: {provider_info['name']}")
        print(f"   Supports folders: {provider_info['supports_folders']}")
        print(f"   Supports nested: {provider_info['supports_nested']}")
        
        # Test connection (will attempt authentication)
        print("\n3. Testing connection...")
        test_config = {}  # Empty config for basic test
        connection_result = cloud_manager.test_connection_and_discover_folders(test_config)
        
        if connection_result['accessible']:
            print(f"âœ… Connection successful!")
            print(f"   User: {connection_result.get('user_info', {}).get('email', 'Unknown')}")
            print(f"   Folders found: {len(connection_result.get('folders', []))}")
        else:
            print(f"âŒ Connection failed: {connection_result['message']}")
        
        # Get authentication status
        print("\n4. Checking authentication status...")
        auth_status = cloud_manager.get_authentication_status()
        print(f"âœ… Authenticated: {auth_status['authenticated']}")
        
        print("\nğŸ‰ CloudManager test completed!")
        
    except Exception as e:
        print(f"âŒ CloudManager test failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_cloud_manager()
```
## ğŸ“„ File: `mock_video_generator.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/mock_video_generator.py`

```python
import os
import shutil
from datetime import datetime, timedelta
import json

class MockVideoGenerator:
    """
    Táº¡o mock video files Ä‘á»ƒ giáº£ láº­p ONVIF camera recordings
    Sá»­ dá»¥ng cho testing vÃ  development khi ONVIF Profile G khÃ´ng kháº£ dá»¥ng
    """
    
    def __init__(self, base_samples_path=None):
        """
        Initialize MockVideoGenerator
        
        Args:
            base_samples_path (str): Path to sample video files (optional)
        """
        self.samples_path = base_samples_path
        self.mock_file_size = 15 * 1024  # 15KB per mock file
        
        # Common camera recording patterns
        self.recording_schedule = {
            'continuous': list(range(0, 24)),  # Every hour
            'business': [8, 9, 10, 11, 12, 13, 14, 15, 16, 17],  # Business hours
            'security': [6, 12, 18, 22],  # 4 times per day
            'minimal': [9, 15, 21],  # 3 times per day
            'testing': 'every_minute'  # ğŸ†• Special mode for testing
        }
    
    def generate_daily_videos(self, camera_name, target_dir, days=7, schedule='security'):
        """
        Táº¡o mock videos cho má»™t camera trong X ngÃ y
        
        Args:
            camera_name (str): TÃªn camera (VD: "Front Door Camera")
            target_dir (str): ThÆ° má»¥c Ä‘Ã­ch
            days (int): Sá»‘ ngÃ y táº¡o videos (máº·c Ä‘á»‹nh 7)
            schedule (str): Loáº¡i lá»‹ch recording ('continuous', 'business', 'security', 'minimal', 'testing')
            
        Returns:
            list: Danh sÃ¡ch cÃ¡c file Ä‘Ã£ táº¡o vá»›i metadata
        """
        print(f"ğŸ¬ Generating mock videos for {camera_name} (schedule: {schedule})...")
        
        # Ensure target directory exists
        os.makedirs(target_dir, exist_ok=True)
        
        videos_created = []
        
        # ğŸ†• TESTING MODE: Táº¡o files vá»›i interval ngáº¯n
        if schedule == 'testing':
            return self._generate_testing_videos(camera_name, target_dir, days)
        
        # Original schedule logic for other modes
        hours_schedule = self.recording_schedule.get(schedule, self.recording_schedule['security'])
        
        for i in range(days):
            date = datetime.now() - timedelta(days=i)
            
            for hour in hours_schedule:
                # Táº¡o timestamp cho recording
                timestamp = date.replace(hour=hour, minute=0, second=0, microsecond=0)
                
                # Format filename theo convention thá»±c táº¿
                safe_camera_name = camera_name.replace(' ', '_').replace('/', '_')
                filename = f"{safe_camera_name}_{timestamp.strftime('%Y%m%d_%H%M%S')}.mp4"
                
                target_file = os.path.join(target_dir, filename)
                
                # Táº¡o mock file
                file_info = self._create_mock_video_file(
                    target_file, 
                    camera_name, 
                    timestamp
                )
                
                videos_created.append(file_info)
                
        print(f"âœ… Created {len(videos_created)} mock videos for {camera_name}")
    def generate_recent_videos(self, camera_name, target_dir, hours=24):
        """
        Táº¡o videos cho X giá» gáº§n Ä‘Ã¢y (Ä‘á»ƒ simulate realtime download)
        
        Args:
            camera_name (str): TÃªn camera
            target_dir (str): ThÆ° má»¥c Ä‘Ã­ch  
            hours (int): Sá»‘ giá» gáº§n Ä‘Ã¢y (máº·c Ä‘á»‹nh 24)
            
        Returns:
            list: Danh sÃ¡ch files Ä‘Æ°á»£c táº¡o
        """
        print(f"ğŸ• Generating recent {hours}h videos for {camera_name}...")
        
        os.makedirs(target_dir, exist_ok=True)
        videos_created = []
        
        # Táº¡o videos má»—i 2 giá» trong khoáº£ng thá»i gian chá»‰ Ä‘á»‹nh
        for i in range(0, hours, 2):
            timestamp = datetime.now() - timedelta(hours=i)
            
            safe_camera_name = camera_name.replace(' ', '_').replace('/', '_')
            filename = f"{safe_camera_name}_{timestamp.strftime('%Y%m%d_%H%M%S')}.mp4"
            
            target_file = os.path.join(target_dir, filename)
            
            file_info = self._create_mock_video_file(
                target_file, 
                camera_name, 
                timestamp
            )
            
            videos_created.append(file_info)
        
        print(f"âœ… Created {len(videos_created)} recent videos for {camera_name}")
        return videos_created
    
    def _generate_testing_videos(self, camera_name, target_dir, days=1):
        """
        ğŸ§ª TESTING MODE: Táº¡o videos vá»›i interval ngáº¯n Ä‘á»ƒ test nhanh
        
        Args:
            camera_name (str): TÃªn camera
            target_dir (str): ThÆ° má»¥c Ä‘Ã­ch
            days (int): Sá»‘ ngÃ y (cho testing thÆ°á»ng lÃ  1)
            
        Returns:
            list: Danh sÃ¡ch files Ä‘Æ°á»£c táº¡o
        """
        print(f"ğŸ§ª TESTING MODE: Creating videos every 1-2 minutes for {camera_name}")
        
        videos_created = []
        base_time = datetime.now()
        
        # Táº¡o videos cho 30 phÃºt gáº§n Ä‘Ã¢y, má»—i 2 phÃºt 1 file
        for i in range(15):  # 15 files, má»—i file cÃ¡ch 2 phÃºt
            timestamp = base_time - timedelta(minutes=i * 2)
            
            safe_camera_name = camera_name.replace(' ', '_').replace('/', '_')
            filename = f"{safe_camera_name}_{timestamp.strftime('%Y%m%d_%H%M%S')}.mp4"
            
            target_file = os.path.join(target_dir, filename)
            
            file_info = self._create_mock_video_file(
                target_file, 
                camera_name, 
                timestamp
            )
            
            videos_created.append(file_info)
        
        print(f"âœ… TESTING: Created {len(videos_created)} videos (2-minute intervals)")
        return videos_created
    
    def generate_realtime_testing_videos(self, camera_name, target_dir, interval_seconds=60, count=5):
        """
        ğŸš€ REALTIME TESTING: Táº¡o videos vá»›i khoáº£ng cÃ¡ch ráº¥t ngáº¯n cho testing realtime
        
        Args:
            camera_name (str): TÃªn camera
            target_dir (str): ThÆ° má»¥c Ä‘Ã­ch
            interval_seconds (int): Khoáº£ng cÃ¡ch giá»¯a cÃ¡c file (giÃ¢y) - máº·c Ä‘á»‹nh 60s
            count (int): Sá»‘ lÆ°á»£ng files táº¡o - máº·c Ä‘á»‹nh 5
            
        Returns:
            list: Danh sÃ¡ch files Ä‘Æ°á»£c táº¡o
        """
        print(f"ğŸš€ REALTIME TESTING: Creating {count} videos every {interval_seconds}s for {camera_name}")
        
        os.makedirs(target_dir, exist_ok=True)
        videos_created = []
        
        base_time = datetime.now()
        
        for i in range(count):
            # Táº¡o timestamp lÃ¹i láº¡i theo interval
            timestamp = base_time - timedelta(seconds=i * interval_seconds)
            
            safe_camera_name = camera_name.replace(' ', '_').replace('/', '_')
            filename = f"{safe_camera_name}_{timestamp.strftime('%Y%m%d_%H%M%S')}.mp4"
            
            target_file = os.path.join(target_dir, filename)
            
            file_info = self._create_mock_video_file(
                target_file, 
                camera_name, 
                timestamp
            )
            
            videos_created.append(file_info)
        
        print(f"âœ… REALTIME TESTING: Created {len(videos_created)} videos ({interval_seconds}s intervals)")
        return videos_created
        """
        Táº¡o videos cho X giá» gáº§n Ä‘Ã¢y (Ä‘á»ƒ simulate realtime download)
        
        Args:
            camera_name (str): TÃªn camera
            target_dir (str): ThÆ° má»¥c Ä‘Ã­ch  
            hours (int): Sá»‘ giá» gáº§n Ä‘Ã¢y (máº·c Ä‘á»‹nh 24)
            
        Returns:
            list: Danh sÃ¡ch files Ä‘Æ°á»£c táº¡o
        """
        print(f"ğŸ• Generating recent {hours}h videos for {camera_name}...")
        
        os.makedirs(target_dir, exist_ok=True)
        videos_created = []
        
        # Táº¡o videos má»—i 2 giá» trong khoáº£ng thá»i gian chá»‰ Ä‘á»‹nh
        for i in range(0, hours, 2):
            timestamp = datetime.now() - timedelta(hours=i)
            
            safe_camera_name = camera_name.replace(' ', '_').replace('/', '_')
            filename = f"{safe_camera_name}_{timestamp.strftime('%Y%m%d_%H%M%S')}.mp4"
            
            target_file = os.path.join(target_dir, filename)
            
            file_info = self._create_mock_video_file(
                target_file, 
                camera_name, 
                timestamp
            )
            
            videos_created.append(file_info)
        
        print(f"âœ… Created {len(videos_created)} recent videos for {camera_name}")
        return videos_created
    
    def _create_mock_video_file(self, filepath, camera_name, timestamp):
        """
        Táº¡o má»™t mock video file vá»›i metadata thá»±c táº¿
        
        Args:
            filepath (str): ÄÆ°á»ng dáº«n file Ä‘áº§y Ä‘á»§
            camera_name (str): TÃªn camera
            timestamp (datetime): Thá»i gian recording
            
        Returns:
            dict: ThÃ´ng tin file Ä‘Ã£ táº¡o
        """
        # Táº¡o mock video content vá»›i metadata
        video_metadata = {
            "camera": camera_name,
            "timestamp": timestamp.isoformat(),
            "duration_minutes": 60,  # Giáº£ láº­p recording 1 giá»
            "resolution": "1920x1080",
            "codec": "H.264",
            "framerate": "30fps",
            "bitrate": "2000kbps",
            "file_type": "MP4",
            "mock_data": True
        }
        
        # Táº¡o file content
        header = f"MOCK VIDEO FILE - {camera_name}\n"
        header += f"Recording Time: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n"
        header += f"Metadata: {json.dumps(video_metadata, indent=2)}\n"
        header += "=" * 50 + "\n"
        
        # Mock binary data (giáº£ láº­p video data)
        mock_binary = b'\x00\x01\x02\x03' * (self.mock_file_size // 4)
        
        # Write file
        with open(filepath, 'wb') as f:
            f.write(header.encode('utf-8'))
            f.write(mock_binary)
        
        # Return file info
        file_info = {
            'filename': os.path.basename(filepath),
            'path': filepath,
            'size': os.path.getsize(filepath),
            'timestamp': timestamp,
            'camera': camera_name,
            'metadata': video_metadata
        }
        
        return file_info
    
    def simulate_continuous_recording(self, camera_name, target_dir, start_time=None, duration_hours=6):
        """
        Giáº£ láº­p continuous recording vá»›i files nhá» má»—i 30 phÃºt
        
        Args:
            camera_name (str): TÃªn camera
            target_dir (str): ThÆ° má»¥c Ä‘Ã­ch
            start_time (datetime): Thá»i gian báº¯t Ä‘áº§u (máº·c Ä‘á»‹nh lÃ  6h trÆ°á»›c)
            duration_hours (int): Tá»•ng thá»i gian recording (giá»)
            
        Returns:
            list: Danh sÃ¡ch files Ä‘Ã£ táº¡o
        """
        if start_time is None:
            start_time = datetime.now() - timedelta(hours=duration_hours)
        
        print(f"ğŸ“¹ Simulating continuous recording for {camera_name} ({duration_hours}h)")
        
        os.makedirs(target_dir, exist_ok=True)
        videos_created = []
        
        # Táº¡o file má»—i 30 phÃºt
        intervals = duration_hours * 2  # 2 files per hour
        
        for i in range(intervals):
            timestamp = start_time + timedelta(minutes=i * 30)
            
            safe_camera_name = camera_name.replace(' ', '_').replace('/', '_')
            filename = f"{safe_camera_name}_{timestamp.strftime('%Y%m%d_%H%M%S')}.mp4"
            
            target_file = os.path.join(target_dir, filename)
            
            file_info = self._create_mock_video_file(
                target_file, 
                camera_name, 
                timestamp
            )
            
            videos_created.append(file_info)
        
        print(f"âœ… Created {len(videos_created)} continuous recording files for {camera_name}")
        return videos_created
    
    def cleanup_old_files(self, target_dir, keep_days=30):
        """
        Dá»n dáº¹p cÃ¡c mock files cÅ© hÆ¡n X ngÃ y
        
        Args:
            target_dir (str): ThÆ° má»¥c cáº§n dá»n dáº¹p
            keep_days (int): Sá»‘ ngÃ y giá»¯ láº¡i (máº·c Ä‘á»‹nh 30)
            
        Returns:
            int: Sá»‘ files Ä‘Ã£ xÃ³a
        """
        if not os.path.exists(target_dir):
            return 0
        
        cutoff_time = datetime.now() - timedelta(days=keep_days)
        deleted_count = 0
        
        for filename in os.listdir(target_dir):
            filepath = os.path.join(target_dir, filename)
            
            if os.path.isfile(filepath):
                # Get file modification time
                file_time = datetime.fromtimestamp(os.path.getmtime(filepath))
                
                if file_time < cutoff_time:
                    try:
                        os.remove(filepath)
                        deleted_count += 1
                        print(f"ğŸ—‘ï¸ Removed old mock file: {filename}")
                    except Exception as e:
                        print(f"âŒ Failed to remove {filename}: {e}")
        
        if deleted_count > 0:
            print(f"âœ… Cleanup completed: {deleted_count} old files removed")
        
        return deleted_count
    
    def get_mock_statistics(self, target_dir):
        """
        Láº¥y thá»‘ng kÃª vá» mock files trong thÆ° má»¥c
        
        Args:
            target_dir (str): ThÆ° má»¥c cáº§n thá»‘ng kÃª
            
        Returns:
            dict: Thá»‘ng kÃª chi tiáº¿t
        """
        if not os.path.exists(target_dir):
            return {
                'total_files': 0,
                'total_size': 0,
                'size_mb': 0,
                'date_range': None
            }
        
        files = [f for f in os.listdir(target_dir) if f.endswith('.mp4')]
        
        if not files:
            return {
                'total_files': 0,
                'total_size': 0,
                'size_mb': 0,
                'date_range': None
            }
        
        total_size = sum(
            os.path.getsize(os.path.join(target_dir, f)) 
            for f in files
        )
        
        # Extract dates from filenames for range
        dates = []
        for filename in files:
            try:
                # Extract date from filename format: CameraName_YYYYMMDD_HHMMSS.mp4
                date_part = filename.split('_')[-2]  # YYYYMMDD
                date_obj = datetime.strptime(date_part, '%Y%m%d')
                dates.append(date_obj)
            except:
                continue
        
        date_range = None
        if dates:
            date_range = {
                'earliest': min(dates).strftime('%Y-%m-%d'),
                'latest': max(dates).strftime('%Y-%m-%d')
            }
        
        return {
            'total_files': len(files),
            'total_size': total_size,
            'size_mb': round(total_size / (1024 * 1024), 2),
            'date_range': date_range,
            'files': files[:10]  # Sample of filenames
        }

# Usage example vÃ  test functions
def test_mock_generator():
    """Test function Ä‘á»ƒ kiá»ƒm tra MockVideoGenerator"""
    generator = MockVideoGenerator()
    
    test_dir = "/tmp/mock_camera_test"
    
    # Test 1: Testing mode (2-minute intervals)
    print("=== Test 1: Testing Mode (2-minute intervals) ===")
    testing_files = generator.generate_daily_videos(
        "Front Door Camera", 
        test_dir, 
        days=1, 
        schedule='testing'
    )
    print(f"Created {len(testing_files)} testing files")
    
    # Test 2: Realtime testing (1-minute intervals)
    print("\n=== Test 2: Realtime Testing Mode (1-minute intervals) ===")
    realtime_files = generator.generate_realtime_testing_videos(
        "Parking Camera",
        test_dir,
        interval_seconds=60,  # 1 phÃºt
        count=10  # 10 files
    )
    print(f"Created {len(realtime_files)} realtime files")
    
    # Test 3: Ultra-fast testing (30-second intervals)
    print("\n=== Test 3: Ultra-Fast Testing (30-second intervals) ===")
    ultrafast_files = generator.generate_realtime_testing_videos(
        "Test Camera",
        test_dir,
        interval_seconds=30,  # 30 giÃ¢y
        count=5  # 5 files
    )
    print(f"Created {len(ultrafast_files)} ultra-fast files")
    
    # Test 4: Statistics
    print("\n=== Test 4: Statistics ===")
    stats = generator.get_mock_statistics(test_dir)
    print(f"Statistics: {stats}")
    
    # Show actual files created
    print("\n=== Files Created ===")
    if os.path.exists(test_dir):
        files = sorted([f for f in os.listdir(test_dir) if f.endswith('.mp4')])
        for i, filename in enumerate(files[:10]):  # Show first 10
            filepath = os.path.join(test_dir, filename)
            size = os.path.getsize(filepath)
            print(f"{i+1:2d}. {filename} ({size} bytes)")
        if len(files) > 10:
            print(f"    ... and {len(files) - 10} more files")
    
    # Cleanup
    import shutil
    if os.path.exists(test_dir):
        shutil.rmtree(test_dir)
        print(f"\nâœ… Cleaned up test directory: {test_dir}")

if __name__ == "__main__":
    test_mock_generator()
```
## ğŸ“„ File: `google_picker_service.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/google_picker_service.py`

```python
#!/usr/bin/env python3
"""
Google Picker Service for VTrack Cloud Integration
Handles Google Picker API token generation with security and rate limiting
"""

import os
import json
import time
import hashlib
import logging
from datetime import datetime
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials

# Configure logging
logger = logging.getLogger(__name__)

class PickerRateLimiter:
    """Rate limiting for Picker token requests"""
    
    def __init__(self, max_requests=10, window_minutes=1):
        self.max_requests = max_requests
        self.window_seconds = window_minutes * 60
        self.requests = {}  # {client_ip: [request_times]}
    
    def is_allowed(self, client_ip):
        """Check if request is allowed under rate limit"""
        current_time = time.time()
        
        # Clean old entries
        if client_ip in self.requests:
            self.requests[client_ip] = [
                req_time for req_time in self.requests[client_ip]
                if current_time - req_time < self.window_seconds
            ]
        else:
            self.requests[client_ip] = []
        
        # Check limit
        if len(self.requests[client_ip]) >= self.max_requests:
            return False
        
        # Add current request
        self.requests[client_ip].append(current_time)
        return True
    
    def get_remaining_requests(self, client_ip):
        """Get number of remaining requests for client"""
        current_time = time.time()
        
        if client_ip not in self.requests:
            return self.max_requests
        
        # Clean old entries
        recent_requests = [
            req_time for req_time in self.requests[client_ip]
            if current_time - req_time < self.window_seconds
        ]
        
        return max(0, self.max_requests - len(recent_requests))

class PickerTokenValidator:
    """Validates tokens and scopes for Picker API"""
    
    REQUIRED_PICKER_SCOPES = [
        'https://www.googleapis.com/auth/drive.readonly',
        'https://www.googleapis.com/auth/drive.metadata.readonly'
    ]
    
    @classmethod
    def validate_scopes(cls, token_scopes):
        """Validate that token has required scopes for Picker"""
        if not token_scopes:
            return False, "No scopes found in token"
        
        missing_scopes = [
            scope for scope in cls.REQUIRED_PICKER_SCOPES 
            if scope not in token_scopes
        ]
        
        if missing_scopes:
            return False, f"Missing required scopes: {', '.join(missing_scopes)}"
        
        return True, "All required scopes present"
    
    @classmethod
    def calculate_expires_in(cls, credentials, default_seconds=3600):
        """Calculate token expiry time in seconds"""
        if not credentials.expiry:
            return default_seconds
        
        time_until_expiry = (credentials.expiry - datetime.now()).total_seconds()
        return min(default_seconds, max(300, int(time_until_expiry)))  # Min 5 minutes

class PickerTokenService:
    """Main service for Google Picker token operations"""
    
    def __init__(self, tokens_directory=None):
        self.tokens_dir = tokens_directory or os.path.join(
            os.path.dirname(__file__), 'tokens'
        )
        self.rate_limiter = PickerRateLimiter(max_requests=10, window_minutes=1)
        self.validator = PickerTokenValidator()
    
    def generate_picker_token(self, client_ip, user_email=None):
        """
        Generate access token for Google Picker API
        
        Args:
            client_ip (str): Client IP address for rate limiting
            user_email (str, optional): Specific user email
            
        Returns:
            dict: Token response or error
        """
        try:
            # Rate limiting check
            if not self.rate_limiter.is_allowed(client_ip):
                remaining = self.rate_limiter.get_remaining_requests(client_ip)
                logger.warning(f"âš ï¸ Rate limit exceeded for IP: {client_ip}")
                return {
                    'success': False,
                    'error_type': 'rate_limit_exceeded',
                    'message': 'Rate limit exceeded. Please wait before requesting another token.',
                    'remaining_requests': remaining,
                    'retry_after_seconds': 60
                }, 429
            
            # Find and load credentials
            credential_data, user_info = self._load_credentials(user_email)
            if not credential_data:
                return {
                    'success': False,
                    'error_type': 'no_credentials',
                    'message': 'No stored credentials found. Please authenticate first.'
                }, 401
            
            # Create credentials object
            credentials = self._create_credentials_object(credential_data)
            
            # Handle token refresh if needed
            refresh_result = self._refresh_token_if_needed(credentials, credential_data)
            if not refresh_result['success']:
                return refresh_result, 401
            
            # Validate scopes for Picker
            scope_valid, scope_message = self.validator.validate_scopes(credentials.scopes)
            if not scope_valid:
                logger.error(f"âŒ Scope validation failed: {scope_message}")
                return {
                    'success': False,
                    'error_type': 'insufficient_scopes',
                    'message': scope_message
                }, 403
            
            # Generate token response
            expires_in = self.validator.calculate_expires_in(credentials)
            issued_at = datetime.now().isoformat()
            
            logger.info(f"âœ… Picker token generated for: {user_info.get('email')}")
            logger.info(f"   Token expires in: {expires_in} seconds")
            
            return {
                'success': True,
                'picker_token': credentials.token,
                'expires_in': expires_in,
                'user_email': user_info.get('email'),
                'user_info': user_info,
                'issued_at': issued_at,
                'scopes': list(credentials.scopes) if credentials.scopes else [],
                'message': 'Picker token generated successfully'
            }, 200
            
        except FileNotFoundError as e:
            logger.error(f"âŒ Credentials file not found: {e}")
            return {
                'success': False,
                'error_type': 'credentials_not_found',
                'message': 'Credentials file not found. Please authenticate first.'
            }, 404
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Invalid credentials file format: {e}")
            return {
                'success': False,
                'error_type': 'invalid_credentials_format',
                'message': 'Invalid credentials file. Please re-authenticate.'
            }, 500
            
        except Exception as e:
            logger.error(f"âŒ Picker token generation error: {e}")
            return {
                'success': False,
                'error_type': 'server_error',
                'message': f'Failed to generate picker token: {str(e)}'
            }, 500
    
    def _load_credentials(self, user_email=None):
        """Load credentials from storage"""
        if not os.path.exists(self.tokens_dir):
            logger.error("âŒ No tokens directory found")
            return None, None
        
        # Get credential files
        token_files = [
            f for f in os.listdir(self.tokens_dir) 
            if f.startswith('google_drive_') and f.endswith('.json')
        ]
        
        if not token_files:
            logger.error("âŒ No stored credentials found")
            return None, None
        
        # Select appropriate credential file
        token_filepath = self._select_credential_file(token_files, user_email)
        if not token_filepath:
            return None, None
        
        # Load credential data
        with open(token_filepath, 'r') as f:
            credential_data = json.load(f)
        
        user_info = credential_data.get('user_info', {})
        return credential_data, user_info
    
    def _select_credential_file(self, token_files, user_email=None):
        """Select appropriate credential file"""
        if user_email:
            # Find credentials for specific user
            email_hash = hashlib.sha256(user_email.encode()).hexdigest()[:16]
            target_filename = f"google_drive_{email_hash}.json"
            target_filepath = os.path.join(self.tokens_dir, target_filename)
            
            if os.path.exists(target_filepath):
                return target_filepath
            else:
                logger.error(f"âŒ No credentials found for user: {user_email}")
                return None
        else:
            # Use first available credentials
            return os.path.join(self.tokens_dir, token_files[0])
    
    def _create_credentials_object(self, credential_data):
        """Create Google credentials object from stored data"""
        return Credentials(
            token=credential_data['token'],
            refresh_token=credential_data['refresh_token'],
            token_uri=credential_data['token_uri'],
            client_id=credential_data['client_id'],
            client_secret=credential_data['client_secret'],
            scopes=credential_data['scopes']
        )
    
    def _refresh_token_if_needed(self, credentials, credential_data):
        """Refresh token if expired"""
        if not credentials.expired:
            return {'success': True}
        
        if not credentials.refresh_token:
            logger.error("âŒ Token expired and no refresh token available")
            return {
                'success': False,
                'error_type': 'refresh_required',
                'message': 'Token expired and cannot be refreshed. Please re-authenticate.'
            }
        
        try:
            logger.info("ğŸ”„ Refreshing expired token...")
            credentials.refresh(Request())
            
            # Update stored credentials with new token
            self._update_stored_credentials(credential_data, credentials)
            
            logger.info("âœ… Token refreshed and stored successfully")
            return {'success': True}
            
        except Exception as refresh_error:
            logger.error(f"âŒ Token refresh failed: {refresh_error}")
            return {
                'success': False,
                'error_type': 'refresh_failed',
                'message': 'Failed to refresh expired token. Please re-authenticate.'
            }
    
    def _update_stored_credentials(self, credential_data, credentials):
        """Update stored credentials with refreshed token"""
        # Find the original file path
        user_info = credential_data.get('user_info', {})
        user_email = user_info.get('email', 'unknown')
        
        email_hash = hashlib.sha256(user_email.encode()).hexdigest()[:16]
        token_filename = f"google_drive_{email_hash}.json"
        token_filepath = os.path.join(self.tokens_dir, token_filename)
        
        # Update credential data
        credential_data['token'] = credentials.token
        credential_data['expires_at'] = credentials.expiry.isoformat() if credentials.expiry else None
        
        # Write back to file
        with open(token_filepath, 'w') as f:
            json.dump(credential_data, f, indent=2)
    
    def get_rate_limit_status(self, client_ip):
        """Get current rate limit status for client"""
        remaining = self.rate_limiter.get_remaining_requests(client_ip)
        return {
            'remaining_requests': remaining,
            'max_requests': self.rate_limiter.max_requests,
            'window_seconds': self.rate_limiter.window_seconds,
            'is_allowed': remaining > 0
        }

# Global service instance
_picker_service = None

def get_picker_service():
    """Get global PickerTokenService instance"""
    global _picker_service
    if _picker_service is None:
        _picker_service = PickerTokenService()
    return _picker_service

def generate_picker_token_for_request(request):
    """
    Convenience function to generate picker token from Flask request
    
    Args:
        request: Flask request object
        
    Returns:
        tuple: (response_dict, status_code)
    """
    service = get_picker_service()
    client_ip = request.remote_addr
    
    # Get user email from request data
    data = request.get_json() or {}
    user_email = data.get('user_email')
    
    return service.generate_picker_token(client_ip, user_email)
```
## ğŸ“„ File: `google_drive_client.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/google_drive_client.py`

```python
# Copy the code from the artifact above
#!/usr/bin/env python3
"""
Google Drive Client for VTrack Cloud Integration
Handles authentication, upload, download, and folder management
"""

import os
import json
import logging
from datetime import datetime
from pathlib import Path

from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GoogleDriveClient:
    """Google Drive API client for VTrack video backup"""
    
    # Google Drive API scopes
    SCOPES = ['https://www.googleapis.com/auth/drive.file']
    
    def __init__(self, credentials_path='google_drive_credentials_web.json', token_path='token.json'):
        """
        Initialize Google Drive client
        
        Args:
            credentials_path (str): Path to OAuth2 credentials file
            token_path (str): Path to store access tokens
        """
        self.credentials_path = os.path.join(os.path.dirname(__file__), credentials_path)
        self.token_path = os.path.join(os.path.dirname(__file__), token_path)
        self.service = None
        self.creds = None
        
        logger.info(f"GoogleDriveClient initialized")
        logger.info(f"Credentials path: {self.credentials_path}")
        logger.info(f"Token path: {self.token_path}")
    
    def authenticate(self):
        """
        Authenticate with Google Drive API using OAuth2
        
        Returns:
            bool: True if authentication successful, False otherwise
        """
        try:
            # Load existing token if available
            if os.path.exists(self.token_path):
                self.creds = Credentials.from_authorized_user_file(self.token_path, self.SCOPES)
            
            # If no valid credentials, start OAuth flow
            if not self.creds or not self.creds.valid:
                if self.creds and self.creds.expired and self.creds.refresh_token:
                    logger.info("Refreshing expired credentials...")
                    self.creds.refresh(Request())
                else:
                    logger.info("Starting OAuth2 authentication flow...")
                    flow = InstalledAppFlow.from_client_secrets_file(
                        self.credentials_path, self.SCOPES)
                    self.creds = flow.run_local_server(port=0)
                
                # Save credentials for next time
                with open(self.token_path, 'w') as token:
                    token.write(self.creds.to_json())
                logger.info("Credentials saved successfully")
            
            # Build Drive service
            self.service = build('drive', 'v3', credentials=self.creds)
            logger.info("âœ… Google Drive authentication successful")
            return True
            
        except FileNotFoundError:
            logger.error(f"âŒ Credentials file not found: {self.credentials_path}")
            return False
        except Exception as e:
            logger.error(f"âŒ Authentication failed: {e}")
            return False
    
    def test_connection(self):
        """
        Test Google Drive API connection
        
        Returns:
            dict: Connection test result
        """
        try:
            if not self.service:
                if not self.authenticate():
                    return {"success": False, "message": "Authentication failed"}
            
            # Test API call - get user info
            about = self.service.about().get(fields='user, storageQuota').execute()
            user_email = about.get('user', {}).get('emailAddress', 'Unknown')
            
            # Storage info
            quota = about.get('storageQuota', {})
            total_gb = int(quota.get('limit', 0)) / (1024**3) if quota.get('limit') else 'Unlimited'
            used_gb = int(quota.get('usage', 0)) / (1024**3)
            
            logger.info(f"âœ… Connected to Google Drive: {user_email}")
            
            return {
                "success": True,
                "message": f"Connected to Google Drive: {user_email}",
                "user_email": user_email,
                "storage_total_gb": total_gb,
                "storage_used_gb": round(used_gb, 2)
            }
            
        except HttpError as e:
            logger.error(f"âŒ Google Drive API error: {e}")
            return {"success": False, "message": f"API error: {e}"}
        except Exception as e:
            logger.error(f"âŒ Connection test failed: {e}")
            return {"success": False, "message": f"Connection failed: {str(e)}"}
    
    def create_folder(self, folder_name, parent_folder_id='root'):
        """
        Create folder in Google Drive
        
        Args:
            folder_name (str): Name of folder to create
            parent_folder_id (str): Parent folder ID ('root' for root directory)
            
        Returns:
            str: Created folder ID, None if failed
        """
        try:
            folder_metadata = {
                'name': folder_name,
                'mimeType': 'application/vnd.google-apps.folder',
                'parents': [parent_folder_id]
            }
            
            folder = self.service.files().create(body=folder_metadata, fields='id').execute()
            folder_id = folder.get('id')
            
            logger.info(f"âœ… Folder created: {folder_name} (ID: {folder_id})")
            return folder_id
            
        except HttpError as e:
            logger.error(f"âŒ Failed to create folder {folder_name}: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ Folder creation error: {e}")
            return None
    
    def find_folder(self, folder_name, parent_folder_id='root'):
        """
        Find folder by name in Google Drive
        
        Args:
            folder_name (str): Name of folder to find
            parent_folder_id (str): Parent folder ID to search in
            
        Returns:
            str: Folder ID if found, None otherwise
        """
        try:
            query = f"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and '{parent_folder_id}' in parents"
            results = self.service.files().list(q=query, fields='files(id, name)').execute()
            folders = results.get('files', [])
            
            if folders:
                folder_id = folders[0]['id']
                logger.info(f"âœ… Found folder: {folder_name} (ID: {folder_id})")
                return folder_id
            else:
                logger.info(f"ğŸ“ Folder not found: {folder_name}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ Folder search error: {e}")
            return None
    
    def get_or_create_folder(self, folder_name, parent_folder_id='root'):
        """
        Get existing folder or create new one
        
        Args:
            folder_name (str): Name of folder
            parent_folder_id (str): Parent folder ID
            
        Returns:
            str: Folder ID
        """
        folder_id = self.find_folder(folder_name, parent_folder_id)
        if not folder_id:
            folder_id = self.create_folder(folder_name, parent_folder_id)
        return folder_id
    
    def upload_file(self, file_path, folder_id='root', custom_name=None):
        """
        Upload file to Google Drive
        
        Args:
            file_path (str): Local file path to upload
            folder_id (str): Google Drive folder ID to upload to
            custom_name (str): Custom name for uploaded file
            
        Returns:
            dict: Upload result with file ID and metadata
        """
        try:
            if not os.path.exists(file_path):
                return {"success": False, "message": f"File not found: {file_path}"}
            
            file_name = custom_name or os.path.basename(file_path)
            file_size = os.path.getsize(file_path)
            
            logger.info(f"ğŸ“¤ Uploading {file_name} ({file_size} bytes) to Google Drive...")
            
            # File metadata
            file_metadata = {
                'name': file_name,
                'parents': [folder_id]
            }
            
            # Create media upload
            media = MediaFileUpload(file_path, resumable=True)
            
            # Upload file
            file = self.service.files().create(
                body=file_metadata,
                media_body=media,
                fields='id, name, size, createdTime'
            ).execute()
            
            file_id = file.get('id')
            upload_size = file.get('size')
            created_time = file.get('createdTime')
            
            logger.info(f"âœ… Upload successful: {file_name}")
            logger.info(f"   File ID: {file_id}")
            logger.info(f"   Size: {upload_size} bytes")
            
            return {
                "success": True,
                "message": f"File uploaded successfully: {file_name}",
                "file_id": file_id,
                "file_name": file_name,
                "file_size": upload_size,
                "created_time": created_time,
                "drive_url": f"https://drive.google.com/file/d/{file_id}/view"
            }
            
        except HttpError as e:
            logger.error(f"âŒ Upload failed: {e}")
            return {"success": False, "message": f"Upload failed: {e}"}
        except Exception as e:
            logger.error(f"âŒ Upload error: {e}")
            return {"success": False, "message": f"Upload error: {str(e)}"}
    
    def upload_video(self, video_path, camera_name=None):
        """
        Upload video file with organized folder structure
        
        Args:
            video_path (str): Path to video file
            camera_name (str): Camera name for folder organization
            
        Returns:
            dict: Upload result
        """
        try:
            # Create VTrack main folder
            vtrack_folder_id = self.get_or_create_folder("VTrack_Videos")
            
            # Create camera folder if specified
            if camera_name:
                camera_folder_id = self.get_or_create_folder(camera_name, vtrack_folder_id)
                upload_folder_id = camera_folder_id
            else:
                upload_folder_id = vtrack_folder_id
            
            # Generate custom filename with timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            original_name = Path(video_path).stem
            extension = Path(video_path).suffix
            custom_name = f"{original_name}_{timestamp}{extension}"
            
            # Upload file
            result = self.upload_file(video_path, upload_folder_id, custom_name)
            
            if result["success"]:
                result["folder_structure"] = f"VTrack_Videos/{camera_name or 'General'}"
                
            return result
            
        except Exception as e:
            logger.error(f"âŒ Video upload error: {e}")
            return {"success": False, "message": f"Video upload failed: {str(e)}"}
    
    def list_files(self, folder_id='root', limit=10):
        """
        List files in Google Drive folder
        
        Args:
            folder_id (str): Folder ID to list files from
            limit (int): Maximum number of files to return
            
        Returns:
            list: List of file metadata
        """
        try:
            query = f"'{folder_id}' in parents"
            results = self.service.files().list(
                q=query,
                pageSize=limit,
                fields='files(id, name, size, createdTime, mimeType)'
            ).execute()
            
            files = results.get('files', [])
            logger.info(f"ğŸ“‹ Found {len(files)} files in folder")
            
            return files
            
        except Exception as e:
            logger.error(f"âŒ File listing error: {e}")
            return []
    
    def download_file(self, file_id, download_path):
        """
        Download file from Google Drive
        
        Args:
            file_id (str): Google Drive file ID
            download_path (str): Local path to save file
            
        Returns:
            dict: Download result
        """
        try:
            # Get file metadata
            file_metadata = self.service.files().get(fileId=file_id).execute()
            file_name = file_metadata.get('name')
            
            logger.info(f"ğŸ“¥ Downloading {file_name} from Google Drive...")
            
            # Download file content
            request = self.service.files().get_media(fileId=file_id)
            
            with open(download_path, 'wb') as file:
                downloader = MediaIoBaseDownload(file, request)
                done = False
                while done is False:
                    status, done = downloader.next_chunk()
                    if status:
                        logger.info(f"Download progress: {int(status.progress() * 100)}%")
            
            logger.info(f"âœ… Download completed: {download_path}")
            
            return {
                "success": True,
                "message": f"File downloaded successfully: {file_name}",
                "file_name": file_name,
                "local_path": download_path
            }
            
        except Exception as e:
            logger.error(f"âŒ Download error: {e}")
            return {"success": False, "message": f"Download failed: {str(e)}"}
    
    def delete_file(self, file_id):
        """
        Delete file from Google Drive
        
        Args:
            file_id (str): Google Drive file ID
            
        Returns:
            dict: Deletion result
        """
        try:
            self.service.files().delete(fileId=file_id).execute()
            logger.info(f"âœ… File deleted: {file_id}")
            
            return {
                "success": True,
                "message": f"File deleted successfully: {file_id}"
            }
            
        except Exception as e:
            logger.error(f"âŒ Delete error: {e}")
            return {"success": False, "message": f"Delete failed: {str(e)}"}


def test_google_drive_client():
    """
    Test function for Google Drive client
    """
    print("ğŸ”§ Testing Google Drive Client...")
    
    # Initialize client
    client = GoogleDriveClient()
    
    # Test authentication
    print("\n1. Testing authentication...")
    auth_result = client.authenticate()
    print(f"Authentication: {'âœ… Success' if auth_result else 'âŒ Failed'}")
    
    if not auth_result:
        return
    
    # Test connection
    print("\n2. Testing connection...")
    conn_result = client.test_connection()
    print(f"Connection: {'âœ… Success' if conn_result['success'] else 'âŒ Failed'}")
    print(f"Message: {conn_result['message']}")
    
    if conn_result['success']:
        print(f"User: {conn_result.get('user_email', 'Unknown')}")
        print(f"Storage: {conn_result.get('storage_used_gb', 0):.2f}GB used")
    
    # Test folder creation
    print("\n3. Testing folder creation...")
    vtrack_folder_id = client.get_or_create_folder("VTrack_Test")
    if vtrack_folder_id:
        print(f"âœ… VTrack_Test folder ready: {vtrack_folder_id}")
    
    # Test file listing
    print("\n4. Testing file listing...")
    files = client.list_files(limit=5)
    print(f"âœ… Found {len(files)} files in root directory")
    
    print("\nğŸ‰ Google Drive Client test completed!")


if __name__ == "__main__":
    test_google_drive_client()
```
## ğŸ“„ File: `cloud_auth.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/cloud_auth.py`

```python
#!/usr/bin/env python3
"""
Cloud Authentication Handler for VTrack
Manages OAuth2 flows, token storage, and session management
Supports Google Drive OAuth2 with extensible design for other providers
"""

import os
import json
import uuid
import logging
import hashlib
import secrets
from datetime import datetime, timedelta
from typing import Dict, Optional, Any, Tuple
from pathlib import Path
import threading
import time

# OAuth2 and Google API imports
try:
    from google.auth.transport.requests import Request
    from google.oauth2.credentials import Credentials
    from google_auth_oauthlib.flow import Flow
    from googleapiclient.discovery import build
    from googleapiclient.errors import HttpError
except ImportError:
    logging.warning("Google API libraries not installed. Run: pip install google-auth google-auth-oauthlib google-api-python-client")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CloudAuthManager:
    """
    Manages OAuth2 authentication flows for cloud providers
    Handles token storage, refresh, and session management
    """
    
    # OAuth2 scopes for different providers
    PROVIDER_SCOPES = {
        'google_drive': [
            'https://www.googleapis.com/auth/drive.readonly',
            'https://www.googleapis.com/auth/drive.metadata.readonly'
        ]
    }
    
    # OAuth2 endpoints
    OAUTH_ENDPOINTS = {
        'google_drive': {
            'auth_uri': 'https://accounts.google.com/o/oauth2/auth',
            'token_uri': 'https://oauth2.googleapis.com/token',
            'client_secrets_file': 'google_drive_credentials_web.json'
        }
    }
    
    def __init__(self, provider: str = 'google_drive', base_dir: Optional[str] = None):
        """
        Initialize CloudAuthManager
        
        Args:
            provider (str): Cloud provider ('google_drive', etc.)
            base_dir (str): Base directory for credential storage
        """
        self.provider = provider
        self.base_dir = base_dir or os.path.dirname(__file__)
        
        # Authentication state
        self.auth_sessions = {}  # Active auth sessions
        self.credentials_cache = {}  # Cached credentials
        
        # Thread safety
        self.auth_lock = threading.Lock()
        
        # Credential file paths
        self.credentials_dir = os.path.join(self.base_dir, 'credentials')
        self.tokens_dir = os.path.join(self.base_dir, 'tokens')
        
        # Ensure directories exist
        os.makedirs(self.credentials_dir, exist_ok=True)
        os.makedirs(self.tokens_dir, exist_ok=True)
        
        logger.info(f"CloudAuthManager initialized for {provider}")
    
    def initiate_oauth_flow(self, redirect_uri: str = 'http://localhost:8080/oauth/callback') -> Dict[str, Any]:
        """
        Initiate OAuth2 authentication flow
        
        Args:
            redirect_uri (str): OAuth2 redirect URI
            
        Returns:
            dict: OAuth flow information including auth URL
        """
        try:
            logger.info(f"ğŸ” Initiating OAuth2 flow for {self.provider}")
            
            if self.provider == 'google_drive':
                return self._initiate_google_oauth(redirect_uri)
            else:
                return {
                    'success': False,
                    'message': f"OAuth2 not implemented for {self.provider}"
                }
                
        except Exception as e:
            logger.error(f"âŒ OAuth2 initiation failed: {e}")
            return {
                'success': False,
                'message': f"OAuth2 initiation failed: {str(e)}",
                'error': str(e)
            }
    
    def _initiate_google_oauth(self, redirect_uri: str) -> Dict[str, Any]:
        """Initiate Google Drive OAuth2 flow"""
        try:
            # Get client secrets file path
            client_secrets_file = os.path.join(
                self.credentials_dir, 
                self.OAUTH_ENDPOINTS['google_drive']['client_secrets_file']
            )
            
            if not os.path.exists(client_secrets_file):
                return {
                    'success': False,
                    'message': f"Google Drive credentials file not found: {client_secrets_file}",
                    'setup_required': True
                }
            
            # Create OAuth2 flow
            flow = Flow.from_client_secrets_file(
                client_secrets_file,
                scopes=self.PROVIDER_SCOPES['google_drive'],
                redirect_uri=redirect_uri
            )
            
            # Generate state parameter for security
            state = secrets.token_urlsafe(32)
            
            # Generate authorization URL
            auth_url, _ = flow.authorization_url(
                access_type='offline',
                include_granted_scopes='true',
                state=state,
                prompt='consent'  # Force consent to get refresh token
            )
            
            # Store flow in session for later completion
            session_id = str(uuid.uuid4())
            
            with self.auth_lock:
                self.auth_sessions[session_id] = {
                    'flow': flow,
                    'state': state,
                    'provider': self.provider,
                    'created_at': datetime.now().isoformat(),
                    'redirect_uri': redirect_uri,
                    'status': 'pending'
                }
            
            logger.info(f"âœ… Google OAuth2 flow created with session: {session_id}")
            
            return {
                'success': True,
                'auth_url': auth_url,
                'session_id': session_id,
                'state': state,
                'provider': self.provider,
                'message': 'OAuth2 flow initiated successfully'
            }
            
        except Exception as e:
            logger.error(f"âŒ Google OAuth2 initiation error: {e}")
            return {
                'success': False,
                'message': f"Google OAuth2 initiation failed: {str(e)}",
                'error': str(e)
            }
    
    def complete_oauth_flow(self, session_id: str, authorization_code: str, state: str) -> Dict[str, Any]:
        """
        Complete OAuth2 flow with authorization code
        
        Args:
            session_id (str): OAuth session ID
            authorization_code (str): OAuth authorization code
            state (str): OAuth state parameter
            
        Returns:
            dict: OAuth completion result with credentials
        """
        try:
            logger.info(f"ğŸ” Completing OAuth2 flow for session: {session_id}")
            
            # Retrieve session
            with self.auth_lock:
                if session_id not in self.auth_sessions:
                    return {
                        'success': False,
                        'message': 'OAuth session not found or expired'
                    }
                
                session = self.auth_sessions[session_id]
                
                # Validate state parameter
                if session['state'] != state:
                    return {
                        'success': False,
                        'message': 'OAuth state mismatch - potential security issue'
                    }
                
                # Complete flow
                flow = session['flow']
            
            # Exchange authorization code for tokens
            flow.fetch_token(code=authorization_code)
            credentials = flow.credentials
            
            # Validate credentials
            if not credentials.valid:
                return {
                    'success': False,
                    'message': 'Invalid credentials received from OAuth flow'
                }
            
            # Get user information
            user_info = self._get_user_info(credentials)
            
            # Store credentials securely
            credential_storage_result = self._store_credentials(credentials, user_info)
            
            if not credential_storage_result['success']:
                return credential_storage_result
            
            # Update session status
            with self.auth_lock:
                if session_id in self.auth_sessions:
                    self.auth_sessions[session_id]['status'] = 'completed'
                    self.auth_sessions[session_id]['completed_at'] = datetime.now().isoformat()
            
            logger.info(f"âœ… OAuth2 flow completed for: {user_info.get('email', 'Unknown')}")
            
            return {
                'success': True,
                'message': 'OAuth2 authentication completed successfully',
                'user_info': user_info,
                'credentials': {
                    'token': credentials.token,
                    'refresh_token': credentials.refresh_token,
                    'token_uri': credentials.token_uri,
                    'client_id': credentials.client_id,
                    'client_secret': credentials.client_secret,
                    'scopes': credentials.scopes
                },
                'provider': self.provider,
                'authenticated_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ OAuth2 completion error: {e}")
            return {
                'success': False,
                'message': f"OAuth2 completion failed: {str(e)}",
                'error': str(e)
            }
    
    def _get_user_info(self, credentials: Credentials) -> Dict[str, Any]:
        """
        Get user information from OAuth2 credentials
        
        Args:
            credentials: OAuth2 credentials
            
        Returns:
            dict: User information
        """
        try:
            if self.provider == 'google_drive':
                return self._get_google_user_info(credentials)
            else:
                return {'email': 'unknown', 'name': 'Unknown User'}
                
        except Exception as e:
            logger.error(f"âŒ Error getting user info: {e}")
            return {'email': 'unknown', 'name': 'Unknown User', 'error': str(e)}
    
    def _get_google_user_info(self, credentials: Credentials) -> Dict[str, Any]:
        """Get Google user information"""
        try:
            # Build Drive service to get user info
            service = build('drive', 'v3', credentials=credentials)
            about = service.about().get(fields='user,storageQuota').execute()
            
            user = about.get('user', {})
            quota = about.get('storageQuota', {})
            
            return {
                'email': user.get('emailAddress', 'unknown'),
                'name': user.get('displayName', 'Unknown User'),
                'photo_url': user.get('photoLink'),
                'storage_used_gb': int(quota.get('usage', 0)) / (1024**3) if quota.get('usage') else 0,
                'storage_total_gb': int(quota.get('limit', 0)) / (1024**3) if quota.get('limit') else 'Unlimited'
            }
            
        except Exception as e:
            logger.error(f"âŒ Google user info error: {e}")
            return {'email': 'unknown', 'name': 'Unknown User', 'error': str(e)}
    
    def _store_credentials(self, credentials: Credentials, user_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        Securely store OAuth2 credentials
        
        Args:
            credentials: OAuth2 credentials
            user_info: User information
            
        Returns:
            dict: Storage result
        """
        try:
            # Create credential data
            credential_data = {
                'token': credentials.token,
                'refresh_token': credentials.refresh_token,
                'token_uri': credentials.token_uri,
                'client_id': credentials.client_id,
                'client_secret': credentials.client_secret,
                'scopes': list(credentials.scopes) if credentials.scopes else [],
                'provider': self.provider,
                'user_info': user_info,
                'created_at': datetime.now().isoformat(),
                'expires_at': credentials.expiry.isoformat() if credentials.expiry else None
            }
            
            # Generate secure filename based on user email
            user_email = user_info.get('email', 'unknown')
            email_hash = hashlib.sha256(user_email.encode()).hexdigest()[:16]
            token_filename = f"{self.provider}_{email_hash}.json"
            token_filepath = os.path.join(self.tokens_dir, token_filename)
            
            # Store credentials
            with open(token_filepath, 'w') as f:
                json.dump(credential_data, f, indent=2)
            
            # Set restrictive permissions
            os.chmod(token_filepath, 0o600)
            
            # Cache credentials
            with self.auth_lock:
                self.credentials_cache[user_email] = {
                    'credentials': credentials,
                    'user_info': user_info,
                    'filepath': token_filepath,
                    'cached_at': datetime.now()
                }
            
            logger.info(f"âœ… Credentials stored for: {user_email}")
            
            return {
                'success': True,
                'message': 'Credentials stored successfully',
                'filepath': token_filepath
            }
            
        except Exception as e:
            logger.error(f"âŒ Credential storage error: {e}")
            return {
                'success': False,
                'message': f"Credential storage failed: {str(e)}",
                'error': str(e)
            }
    
    def load_stored_credentials(self, user_email: Optional[str] = None) -> Optional[Credentials]:
        """
        Load stored credentials for user
        
        Args:
            user_email (str): User email (if None, loads first available)
            
        Returns:
            Credentials: OAuth2 credentials if found
        """
        try:
            # Check cache first
            if user_email and user_email in self.credentials_cache:
                cached = self.credentials_cache[user_email]
                if (datetime.now() - cached['cached_at']).seconds < 3600:  # 1 hour cache
                    logger.info(f"âœ… Using cached credentials for: {user_email}")
                    return cached['credentials']
            
            # Load from file
            token_files = []
            for filename in os.listdir(self.tokens_dir):
                if filename.startswith(f"{self.provider}_") and filename.endswith('.json'):
                    token_files.append(filename)
            
            if not token_files:
                logger.info("ğŸ“­ No stored credentials found")
                return None
            
            # If specific user requested, find their file
            if user_email:
                email_hash = hashlib.sha256(user_email.encode()).hexdigest()[:16]
                target_filename = f"{self.provider}_{email_hash}.json"
                if target_filename in token_files:
                    token_files = [target_filename]
                else:
                    logger.warning(f"âš ï¸ No credentials found for: {user_email}")
                    return None
            
            # Load the first (or specified) credential file
            token_filepath = os.path.join(self.tokens_dir, token_files[0])
            
            with open(token_filepath, 'r') as f:
                credential_data = json.load(f)
            
            # Reconstruct credentials
            credentials = Credentials(
                token=credential_data['token'],
                refresh_token=credential_data['refresh_token'],
                token_uri=credential_data['token_uri'],
                client_id=credential_data['client_id'],
                client_secret=credential_data['client_secret'],
                scopes=credential_data['scopes']
            )
            
            # Refresh if expired
            if credentials.expired and credentials.refresh_token:
                logger.info("ğŸ”„ Refreshing expired credentials...")
                credentials.refresh(Request())
                
                # Update stored credentials
                credential_data['token'] = credentials.token
                credential_data['expires_at'] = credentials.expiry.isoformat() if credentials.expiry else None
                
                with open(token_filepath, 'w') as f:
                    json.dump(credential_data, f, indent=2)
            
            # Cache credentials
            stored_user_email = credential_data['user_info']['email']
            with self.auth_lock:
                self.credentials_cache[stored_user_email] = {
                    'credentials': credentials,
                    'user_info': credential_data['user_info'],
                    'filepath': token_filepath,
                    'cached_at': datetime.now()
                }
            
            logger.info(f"âœ… Loaded credentials for: {stored_user_email}")
            return credentials
            
        except Exception as e:
            logger.error(f"âŒ Error loading credentials: {e}")
            return None
    
    def get_authentication_status(self, user_email: Optional[str] = None) -> Dict[str, Any]:
        """
        Get current authentication status
        
        Args:
            user_email (str): Optional user email to check
            
        Returns:
            dict: Authentication status
        """
        try:
            credentials = self.load_stored_credentials(user_email)
            
            if credentials and credentials.valid:
                # Get user info from cache or credentials
                user_info = {}
                if user_email and user_email in self.credentials_cache:
                    user_info = self.credentials_cache[user_email]['user_info']
                
                return {
                    'authenticated': True,
                    'provider': self.provider,
                    'user_email': user_info.get('email', 'unknown'),
                    'user_name': user_info.get('name', 'Unknown'),
                    'expires_at': credentials.expiry.isoformat() if credentials.expiry else None,
                    'scopes': list(credentials.scopes) if credentials.scopes else [],
                    'last_check': datetime.now().isoformat()
                }
            else:
                return {
                    'authenticated': False,
                    'provider': self.provider,
                    'message': 'No valid credentials found',
                    'last_check': datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"âŒ Authentication status check error: {e}")
            return {
                'authenticated': False,
                'provider': self.provider,
                'error': str(e),
                'last_check': datetime.now().isoformat()
            }
    
    def revoke_credentials(self, user_email: Optional[str] = None) -> Dict[str, Any]:
        """
        Revoke stored credentials
        
        Args:
            user_email (str): User email (if None, revokes all)
            
        Returns:
            dict: Revocation result
        """
        try:
            revoked_count = 0
            
            if user_email:
                # Revoke specific user credentials
                email_hash = hashlib.sha256(user_email.encode()).hexdigest()[:16]
                token_filename = f"{self.provider}_{email_hash}.json"
                token_filepath = os.path.join(self.tokens_dir, token_filename)
                
                if os.path.exists(token_filepath):
                    os.remove(token_filepath)
                    revoked_count = 1
                    
                    # Remove from cache
                    with self.auth_lock:
                        if user_email in self.credentials_cache:
                            del self.credentials_cache[user_email]
            else:
                # Revoke all credentials for provider
                for filename in os.listdir(self.tokens_dir):
                    if filename.startswith(f"{self.provider}_") and filename.endswith('.json'):
                        os.remove(os.path.join(self.tokens_dir, filename))
                        revoked_count += 1
                
                # Clear cache
                with self.auth_lock:
                    self.credentials_cache.clear()
            
            logger.info(f"ğŸ”Œ Revoked {revoked_count} credential(s) for {self.provider}")
            
            return {
                'success': True,
                'message': f"Revoked {revoked_count} credential(s)",
                'revoked_count': revoked_count,
                'provider': self.provider
            }
            
        except Exception as e:
            logger.error(f"âŒ Credential revocation error: {e}")
            return {
                'success': False,
                'message': f"Credential revocation failed: {str(e)}",
                'error': str(e)
            }
    
    def cleanup_expired_sessions(self, max_age_hours: int = 1):
        """
        Clean up expired OAuth sessions
        
        Args:
            max_age_hours (int): Maximum session age in hours
        """
        try:
            cutoff_time = datetime.now() - timedelta(hours=max_age_hours)
            expired_sessions = []
            
            with self.auth_lock:
                for session_id, session_data in list(self.auth_sessions.items()):
                    created_at = datetime.fromisoformat(session_data['created_at'])
                    if created_at < cutoff_time:
                        expired_sessions.append(session_id)
                
                for session_id in expired_sessions:
                    del self.auth_sessions[session_id]
            
            if expired_sessions:
                logger.info(f"ğŸ§¹ Cleaned up {len(expired_sessions)} expired OAuth sessions")
                
        except Exception as e:
            logger.error(f"âŒ Session cleanup error: {e}")


def test_cloud_auth():
    """
    Test function for CloudAuthManager functionality
    """
    print("ğŸ”§ Testing CloudAuthManager...")
    
    try:
        # Initialize CloudAuthManager
        print("\n1. Initializing CloudAuthManager...")
        auth_manager = CloudAuthManager('google_drive')
        print("âœ… CloudAuthManager initialized")
        
        # Check authentication status
        print("\n2. Checking authentication status...")
        auth_status = auth_manager.get_authentication_status()
        print(f"âœ… Authenticated: {auth_status['authenticated']}")
        
        if auth_status['authenticated']:
            print(f"   User: {auth_status.get('user_email', 'Unknown')}")
        else:
            print("   No existing credentials found")
        
        # Load stored credentials (if any)
        print("\n3. Loading stored credentials...")
        credentials = auth_manager.load_stored_credentials()
        
        if credentials:
            print(f"âœ… Credentials loaded successfully")
            print(f"   Valid: {credentials.valid}")
            print(f"   Scopes: {list(credentials.scopes) if credentials.scopes else 'None'}")
        else:
            print("ğŸ“­ No stored credentials found")
        
        print("\nğŸ‰ CloudAuthManager test completed!")
        
    except Exception as e:
        print(f"âŒ CloudAuthManager test failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_cloud_auth()

```
## ğŸ“„ File: `nvr_client.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/nvr_client.py`

```python
# nvr_client.py - Fixed Authentication for ZM v1.34.26
import requests
import logging
import socket
import random
import json
import os
from typing import Dict, List, Tuple
from datetime import datetime, timedelta
from .onvif_client import onvif_client

class NVRClient:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.session = requests.Session()
        self.base_url = None
        # ğŸ†• JWT Token storage
        self.access_token = None
        self.auth_credentials = None
    
    def _authenticate_zoneminder(self, username: str, password: str) -> bool:
        """ğŸ”§ DEBUG: ZoneMinder authentication with detailed logging"""
        try:
            auth_data = {
                'user': username,
                'pass': password
            }
            
            login_url = f"{self.base_url}/host/login.json"
            self.logger.info(f"ğŸ” Attempting login to: {login_url}")
            self.logger.info(f"ğŸ” Login data: user={username}, pass=***")
            
            response = self.session.post(login_url, data=auth_data, timeout=10)
            
            self.logger.info(f"ğŸ” Login response status: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                self.logger.info(f"ğŸ” Login response data: {json.dumps(data, indent=2)}")
                
                # ğŸ¯ PRIORITY: Use auth credentials (works with current ZM config)
                if 'credentials' in data and data.get('credentials'):
                    self.auth_credentials = data['credentials']  # "auth=abc123"
                    self.logger.info(f"âœ… ZoneMinder auth hash successful: {self.auth_credentials}")
                    return True
                
                # Fallback: JWT token (if ZM configured differently)
                elif 'access_token' in data and data.get('access_token'):
                    self.access_token = data['access_token']
                    self.session.headers.update({
                        'Authorization': f'Bearer {self.access_token}'
                    })
                    self.logger.info("âœ… ZoneMinder JWT authentication successful")
                    return True
                
                # Simple success
                elif data.get('success', False):
                    self.logger.info("âœ… ZoneMinder basic auth successful")
                    return True
                
                else:
                    self.logger.error("âŒ ZoneMinder authentication failed - no valid response")
                    return False
            else:
                self.logger.error(f"âŒ ZoneMinder auth request failed: {response.status_code}")
                self.logger.error(f"âŒ Response text: {response.text}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ZoneMinder authentication error: {e}")
            return False
    
    def _make_authenticated_request(self, url: str, **kwargs):
        """ğŸ”§ DEBUG: Make request with detailed logging"""
        
        self.logger.info(f"ğŸŒ Making request to: {url}")
        
        # Priority: Auth credentials (current working method)
        if self.auth_credentials:
            separator = '&' if '?' in url else '?'
            auth_url = f"{url}{separator}{self.auth_credentials}"
            self.logger.info(f"ğŸ”‘ Using auth hash URL: {auth_url}")
            response = self.session.get(auth_url, **kwargs)
            self.logger.info(f"ğŸŒ Auth hash response status: {response.status_code}")
            if response.status_code != 200:
                self.logger.error(f"âŒ Auth hash response error: {response.text[:200]}")
            return response
        
        # Fallback: JWT token
        elif self.access_token:
            self.logger.info(f"ğŸ”‘ Using JWT token in headers")
            response = self.session.get(url, **kwargs)
            self.logger.info(f"ğŸŒ JWT response status: {response.status_code}")
            return response
        
        # No auth - try direct request
        else:
            self.logger.info(f"ğŸ”“ No auth - direct request")
            response = self.session.get(url, **kwargs)
            self.logger.info(f"ğŸŒ Direct response status: {response.status_code}")
            return response

    def _discover_zoneminder_real(self, url: str, config: dict) -> dict:
        """ğŸ”§ FIXED: Auth first, then version check"""
        self.base_url = f"http://{url}/zm/api"
        username = config.get('username', '')
        password = config.get('password', '')
        
        self.logger.info(f"ğŸ¯ === ZONEMINDER DISCOVERY START ===")
        self.logger.info(f"ğŸ¯ Base URL: {self.base_url}")
        self.logger.info(f"ğŸ¯ Username: {username}")
        self.logger.info(f"ğŸ¯ Password provided: {'Yes' if password else 'No'}")
        
        try:
            # Step 1: Authentication FIRST (if credentials provided)
            if username and password:
                self.logger.info(f"ğŸ” Step 1: Authenticating with credentials")
                auth_success = self._authenticate_zoneminder(username, password)
                if not auth_success:
                    error_msg = "ZoneMinder authentication failed"
                    self.logger.error(f"âŒ {error_msg}")
                    return self._error_response(error_msg)
            elif username or password:
                error_msg = "Both username and password required for authentication"
                self.logger.error(f"âŒ {error_msg}")
                return self._error_response(error_msg)
            else:
                self.logger.info(f"ğŸ”“ Step 1: No credentials provided - skipping auth")
            
            # Step 2: Test connectivity & get version (AFTER auth)
            self.logger.info(f"ğŸ“¡ Step 2: Testing version endpoint")
            version_response = self._make_authenticated_request(f"{self.base_url}/host/getVersion.json", timeout=10)
            if version_response.status_code != 200:
                error_msg = f"ZoneMinder API not accessible. Status: {version_response.status_code}"
                self.logger.error(f"âŒ {error_msg}")
                return self._error_response(error_msg)
            
            version_data = version_response.json()
            zm_version = version_data.get('version', 'Unknown')
            api_version = version_data.get('apiversion', 'Unknown')
            
            self.logger.info(f"âœ… ZoneMinder version: {zm_version}, API: {api_version}")
            
            # Step 3: Get real monitors with authentication
            self.logger.info(f"ğŸ“¹ Step 3: Getting monitors")
            monitors_response = self._make_authenticated_request(f"{self.base_url}/monitors.json", timeout=10)
            if monitors_response.status_code != 200:
                if monitors_response.status_code == 401:
                    error_msg = "Authentication required but failed. Check username/password."
                    self.logger.error(f"âŒ {error_msg}")
                    return self._error_response(error_msg)
                else:
                    error_msg = f"Failed to get monitors. Status: {monitors_response.status_code}"
                    self.logger.error(f"âŒ {error_msg}")
                    return self._error_response(error_msg)
            
            monitors_data = monitors_response.json()
            monitors = monitors_data.get('monitors', [])
            
            self.logger.info(f"âœ… Found {len(monitors)} monitors")
            
            if not monitors:
                error_msg = "No monitors found in ZoneMinder"
                self.logger.error(f"âŒ {error_msg}")
                return self._error_response(error_msg)
            
            # Step 4: Process real monitor data
            cameras = []
            for monitor_data in monitors:
                monitor = monitor_data.get('Monitor', {})
                monitor_status = monitor_data.get('Monitor_Status', {})
                
                monitor_id = monitor.get('Id')
                monitor_name = monitor.get('Name', f"Monitor_{monitor_id}")
                
                camera_info = {
                    "id": f"zm_monitor_{monitor_id}",
                    "name": monitor_name,
                    "description": f"ZM {monitor_name} ({monitor.get('Function', 'Record')})",
                    "zm_id": monitor_id,
                    "status": monitor_status.get('Status', 'Unknown'),
                    "resolution": f"{monitor.get('Width', 'Unknown')}x{monitor.get('Height', 'Unknown')}",
                    "function": monitor.get('Function', 'Record'),
                    "enabled": monitor.get('Enabled') == '1',
                    "type": monitor.get('Type', 'Unknown'),
                    "fps": {
                        "capture": float(monitor_status.get('CaptureFPS', '0.00')),
                        "analysis": float(monitor_status.get('AnalysisFPS', '0.00'))
                    },
                    "capabilities": self._get_zm_capabilities(monitor)
                }
                
                # Add path information
                monitor_path = monitor.get('Path', '')
                if monitor.get('Type') == 'File' and monitor_path:
                    camera_info['file_path'] = monitor_path
                elif monitor_path.startswith('rtsp://') or monitor_path.startswith('http://'):
                    camera_info['stream_url'] = monitor_path
                
                cameras.append(camera_info)
                
                self.logger.info(f"ğŸ“¹ Found ZM monitor: {monitor_name} (ID: {monitor_id}, Status: {camera_info['status']})")
            
            # Step 5: Get system information
            self.logger.info(f"ğŸ”§ Step 5: Getting system info")
            system_info = self._get_zoneminder_system_info()
            
            success_result = {
                "accessible": True,
                "message": f"ZoneMinder connection successful - Discovered {len(cameras)} camera(s)",
                "source_type": "nvr",
                "protocol": "zoneminder",
                "cameras": cameras,
                "device_info": {
                    "manufacturer": "ZoneMinder",
                    "model": "Open Source NVR",
                    "firmware": zm_version,
                    "api_version": api_version,
                    "disk_usage": system_info.get('disk_usage', 'Unknown'),
                    "total_cameras": len(cameras),
                    "api_url": self.base_url,
                    "auth_method": "Auth Hash" if self.auth_credentials else ("JWT Token" if self.access_token else "No Auth")
                }
            }
            
            self.logger.info(f"ğŸ¯ === ZONEMINDER DISCOVERY SUCCESS ===")
            return success_result
            
        except requests.exceptions.RequestException as e:
            error_msg = f"ZoneMinder API connection failed: {str(e)}"
            self.logger.error(f"âŒ Request error: {error_msg}")
            return self._error_response(error_msg)
        except Exception as e:
            error_msg = f"ZoneMinder discovery failed: {str(e)}"
            self.logger.error(f"âŒ Unexpected error: {error_msg}")
            return self._error_response(error_msg)
    
    def _get_zoneminder_system_info(self) -> dict:
        """ğŸ”§ FIXED: Get ZoneMinder system information with auth"""
        system_info = {}
        
        try:
            # Get disk usage with authentication
            disk_response = self._make_authenticated_request(f"{self.base_url}/host/getDiskPercent.json", timeout=5)
            if disk_response.status_code == 200:
                disk_data = disk_response.json()
                usage = disk_data.get('usage', {})
                total_usage = usage.get('Total', {}).get('space', 'Unknown')
                system_info['disk_usage'] = f"{total_usage}% used" if total_usage != 'Unknown' else 'Unknown'
                
                self.logger.info(f"ZoneMinder disk usage: {system_info['disk_usage']}")
            
        except Exception as e:
            self.logger.warning(f"Failed to get ZoneMinder system info: {e}")
            system_info['disk_usage'] = 'Unknown'
        
        return system_info
    
    def test_connection_and_discover_cameras(self, source_data: dict) -> dict:
        """
        Universal NVR connection test and camera discovery
        Supports: ZoneMinder (Real), ONVIF (Mock), RTSP, etc.
        """
        url = source_data.get('path', '')
        config = source_data.get('config', {})
        protocol = config.get('protocol', 'onvif').lower()
        username = config.get('username', '')
        password = config.get('password', '')
        
        self.logger.info(f"Testing NVR connection to {url} using {protocol}")
        
        try:
            # Basic validation
            if not url:
                return self._error_response("NVR URL is required")
            
            # Extract host for network check
            host = self._extract_host(url)
            
            # Network connectivity check
            if not self._check_network_connectivity(host):
                return self._error_response(f"Cannot reach NVR at {host}. Check network connectivity.")
            
            # Protocol-specific discovery
            if protocol == 'zoneminder':
                return self._discover_zoneminder_real(url, config)
            elif protocol == 'onvif':
                return self._discover_onvif_real(url, config)
            elif protocol == 'rtsp':
                return self._discover_rtsp_mock(url, config)
            elif protocol == 'hikvision':
                return self._discover_vendor_mock(url, config, 'hikvision')
            elif protocol == 'dahua':
                return self._discover_vendor_mock(url, config, 'dahua')
            else:
                return self._discover_generic_mock(url, config)
                
        except Exception as e:
            self.logger.error(f"NVR connection test failed: {e}")
            return self._error_response(f"Connection test failed: {str(e)}")
    
    # ... (rest of the methods remain the same)
    
    def _get_zm_capabilities(self, monitor: dict) -> List[str]:
        """Get ZoneMinder monitor capabilities"""
        capabilities = []
        
        function = monitor.get('Function', '')
        if function in ['Record', 'Mocord']:
            capabilities.append('recording')
        if function in ['Monitor', 'Modect', 'Mocord']:
            capabilities.append('monitoring')
        if function in ['Modect', 'Mocord']:
            capabilities.append('motion_detection')
        
        if monitor.get('Controllable') == '1':
            capabilities.append('ptz')
        
        if monitor.get('RecordAudio') == '1':
            capabilities.append('audio')
            
        return capabilities
    
    def _extract_host(self, url: str) -> str:
        """Extract hostname/IP from URL"""
        url = url.replace('http://', '').replace('https://', '').replace('rtsp://', '')
        host = url.split(':')[0].split('/')[0]
        return host
    
    def _check_network_connectivity(self, host: str) -> bool:
        """Check network connectivity"""
        try:
            socket.gethostbyname(host)
            return True
        except socket.gaierror:
            # For local IPs and localhost, assume reachable
            if host.startswith('192.168.') or host.startswith('10.') or host.startswith('172.') or host in ['localhost', '127.0.0.1']:
                return True
            return False
    
    def _error_response(self, message: str) -> dict:
        """Standard error response"""
        return {
            "accessible": False,
            "message": message,
            "source_type": "nvr",
            "cameras": [],
            "device_info": {}
        }
    
    # Mock implementations for other protocols
    def _discover_onvif_mock(self, url: str, config: dict) -> dict:
        """Mock ONVIF discovery (for other NVR types)"""
        if not config.get('username') or not config.get('password'):
            return self._error_response("Username and password are required for ONVIF")
        
        host = self._extract_host(url)
        num_cameras = random.randint(2, 6)
        cameras = []
        
        camera_names = ["Front Door", "Parking", "Warehouse", "Office", "Storage", "Loading"]
        
        for i in range(num_cameras):
            cameras.append({
                "id": f"onvif_profile_{i+1}",
                "name": camera_names[i] if i < len(camera_names) else f"Camera {i+1}",
                "description": f"ONVIF Camera {i+1}",
                "stream_url": f"rtsp://{host}:554/stream{i+1}",
                "resolution": random.choice(["1920x1080", "1280x720", "2560x1440"]),
                "codec": random.choice(["H264", "H265"]),
                "capabilities": ["recording"] + (["ptz"] if i < 2 else [])
            })
        
        return {
            "accessible": True,
            "message": f"ONVIF connection successful - Discovered {num_cameras} cameras",
            "source_type": "nvr",
            "protocol": "onvif",
            "cameras": cameras,
            "device_info": {
                "manufacturer": "Generic ONVIF",
                "model": f"NVR-{num_cameras}CH",
                "firmware": f"V{random.randint(2,5)}.{random.randint(0,9)}.0"
            }
        }
    
    def _discover_rtsp_mock(self, url: str, config: dict) -> dict:
        """Mock RTSP discovery"""
        if not config.get('username') or not config.get('password'):
            return self._error_response("Username and password are required for RTSP")
        
        return {
            "accessible": True,
            "message": "RTSP connection successful - Found 2 streams",
            "source_type": "nvr",
            "protocol": "rtsp",
            "cameras": [
                {"id": "rtsp_1", "name": "Main Stream", "stream_url": f"rtsp://{url}/stream1"},
                {"id": "rtsp_2", "name": "Sub Stream", "stream_url": f"rtsp://{url}/stream2"}
            ],
            "device_info": {"manufacturer": "Generic RTSP", "model": "RTSP Server"}
        }
    
    def _discover_vendor_mock(self, url: str, config: dict, vendor: str) -> dict:
        """Mock vendor-specific discovery"""
        return {
            "accessible": False,
            "message": f"{vendor.title()} API integration coming soon. Use ONVIF protocol for now.",
            "source_type": "nvr",
            "protocol": vendor,
            "cameras": [],
            "device_info": {}
        }
    
    def _discover_generic_mock(self, url: str, config: dict) -> dict:
        """Mock generic discovery"""
        return {
            "accessible": False,
            "message": "Generic HTTP API not implemented. Try ONVIF, RTSP, or ZoneMinder protocols.",
            "source_type": "nvr",
            "protocol": "generic",
            "cameras": [],
            "device_info": {}
        }
    def _discover_onvif_real(self, url: str, config: dict) -> dict:
        """Real ONVIF discovery"""
        try:
            host = self._extract_host(url)
            port = int(config.get('port', 80))
            username = config.get('username', '')
            password = config.get('password', '')
            
            response = onvif_client.test_device_connection(host, port, username, password)
            
            # Validate cameras is array
            if 'cameras' in response and not isinstance(response['cameras'], list):
                response['cameras'] = [response['cameras']]
            
            # Handle multiple cameras and generate IDs
            if 'cameras' in response:
                for i, camera in enumerate(response['cameras']):
                    camera['id'] = f"onvif_{host}_channel_{i+1}"
            
            return response
            
        except Exception as e:
            return self._error_response(f"ONVIF lá»—i: {str(e)}")
```
## ğŸ“„ File: `cloud_endpoints.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/cloud_endpoints.py`

```python
#!/usr/bin/env python3

from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import Flow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import hashlib
import json
import os
import logging
from datetime import datetime, timedelta
from flask import Blueprint, request, jsonify, session
from google.oauth2.credentials import Credentials
from flask_cors import CORS, cross_origin

from functools import wraps
from flask import g
import time
from collections import defaultdict

# ğŸ†• Import lazy folder routes
from modules.sources.cloud_lazy_folder_routes import lazy_folder_bp

# ğŸ†• Rate limiting storage (in-memory for simplicity)
rate_limit_storage = defaultdict(list)

# ğŸ†• Caching storage for performance optimization
cache_storage = {}
CACHE_DURATIONS = {
    'auth_status': 300,      # 5 minutes
    'subfolders': 180,       # 3 minutes
    'user_info': 600,        # 10 minutes
    'picker_token': 60       # 1 minute
}

def get_cache_key(endpoint, *args):
    """Generate cache key"""
    key_parts = [endpoint] + [str(arg) for arg in args]
    return hashlib.sha256(':'.join(key_parts).encode()).hexdigest()[:16]

def get_cached_data(cache_key):
    """Get cached data if valid"""
    if cache_key in cache_storage:
        cached_item = cache_storage[cache_key]
        if time.time() < cached_item['expires_at']:
            return cached_item['data']
        else:
            # Remove expired cache
            del cache_storage[cache_key]
    return None

def set_cached_data(cache_key, data, cache_type='default'):
    """Cache data with expiration"""
    duration = CACHE_DURATIONS.get(cache_type, 300)
    cache_storage[cache_key] = {
        'data': data,
        'expires_at': time.time() + duration,
        'created_at': time.time()
    }

RATE_LIMITS = {
    'picker_token': {'calls': 10, 'window': 60},  # 10 calls per minute
    'auth_status': {'calls': 30, 'window': 60},   # 30 calls per minute  
    'default': {'calls': 60, 'window': 60}        # 60 calls per minute default
}

def rate_limit(endpoint_type='default'):
    """Rate limiting decorator"""
    def decorator(f):
        @wraps(f)
        def decorated_function(*args, **kwargs):
            client_ip = request.remote_addr
            current_time = time.time()
            
            # Get rate limit config
            limit_config = RATE_LIMITS.get(endpoint_type, RATE_LIMITS['default'])
            max_calls = limit_config['calls']
            time_window = limit_config['window']
            
            # Clean old entries
            cutoff_time = current_time - time_window
            rate_limit_storage[client_ip] = [
                call_time for call_time in rate_limit_storage[client_ip] 
                if call_time > cutoff_time
            ]
            
            # Check rate limit
            if len(rate_limit_storage[client_ip]) >= max_calls:
                logger.warning(f"ğŸš« Rate limit exceeded for {client_ip} on {endpoint_type}")
                return jsonify({
                    'success': False,
                    'message': f'Rate limit exceeded. Max {max_calls} calls per {time_window} seconds.',
                    'retry_after': int(time_window - (current_time - rate_limit_storage[client_ip][0]))
                }), 429
            
            # Record this call
            rate_limit_storage[client_ip].append(current_time)
            
            return f(*args, **kwargs)
        return decorated_function
    return decorator

# Configure logging
logger = logging.getLogger(__name__)

# Create Blueprint for cloud endpoints
cloud_bp = Blueprint('cloud', __name__, url_prefix='/api/cloud')
# ğŸ”§ FIX: Add CORS to cloud blueprint
CORS(cloud_bp, 
     origins=['http://localhost:3000', 'http://127.0.0.1:3000'],
     supports_credentials=True,
     allow_headers=['Content-Type', 'Authorization', 'X-Requested-With'],
     methods=['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'])

# ğŸ†• Register lazy folder routes AFTER CORS setup
cloud_bp.register_blueprint(lazy_folder_bp)

@cloud_bp.route('/authenticate', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
def cloud_authenticate():
    """Google OAuth2 authentication - FIXED for multiple environments"""
    try:
        data = request.get_json()
        provider = data.get('provider', 'google_drive')
        action = data.get('action', 'initiate_auth')
        
        # ğŸ†• NEW: Get redirect URI from request or determine automatically
        custom_redirect = data.get('redirect_uri')
        
        logger.info(f"ğŸ” Cloud authentication request: {provider}, action: {action}")
        
        if action == 'initiate_auth':
            CLIENT_SECRETS_FILE = os.path.join(
                os.path.dirname(__file__), 
                'credentials/google_drive_credentials_web.json'
            )
            
            if not os.path.exists(CLIENT_SECRETS_FILE):
                return jsonify({
                    'success': False,
                    'message': f'Credentials file not found: {CLIENT_SECRETS_FILE}',
                    'setup_required': True
                }), 400
            
            SCOPES = [
                'https://www.googleapis.com/auth/drive.file',
                'https://www.googleapis.com/auth/drive.readonly',
                'https://www.googleapis.com/auth/drive.metadata.readonly'
            ]
            
            # Create flow
            flow = Flow.from_client_secrets_file(
                CLIENT_SECRETS_FILE, 
                scopes=SCOPES
            )
            
            # ğŸ”§ FIX: Determine redirect URI dynamically
            if custom_redirect:
                redirect_uri = custom_redirect
            else:
                # Auto-detect based on request headers
                host = request.headers.get('Host', 'localhost:8080')
                if '127.0.0.1' in host:
                    redirect_uri = 'http://127.0.0.1:8080/api/cloud/oauth/callback'
                else:
                    redirect_uri = 'http://localhost:8080/api/cloud/oauth/callback'
            
            flow.redirect_uri = redirect_uri
            
            # Generate authorization URL
            authorization_url, state = flow.authorization_url(
                access_type='offline',
                include_granted_scopes='true',
                prompt='consent'
            )
            
            # Store in session
            session['oauth2_state'] = state
            session['oauth2_flow_data'] = {
                'scopes': SCOPES,
                'redirect_uri': flow.redirect_uri,  # âœ… Store used URI
                'client_secrets_file': CLIENT_SECRETS_FILE
            }
            session.permanent = True
            
            logger.info(f"âœ… OAuth flow initiated")
            logger.info(f"   Auth URL: {authorization_url}")
            logger.info(f"   Redirect URI: {flow.redirect_uri}")
            
            return jsonify({
                'success': True,
                'auth_url': authorization_url,
                'state': state,
                'redirect_uri': flow.redirect_uri,
                'message': 'OAuth flow initiated - open popup window'
            }), 200
            
        else:
            return jsonify({
                'success': False,
                'message': f'Unknown action: {action}'
            }), 400
            
    except Exception as e:
        logger.error(f"âŒ Cloud authentication error: {e}")
        return jsonify({
            'success': False,
            'message': f'Cloud authentication failed: {str(e)}'
        }), 500

@cloud_bp.route('/oauth/callback', methods=['GET', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
def cloud_oauth_callback():
    """OAuth2 callback handler - FIXED for session management and CORS"""
    try:
        logger.info("ğŸ”„ Processing OAuth callback...")
        logger.info(f"   Request URL: {request.url}")
        logger.info(f"   Session ID: {session.get('_id', 'no-session')}")
        
        # Get OAuth parameters
        code = request.args.get('code')
        state = request.args.get('state')
        error = request.args.get('error')
        error_description = request.args.get('error_description')
        
        # Handle errors
        if error:
            logger.error(f"âŒ OAuth error: {error}")
            return _create_error_page(f"OAuth error: {error}", error_description)
        
        if not code or not state:
            logger.error("âŒ Missing required OAuth parameters")
            return _create_error_page("Missing OAuth parameters", "Authorization code or state missing")
        
        # ğŸ”§ FIX: More flexible state verification
        stored_state = session.get('oauth2_state')
        if not stored_state:
            logger.warning("âš ï¸ No stored state found, but proceeding (session might have expired)")
            # Don't fail immediately - try to continue with OAuth
        elif state != stored_state:
            logger.error(f"âŒ State mismatch: got {state}, expected {stored_state}")
            return _create_error_page("Security verification failed", "Please try authenticating again")
        
        # Get flow data (with fallback)
        flow_data = session.get('oauth2_flow_data')
        if not flow_data:
            logger.warning("âš ï¸ No flow data, using defaults")
            flow_data = {
                'scopes': [
                    'https://www.googleapis.com/auth/drive.file',
                    'https://www.googleapis.com/auth/drive.readonly',
                    'https://www.googleapis.com/auth/drive.metadata.readonly'
                ],
                'redirect_uri': 'http://localhost:8080/api/cloud/oauth/callback',
                'client_secrets_file': os.path.join(
                    os.path.dirname(__file__), 
                    'credentials/google_drive_credentials_web.json'
                )
            }
        
        # Recreate flow
        flow = Flow.from_client_secrets_file(
            flow_data['client_secrets_file'], 
            scopes=flow_data['scopes']
        )
        flow.redirect_uri = flow_data['redirect_uri']
        
        logger.info(f"   Using redirect URI: {flow.redirect_uri}")
        
        # Exchange code for tokens
        try:
            logger.info(f"ğŸ”„ Attempting token exchange...")
            flow.fetch_token(code=code)
            credentials = flow.credentials
            logger.info("âœ… Token exchange successful")
        except Exception as token_error:
            logger.error(f"âŒ Token exchange failed: {token_error}")
            return _create_error_page("Token exchange failed", str(token_error))
        
        # Get user info
        try:
            service = build('drive', 'v3', credentials=credentials)
            about = service.about().get(fields='user,storageQuota').execute()
            
            user_info = {
                'email': about.get('user', {}).get('emailAddress', 'unknown'),
                'name': about.get('user', {}).get('displayName', 'Unknown User'),
                'photo_url': about.get('user', {}).get('photoLink'),
                'storage_used_gb': int(about.get('storageQuota', {}).get('usage', 0)) / (1024**3),
                'storage_total_gb': int(about.get('storageQuota', {}).get('limit', 0)) / (1024**3) if about.get('storageQuota', {}).get('limit') else 'Unlimited'
            }
            logger.info(f"âœ… User info retrieved: {user_info['email']}")
            
        except Exception as user_error:
            logger.error(f"âŒ Failed to get user info: {user_error}")
            user_info = {'email': 'unknown', 'name': 'Unknown User'}
        
        # ğŸ”§ FIX: Store credentials safely first
        credentials_dict = None
        try:
            credentials_dict = _store_credentials_safely(credentials, user_info)
            logger.info("âœ… Credentials stored successfully")
        except Exception as storage_error:
            logger.warning(f"âš ï¸ Failed to store credentials: {storage_error}")
            # Continue anyway
        
        # ğŸ†• UPDATED: Get only root folders (not all folders) for lazy loading
        folders = []
        try:
            # Import lazy loading service
            from modules.sources.google_drive_service import GoogleDriveFolderService
            
            # Initialize folder service for lazy loading
            folder_service = GoogleDriveFolderService(credentials)
            
            # Get only root level folders
            root_folders = folder_service.get_subfolders('root', 50)
            
            for folder in root_folders:
                folders.append({
                    'id': folder['id'],
                    'name': folder['name'],
                    'type': 'folder',
                    'depth': 1,  # Root level folders are depth 1
                    'selectable': False,  # Only depth 4 is selectable
                    'has_subfolders': folder_service.has_subfolders(folder['id']),
                    'created': folder.get('created'),
                    'path': f"/My Drive/{folder['name']}"
                })
            
            logger.info(f"ğŸ“ Loaded {len(folders)} root folders for lazy loading")
            
        except Exception as folder_error:
            logger.warning(f"âš ï¸ Failed to load root folders: {folder_error}")
            # Continue without folders
        
        # ğŸ”§ FIX: Create comprehensive session result
        session_result = {
            'success': True,
            'authenticated': True,
            'user_info': user_info,
            'user_email': user_info['email'],
            'folders': folders,  # Only root folders
            'lazy_loading_enabled': True,  # ğŸ†• NEW: Indicate lazy loading support
            'credentials': credentials_dict or {
                'token': credentials.token,
                'refresh_token': credentials.refresh_token,
                'token_uri': credentials.token_uri,
                'client_id': credentials.client_id,
                'client_secret': credentials.client_secret,
                'scopes': list(credentials.scopes) if credentials.scopes else []
            },
            'existing_auth': False,
            'message': f'Successfully authenticated as {user_info["email"]}',
            'backend_port': 8080,
            'timestamp': datetime.now().isoformat()
        }
        
        # Store in session with longer lifetime
        session['auth_result'] = session_result
        session.permanent = True
        
        # ğŸ”§ FIX: Clear OAuth session data
        session.pop('oauth2_state', None)
        session.pop('oauth2_flow_data', None)
        
        logger.info(f"âœ… OAuth completed successfully for: {user_info['email']}")
        
        # Return success page with postMessage
        return _create_success_page_with_postmessage(session_result)
        
    except Exception as e:
        logger.error(f"âŒ OAuth callback error: {e}")
        import traceback
        traceback.print_exc()
        return _create_error_page("Authentication error", str(e))

def _store_credentials_safely(credentials, user_info):
    """Store credentials safely and return credentials dict"""
    try:
        tokens_dir = os.path.join(os.path.dirname(__file__), 'tokens')
        os.makedirs(tokens_dir, exist_ok=True)
        
        email_hash = hashlib.sha256(user_info['email'].encode()).hexdigest()[:16]
        token_filename = f"google_drive_{email_hash}.json"
        token_filepath = os.path.join(tokens_dir, token_filename)
        
        credential_data = {
            'token': credentials.token,
            'refresh_token': credentials.refresh_token,
            'token_uri': credentials.token_uri,
            'client_id': credentials.client_id,
            'client_secret': credentials.client_secret,
            'scopes': list(credentials.scopes) if credentials.scopes else [],
            'user_info': user_info,
            'created_at': datetime.now().isoformat(),
            'expires_at': credentials.expiry.isoformat() if credentials.expiry else None
        }
        
        with open(token_filepath, 'w') as f:
            json.dump(credential_data, f, indent=2)
        
        os.chmod(token_filepath, 0o600)
        logger.info(f"âœ… Credentials stored to: {token_filepath}")
        
        # Return credentials for session
        return {
            'token': credentials.token,
            'refresh_token': credentials.refresh_token,
            'token_uri': credentials.token_uri,
            'client_id': credentials.client_id,
            'client_secret': credentials.client_secret,
            'scopes': list(credentials.scopes) if credentials.scopes else []
        }
        
    except Exception as e:
        logger.error(f"âŒ Credential storage error: {e}")
        raise

def _create_success_page_with_postmessage(session_result):
    """Success page with postMessage for COOP compatibility"""
    return f"""
    <!DOCTYPE html>
    <html>
        <head>
            <title>VTrack - Authentication Successful</title>
            <meta charset="utf-8">
            <style>
                body {{
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                    text-align: center;
                    padding: 50px;
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    margin: 0;
                    min-height: 100vh;
                }}
                .container {{
                    background: white;
                    color: #333;
                    padding: 40px;
                    border-radius: 15px;
                    display: inline-block;
                    box-shadow: 0 20px 40px rgba(0,0,0,0.1);
                    max-width: 500px;
                }}
                .success-icon {{ font-size: 4em; margin-bottom: 20px; }}
                .user-info {{
                    background: #d4edda;
                    border: 1px solid #c3e6cb;
                    padding: 20px;
                    margin: 20px 0;
                    border-radius: 8px;
                    text-align: left;
                }}
                .status {{
                    background: #cce5ff;
                    border: 1px solid #99ccff;
                    padding: 15px;
                    margin: 15px 0;
                    border-radius: 8px;
                    text-align: left;
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="success-icon">âœ…</div>
                <h1 style="color: #28a745;">Authentication Successful!</h1>
                
                <div class="user-info">
                    <h3>Google Drive Connected (Lazy Loading)</h3>
                    <p><strong>Account:</strong> {session_result['user_email']}</p>
                    <p><strong>Name:</strong> {session_result['user_info']['name']}</p>
                    <p><strong>Root Folders:</strong> {len(session_result['folders'])}</p>
                    <p><strong>Mode:</strong> Lazy Loading Tree Navigation</p>
                    <p><strong>Backend:</strong> localhost:8080</p>
                </div>
                
                <div class="status" id="status">
                    <strong>Status:</strong> <span id="statusText">Notifying VTrack...</span>
                </div>
                
                <p style="color: #28a745; font-weight: bold;">
                    ğŸ‰ You can now close this window and return to VTrack!
                </p>
                
                <p style="font-size: 0.9em; color: #666;">
                    This window will close automatically in <span id="countdown">10</span> seconds.
                </p>
                
                <button onclick="window.close()" style="padding: 10px 20px; background: #007bff; color: white; border: none; border-radius: 5px; cursor: pointer; margin: 10px;">
                    Close Window
                </button>
            </div>
            
            <script>
                console.log('âœ… OAuth success page loaded');
                
                const statusEl = document.getElementById('statusText');
                const countdownEl = document.getElementById('countdown');
                
                // Prepare data for postMessage
                const authData = {json.dumps(session_result)};
                
                // Function to notify parent window
                function notifyParent() {{
                    try {{
                        if (window.opener && !window.opener.closed) {{
                            console.log('ğŸ“¬ Sending success message to parent window');
                            window.opener.postMessage({{
                                type: 'OAUTH_SUCCESS',
                                ...authData
                            }}, window.location.origin.replace(':8080', ':3000')); // Send to frontend port
                            
                            statusEl.textContent = 'VTrack notified successfully!';
                            statusEl.style.color = '#28a745';
                        }} else {{
                            console.log('âš ï¸ Parent window not available');
                            statusEl.textContent = 'Parent window not found';
                            statusEl.style.color = '#856404';
                        }}
                    }} catch (error) {{
                        console.error('âŒ Error notifying parent:', error);
                        statusEl.textContent = 'Error notifying VTrack';
                        statusEl.style.color = '#dc3545';
                    }}
                }}
                
                // Try multiple notification methods
                function attemptNotification() {{
                    // Method 1: Direct postMessage to opener
                    notifyParent();
                    
                    // Method 2: Try broadcasting to all windows (fallback)
                    setTimeout(() => {{
                        try {{
                            window.postMessage({{
                                type: 'OAUTH_SUCCESS',
                                ...authData
                            }}, '*');
                            console.log('ğŸ“¡ Broadcasted success message');
                        }} catch (error) {{
                            console.error('âŒ Broadcast error:', error);
                        }}
                    }}, 500);
                    
                    // Method 3: Try localStorage as fallback (if available)
                    setTimeout(() => {{
                        try {{
                            localStorage.setItem('vtrack_oauth_result', JSON.stringify({{
                                type: 'OAUTH_SUCCESS',
                                timestamp: Date.now(),
                                ...authData
                            }}));
                            console.log('ğŸ’¾ Stored auth result in localStorage');
                        }} catch (error) {{
                            console.log('âš ï¸ localStorage not available:', error);
                        }}
                    }}, 1000);
                }}
                
                // Start notification attempts
                attemptNotification();
                
                // Countdown and auto-close
                let countdown = 10;
                
                function updateCountdown() {{
                    countdownEl.textContent = countdown;
                    if (countdown <= 0) {{
                        console.log('ğŸšª Auto-closing window');
                        window.close();
                    }} else {{
                        countdown--;
                        setTimeout(updateCountdown, 1000);
                    }}
                }}
                
                updateCountdown();
                
                // Also try to close when parent receives message
                window.addEventListener('beforeunload', () => {{
                    console.log('ğŸšª Window closing');
                }});
            </script>
        </body>
    </html>
    """

def _create_error_page(error_message, error_details=None):
    """Error page for port 8080"""
    return f"""
    <!DOCTYPE html>
    <html>
        <head>
            <title>VTrack - Authentication Failed</title>
            <meta charset="utf-8">
            <style>
                body {{
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                    text-align: center;
                    padding: 50px;
                    background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%);
                    color: white;
                    margin: 0;
                    min-height: 100vh;
                }}
                .container {{
                    background: white;
                    color: #333;
                    padding: 40px;
                    border-radius: 15px;
                    display: inline-block;
                    box-shadow: 0 20px 40px rgba(0,0,0,0.1);
                }}
                .error-icon {{ font-size: 4em; margin-bottom: 20px; }}
                .error-details {{
                    background: #f8d7da;
                    border: 1px solid #f5c6cb;
                    padding: 15px;
                    margin: 20px 0;
                    border-radius: 8px;
                    text-align: left;
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="error-icon">âŒ</div>
                <h1 style="color: #dc3545;">Authentication Failed</h1>
                
                <div class="error-details">
                    <h4>Error Details:</h4>
                    <p><strong>Message:</strong> {error_message}</p>
                    {f"<p><strong>Details:</strong> {error_details}</p>" if error_details else ""}
                    <p><strong>Backend:</strong> localhost:8080</p>
                </div>
                
                <p>Please try again or contact support.</p>
                <button onclick="window.close()" style="padding: 10px 20px; background: #007bff; color: white; border: none; border-radius: 5px; cursor: pointer;">
                    Close Window
                </button>
            </div>
            
            <script>
                // Notify parent window of error
                if (window.opener) {{
                    window.opener.postMessage({{
                        type: 'OAUTH_ERROR',
                        error: '{error_message}',
                        details: '{error_details or ""}',
                        backend_port: 8080
                    }}, '*');
                }}
                
                setTimeout(() => window.close(), 10000);
            </script>
        </body>
    </html>
    """, 400

# ğŸ†• NEW: Auth status check with lazy loading support
@cloud_bp.route('/auth-status', methods=['GET', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@rate_limit('auth_status')
def auth_status():
    """Check authentication status"""

    # Handle OPTIONS request for CORS preflight
    if request.method == 'OPTIONS':
        response = jsonify({'status': 'ok'})
        response.headers.add('Access-Control-Allow-Origin', 'http://localhost:3000')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')
        response.headers.add('Access-Control-Allow-Methods', 'GET,OPTIONS')
        response.headers.add('Access-Control-Allow-Credentials', 'true')
        return response
    
    try:
        cache_key = get_cache_key('auth_status', session.get('_id', 'anonymous'))
        cached_result = get_cached_data(cache_key)
        
        if cached_result:
            logger.debug("ğŸ“‹ Auth status cache hit")
            return jsonify(cached_result), 200
        
        auth_result = session.get('auth_result')
        
        if not auth_result or not auth_result.get('authenticated'):
            result = {
                'success': False,
                'authenticated': False,
                'message': 'No authentication found',
                'lazy_loading_enabled': False
            }
        else:
            result = {
                'success': True,
                'authenticated': True,
                'user_email': auth_result.get('user_email'),
                'user_info': auth_result.get('user_info', {}),
                'credentials': auth_result.get('credentials', {}),
                'folders': auth_result.get('folders', []),  # Root folders only
                'lazy_loading_enabled': auth_result.get('lazy_loading_enabled', True),
                'message': f"Authenticated as {auth_result.get('user_email')}",
                'existing_auth': True,
                'backend_port': 8080
            }
        
        set_cached_data(cache_key, result, 'auth_status')
        return jsonify(result), 200
        
    except Exception as e:
        logger.error(f"âŒ Auth status error: {e}")
        return jsonify({
            'success': False,
            'authenticated': False,
            'message': f'Auth status check failed: {str(e)}'
        }), 500

# ğŸ†• NEW: Disconnect endpoint
@cloud_bp.route('/disconnect', methods=['POST'])
def cloud_disconnect():
    """Disconnect from cloud provider"""
    try:
        data = request.get_json()
        provider = data.get('provider', 'google_drive')
        user_email = data.get('user_email')
        
        logger.info(f"ğŸ”Œ Disconnecting {provider} for {user_email}")
        
        # Clear session
        session.pop('auth_result', None)
        session.pop('oauth2_state', None)
        session.pop('oauth2_flow_data', None)
        
        # Clear cache
        cache_storage.clear()
        
        # Optionally remove stored token file
        if user_email:
            try:
                tokens_dir = os.path.join(os.path.dirname(__file__), 'tokens')
                email_hash = hashlib.sha256(user_email.encode()).hexdigest()[:16]
                token_filename = f"google_drive_{email_hash}.json"
                token_filepath = os.path.join(tokens_dir, token_filename)
                
                if os.path.exists(token_filepath):
                    os.remove(token_filepath)
                    logger.info(f"ğŸ—‘ï¸ Removed token file: {token_filename}")
            except Exception as e:
                logger.warning(f"âš ï¸ Could not remove token file: {e}")
        
        return jsonify({
            'success': True,
            'message': f'Successfully disconnected from {provider}',
            'provider': provider
        }), 200
        
    except Exception as e:
        logger.error(f"âŒ Disconnect error: {e}")
        return jsonify({
            'success': False,
            'message': f'Disconnect failed: {str(e)}'
        }), 500
```
## ğŸ“„ File: `auto_sync_service.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/auto_sync_service.py`

```python
import threading
import logging
import time
from datetime import datetime, timedelta
import json
from modules.db_utils import get_db_connection
from modules.sources.nvr_downloader import NVRDownloader  # Assuming this exists or will be implemented
from database import get_sync_status, initialize_sync_status

logger = logging.getLogger(__name__)

class AutoSyncService:
    def __init__(self):
        self.sync_timers = {}  # Store timers for each source
        self.sync_locks = {}   # Locks to prevent concurrent syncs
        self.downloader = NVRDownloader()
        
    def start_auto_sync(self, source_config: dict) -> bool:
        """Start auto-sync for a source"""
        source_id = source_config.get('id')
        if not source_id:
            logger.error("Source ID required to start sync")
            return False
            
        if source_id in self.sync_timers:
            logger.warning(f"Sync already running for source {source_id}")
            return True
            
        # Initialize status if not exists
        current_status = get_sync_status(source_id)
        if not current_status:
            initialize_sync_status(source_id, sync_enabled=True, interval_minutes=10)
            
        self.sync_locks[source_id] = threading.Lock()
        self._schedule_next_sync(source_id)
        
        logger.info(f"Auto-sync started for source {source_id}")
        return True
        
    def stop_auto_sync(self, source_id: int) -> bool:
        """Stop auto-sync for a source"""
        try:
            if source_id not in self.sync_timers:
                logger.warning(f"No active sync for source {source_id}")
                return True
                
            # Cancel timer
            self.sync_timers[source_id].cancel()
            del self.sync_timers[source_id]
            del self.sync_locks[source_id]
            
            # Update status
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE sync_status 
                SET sync_enabled = 0, 
                    last_sync_status = 'stopped',
                    last_sync_message = 'Auto-sync stopped by user'
                WHERE source_id = ?
            """, (source_id,))
            conn.commit()
            conn.close()
            
            logger.info(f"Auto-sync stopped for source {source_id}")
            return True
            
        except Exception as e:
            logger.error(f"Error stopping sync for {source_id}: {e}")
            return False
            
    def get_sync_status(self, source_id: int) -> dict:
        """Get current sync status"""
        return get_sync_status(source_id) or {}
        
    def _sync_latest_recordings(self, source_id: int) -> dict:
        """Perform sync of latest recordings"""
        with self.sync_locks.get(source_id, threading.Lock()):
            try:
                # Get source config
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT config FROM video_sources 
                    WHERE id = ?
                """, (source_id,))
                result = cursor.fetchone()
                conn.close()
                
                if not result:
                    return {'success': False, 'message': 'Source not found'}
                    
                config = json.loads(result[0])
                
                # Update status to in_progress
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE sync_status 
                    SET last_sync_status = 'in_progress',
                        last_sync_message = 'Sync started'
                    WHERE source_id = ?
                """, (source_id,))
                conn.commit()
                conn.close()
                
                # Download last 24 hours
                time_range = {
                    'from': datetime.now() - timedelta(hours=24),
                    'to': datetime.now()
                }
                
                download_result = self.downloader.download_latest_recordings(config, time_range)
                
                # Update status
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE sync_status 
                    SET last_sync_timestamp = ?,
                        last_sync_status = ?,
                        last_sync_message = ?,
                        files_downloaded_count = files_downloaded_count + ?,
                        total_download_size_mb = total_download_size_mb + ?
                    WHERE source_id = ?
                """, (
                    datetime.now().isoformat(),
                    'success' if download_result['success'] else 'failed',
                    download_result['message'],
                    download_result.get('files_downloaded', 0),
                    download_result.get('total_size_mb', 0.0),
                    source_id
                ))
                conn.commit()
                conn.close()
                
                return download_result
                
            except Exception as e:
                logger.error(f"Sync error for {source_id}: {e}")
                
                # Update error status
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE sync_status 
                    SET last_sync_status = 'failed',
                        last_sync_message = ?
                    WHERE source_id = ?
                """, (str(e), source_id))
                conn.commit()
                conn.close()
                
                return {'success': False, 'message': str(e)}
    
    def _schedule_next_sync(self, source_id: int):
        """Schedule next sync run"""
        status = self.get_sync_status(source_id)
        if not status.get('sync_enabled', True):
            return
            
        interval = status.get('sync_interval_minutes', 10)
        next_sync = datetime.now() + timedelta(minutes=interval)
        
        # Update next timestamp
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE sync_status 
            SET next_sync_timestamp = ?
            WHERE source_id = ?
        """, (next_sync.isoformat(), source_id))
        conn.commit()
        conn.close()
        
        # Schedule timer
        timer = threading.Timer(interval * 60, self._perform_sync, args=(source_id,))
        timer.daemon = True
        self.sync_timers[source_id] = timer
        timer.start()
        
        logger.info(f"Next sync scheduled for {source_id} at {next_sync}")
    
    def _perform_sync(self, source_id: int):
        """Perform sync and schedule next"""
        self._sync_latest_recordings(source_id)
        self._schedule_next_sync(source_id)
```
## ğŸ“„ File: `path_manager.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/path_manager.py`

```python
import sqlite3
import os
import json
import logging
import uuid
from datetime import datetime
import pytz
from modules.db_utils import get_db_connection
from modules.scheduler.db_sync import db_rwlock

# Cáº¥u hÃ¬nh mÃºi giá» Viá»‡t Nam - Äá»’NG NHáº¤T Vá»šI FILE_LISTER
VIETNAM_TZ = pytz.timezone('Asia/Ho_Chi_Minh')

class PathManager:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
    def get_all_active_sources(self):
        """Get all active video sources from database"""
        try:
            with db_rwlock.gen_rlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT id, source_type, name, path, config, active, created_at 
                    FROM video_sources 
                    WHERE active = 1 
                    ORDER BY source_type, name
                """)
                sources = []
                for row in cursor.fetchall():
                    source = {
                        'id': row[0],
                        'source_type': row[1],
                        'name': row[2],
                        'path': row[3],
                        'config': json.loads(row[4]) if row[4] else {},
                        'active': row[5],
                        'created_at': row[6]
                    }
                    sources.append(source)
                conn.close()
                return sources
        except Exception as e:
            self.logger.error(f"Error getting active sources: {e}")
            return []

    def get_current_active_source(self):
        """Get current active source (Single Active Source)"""
        sources = self.get_all_active_sources()
        return sources[0] if sources else None
    
    def get_source_by_id(self, source_id):
        """Get specific video source by ID"""
        try:
            with db_rwlock.gen_rlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT id, source_type, name, path, config, active, created_at 
                    FROM video_sources 
                    WHERE id = ?
                """, (source_id,))
                row = cursor.fetchone()
                conn.close()
                if row:
                    return {
                        'id': row[0],
                        'source_type': row[1],
                        'name': row[2],
                        'path': row[3],
                        'config': json.loads(row[4]) if row[4] else {},
                        'active': row[5],
                        'created_at': row[6]
                    }
                return None
        except Exception as e:
            self.logger.error(f"Error getting source by id {source_id}: {e}")
            return None
    
    def get_source_id_by_name(self, source_name):
        """Get source ID by name"""
        try:
            with db_rwlock.gen_rlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("SELECT id FROM video_sources WHERE name = ?", (source_name,))
                result = cursor.fetchone()
                conn.close()
                return result[0] if result else None
        except Exception as e:
            self.logger.error(f"Error getting source id by name {source_name}: {e}")
            return None
    
    def set_active_source(self, source_id):
        """Set single active source (disable all others)"""
        try:
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                
                # Disable all sources first
                cursor.execute("UPDATE video_sources SET active = 0")
                
                # Enable the specified source
                cursor.execute("UPDATE video_sources SET active = 1 WHERE id = ?", (source_id,))
                
                if cursor.rowcount == 0:
                    conn.rollback()
                    conn.close()
                    return False, f"No source found with id {source_id}"
                
                conn.commit()
                conn.close()
                
                self.logger.info(f"Set source id {source_id} as active")
                return True, "Active source updated successfully"
                
        except Exception as e:
            self.logger.error(f"Error setting active source: {e}")
            return False, str(e)
    
    def validate_source_accessibility(self, source_config):
        """Check if source is accessible based on type"""
        source_type = source_config.get('source_type')
        path = source_config.get('path')
        config = source_config.get('config', {})
        
        try:
            if source_type == 'local':
                return self._validate_local_path(path)
            elif source_type == 'camera':
                return self._validate_camera_source(path, config)
            elif source_type == 'cloud':
                return self._validate_cloud_source(path, config)
            else:
                return False, f"Unknown source type: {source_type}"
        except Exception as e:
            self.logger.error(f"Error validating source accessibility: {e}")
            return False, str(e)
    
    def _validate_local_path(self, path):
        """Validate local file system path"""
        if not path:
            return False, "Path is required"
        if not os.path.exists(path):
            return False, f"Path does not exist: {path}"
        if not os.access(path, os.R_OK):
            return False, f"No read permission for path: {path}"
        return True, "Local path is accessible"
    
    
    def _validate_camera_source(self, path, config):
        """Validate camera/NVR source"""
        if config.get('type') == 'directory':
            return self._validate_local_path(path)
        elif config.get('type') == 'api':
            api_url = config.get('api_url')
            if not api_url:
                return False, "API URL is required for camera source"
            
            try:
                import requests
                response = requests.get(api_url, timeout=10)
                if response.status_code == 200:
                    return True, "Camera API accessible"
                else:
                    return False, f"Camera API returned status {response.status_code}"
            except ImportError:
                return False, "requests library not installed"
            except Exception as e:
                return False, f"Camera API validation failed: {e}"
        else:
            return False, "Invalid camera source type"
    
    def _validate_cloud_source(self, path, config):
        """Validate cloud storage source"""
        provider = config.get('provider', '').lower()
        
        if provider == 'google_drive':
            return self._validate_google_drive(config)
        elif provider == 'dropbox':
            return self._validate_dropbox(config)
        elif provider == 'onedrive':
            return self._validate_onedrive(config)
        else:
            return False, f"Unsupported cloud provider: {provider}"
    
    def _validate_google_drive(self, config):
        """Validate Google Drive access"""
        try:
            from google.oauth2.credentials import Credentials
            from googleapiclient.discovery import build
            
            credentials_data = config.get('credentials')
            if not credentials_data:
                return False, "Google Drive credentials not found"
            
            credentials = Credentials.from_authorized_user_info(credentials_data)
            service = build('drive', 'v3', credentials=credentials)
            service.files().list(pageSize=1).execute()
            
            return True, "Google Drive connection successful"
            
        except ImportError:
            return False, "Google API library not installed"
        except Exception as e:
            return False, f"Google Drive validation failed: {e}"
    
    def _validate_dropbox(self, config):
        """Validate Dropbox access"""
        try:
            import dropbox
            
            access_token = config.get('access_token')
            if not access_token:
                return False, "Dropbox access token not found"
            
            dbx = dropbox.Dropbox(access_token)
            dbx.users_get_current_account()
            
            return True, "Dropbox connection successful"
            
        except ImportError:
            return False, "Dropbox library not installed"
        except Exception as e:
            return False, f"Dropbox validation failed: {e}"
    
    def _validate_onedrive(self, config):
        """Validate OneDrive access"""
        return False, "OneDrive validation not implemented yet"
    
    def add_source(self, source_type, name, path, config=None):
        """Add new video source"""
        try:
            if not all([source_type, name, path]):
                return False, "source_type, name, and path are required"
            
            config_json = json.dumps(config) if config else None
            
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                
                # Check if name already exists
                cursor.execute("SELECT COUNT(*) FROM video_sources WHERE name = ?", (name,))
                if cursor.fetchone()[0] > 0:
                    conn.close()
                    return False, f"Source name '{name}' already exists"
                
                # âœ… FIXED: Use VIETNAM_TZ for created_at - Äá»’NG NHáº¤T Vá»šI FILE_LISTER
                cursor.execute("""
                    INSERT INTO video_sources (source_type, name, path, config, active, created_at)
                    VALUES (?, ?, ?, ?, 1, ?)
                """, (source_type, name, path, config_json, datetime.now(VIETNAM_TZ)))
                
                source_id = cursor.lastrowid
                conn.commit()
                conn.close()
                
                self.logger.info(f"Added new video source: {name} (id: {source_id})")
                return True, f"Source '{name}' added successfully"
                
        except Exception as e:
            self.logger.error(f"Error adding source: {e}")
            return False, str(e)
    
    def update_source(self, source_id, **kwargs):
        """Update existing video source"""
        try:
            if not source_id:
                return False, "source_id is required"
            
            # Build update query
            update_fields = []
            update_values = []
            
            for field, value in kwargs.items():
                if field in ['source_type', 'name', 'path', 'active']:
                    update_fields.append(f"{field} = ?")
                    update_values.append(value)
                elif field == 'config':
                    update_fields.append("config = ?")
                    update_values.append(json.dumps(value) if value else None)
            
            if not update_fields:
                return False, "No valid fields to update"
            
            update_values.append(source_id)
            
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                
                query = f"UPDATE video_sources SET {', '.join(update_fields)} WHERE id = ?"
                cursor.execute(query, update_values)
                
                if cursor.rowcount == 0:
                    conn.close()
                    return False, f"No source found with id {source_id}"
                
                conn.commit()
                conn.close()
                
                self.logger.info(f"Updated video source id: {source_id}")
                return True, "Source updated successfully"
                
        except Exception as e:
            self.logger.error(f"Error updating source: {e}")
            return False, str(e)
    
    def delete_source(self, source_id):
        """Delete video source"""
        try:
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                
                cursor.execute("DELETE FROM video_sources WHERE id = ?", (source_id,))
                
                if cursor.rowcount == 0:
                    conn.close()
                    return False, f"No source found with id {source_id}"
                
                conn.commit()
                conn.close()
                
                self.logger.info(f"Deleted video source id: {source_id}")
                return True, "Source deleted successfully"
                
        except Exception as e:
            self.logger.error(f"Error deleting source: {e}")
            return False, str(e)
    
    def toggle_source_status(self, source_id, active):
        """Toggle source active status"""
        try:
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                
                cursor.execute("UPDATE video_sources SET active = ? WHERE id = ?", (active, source_id,))
                
                if cursor.rowcount == 0:
                    conn.close()
                    return False, f"No source found with id {source_id}"
                
                conn.commit()
                conn.close()
                
                status = "activated" if active else "deactivated"
                self.logger.info(f"Source id {source_id} {status}")
                return True, f"Source {status} successfully"
                
        except Exception as e:
            self.logger.error(f"Error toggling source status: {e}")
            return False, str(e)
```
## ğŸ“„ File: `cloud_lazy_folder_routes.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/cloud_lazy_folder_routes.py`

```python
#!/usr/bin/env python3

"""
Lazy Loading Folder Tree Routes for Google Drive Integration
Separated from main cloud_endpoints.py for better organization
"""

from flask import Blueprint, request, jsonify, session
from flask_cors import cross_origin
from google.oauth2.credentials import Credentials
from modules.sources.google_drive_service import GoogleDriveFolderService
from datetime import datetime
import logging
from functools import wraps
import time
from collections import defaultdict

logger = logging.getLogger(__name__)

# Rate limiting storage
lazy_folder_rate_limit_storage = defaultdict(list)

LAZY_FOLDER_RATE_LIMITS = {
    'folder_discovery': {'calls': 15, 'window': 60},
    'folder_search': {'calls': 10, 'window': 60},
    'folder_info': {'calls': 20, 'window': 60}
}

def lazy_folder_rate_limit(endpoint_type='folder_discovery'):
    """Rate limiting decorator for lazy folder operations"""
    def decorator(f):
        @wraps(f)
        def decorated_function(*args, **kwargs):
            client_ip = request.remote_addr
            current_time = time.time()
            
            limit_config = LAZY_FOLDER_RATE_LIMITS.get(endpoint_type, {'calls': 15, 'window': 60})
            max_calls = limit_config['calls']
            time_window = limit_config['window']
            
            # Clean old entries
            cutoff_time = current_time - time_window
            lazy_folder_rate_limit_storage[client_ip] = [
                call_time for call_time in lazy_folder_rate_limit_storage[client_ip] 
                if call_time > cutoff_time
            ]
            
            # Check rate limit
            if len(lazy_folder_rate_limit_storage[client_ip]) >= max_calls:
                logger.warning(f"ğŸš« Lazy folder rate limit exceeded for {client_ip} on {endpoint_type}")
                return jsonify({
                    'success': False,
                    'message': f'Rate limit exceeded. Max {max_calls} calls per {time_window} seconds.',
                    'retry_after': int(time_window - (current_time - lazy_folder_rate_limit_storage[client_ip][0]))
                }), 429
            
            # Record this call
            lazy_folder_rate_limit_storage[client_ip].append(current_time)
            
            return f(*args, **kwargs)
        return decorated_function
    return decorator

def get_credentials_from_session():
    """Get Google Drive credentials from session data"""
    try:
        auth_result = session.get('auth_result')
        if not auth_result or not auth_result.get('credentials'):
            return None
        
        cred_data = auth_result['credentials']
        credentials = Credentials(
            token=cred_data['token'],
            refresh_token=cred_data.get('refresh_token'),
            token_uri=cred_data.get('token_uri'),
            client_id=cred_data.get('client_id'),
            client_secret=cred_data.get('client_secret'),
            scopes=cred_data.get('scopes', [])
        )
        
        return credentials
    except Exception as e:
        logger.error(f"âŒ Error getting credentials from session: {e}")
        return None

# Create Blueprint for lazy folder routes
lazy_folder_bp = Blueprint('lazy_folders', __name__, url_prefix='/folders')

@lazy_folder_bp.route('/list_subfolders', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@lazy_folder_rate_limit('folder_discovery')
def list_subfolders():
    """
    Get subfolders of a specific parent folder with lazy loading
    
    Request JSON:
    {
        "parent_id": "folder_id_or_root",
        "max_results": 50,
        "include_stats": false
    }
    
    Response:
    {
        "success": true,
        "folders": [...],
        "parent_info": {...},
        "total_count": 25,
        "has_more": false
    }
    """
    try:
        data = request.get_json()
        parent_id = data.get('parent_id', 'root')
        max_results = min(data.get('max_results', 50), 100)  # Cap at 100
        include_stats = data.get('include_stats', False)
        
        logger.info(f"ğŸ“‚ Listing subfolders for parent: {parent_id}")
        
        # Get credentials
        credentials = get_credentials_from_session()
        if not credentials:
            return jsonify({
                'success': False,
                'message': 'No valid Google Drive credentials found. Please authenticate first.',
                'requires_auth': True
            }), 401
        
        # Initialize folder service
        folder_service = GoogleDriveFolderService(credentials)
        
        # Get subfolders
        subfolders = folder_service.get_subfolders(parent_id, max_results)
        
        # Enrich folder data with depth and selection info
        enriched_folders = []
        for folder in subfolders:
            # Calculate depth for this folder
            depth = folder_service.calculate_folder_depth(folder['id'])
            
            # Check if folder has subfolders (for expand indicator)
            has_subfolders = folder_service.has_subfolders(folder['id'])
            
            enriched_folder = {
                'id': folder['id'],
                'name': folder['name'],
                'type': 'folder',
                'parent_id': parent_id,
                'depth': depth,
                'selectable': folder_service.is_selectable_folder(depth),
                'has_subfolders': has_subfolders,
                'created': folder.get('created'),
                'modified': folder.get('modified'),
                'path': folder_service.build_folder_path(folder['id'])
            }
            
            # Add statistics if requested
            if include_stats:
                stats = folder_service.get_folder_statistics(folder['id'])
                enriched_folder['stats'] = stats
            
            enriched_folders.append(enriched_folder)
        
        # Get parent folder info
        parent_info = {}
        if parent_id != 'root':
            parent_info = folder_service.get_folder_info(parent_id)
        else:
            parent_info = {
                'id': 'root',
                'name': 'My Drive',
                'depth': 0,
                'path': '/My Drive',
                'selectable': False
            }
        
        response_data = {
            'success': True,
            'folders': enriched_folders,
            'parent_info': parent_info,
            'total_count': len(enriched_folders),
            'has_more': len(enriched_folders) == max_results,  # Might have more if we hit the limit
            'cache_info': folder_service.get_cache_info(),
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"âœ… Found {len(enriched_folders)} subfolders in {parent_id}")
        return jsonify(response_data), 200
        
    except Exception as e:
        logger.error(f"âŒ Error listing subfolders: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to list subfolders: {str(e)}',
            'error_type': type(e).__name__
        }), 500

@lazy_folder_bp.route('/get_depth', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@lazy_folder_rate_limit('folder_info')
def get_folder_depth():
    """
    Get the depth level of a specific folder
    
    Request JSON:
    {
        "folder_id": "folder_id"
    }
    
    Response:
    {
        "success": true,
        "folder_id": "folder_id",
        "depth": 2,
        "selectable": false,
        "path": "/My Drive/Project/Area"
    }
    """
    try:
        data = request.get_json()
        folder_id = data.get('folder_id')
        
        if not folder_id:
            return jsonify({
                'success': False,
                'message': 'folder_id is required'
            }), 400
        
        # Get credentials
        credentials = get_credentials_from_session()
        if not credentials:
            return jsonify({
                'success': False,
                'message': 'No valid Google Drive credentials found',
                'requires_auth': True
            }), 401
        
        # Initialize folder service
        folder_service = GoogleDriveFolderService(credentials)
        
        # Calculate depth
        depth = folder_service.calculate_folder_depth(folder_id)
        path = folder_service.build_folder_path(folder_id)
        selectable = folder_service.is_selectable_folder(depth)
        
        return jsonify({
            'success': True,
            'folder_id': folder_id,
            'depth': depth,
            'selectable': selectable,
            'path': path,
            'timestamp': datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"âŒ Error getting folder depth: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to get folder depth: {str(e)}'
        }), 500

@lazy_folder_bp.route('/search', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@lazy_folder_rate_limit('folder_search')
def search_folders():
    """
    Search for folders by name
    
    Request JSON:
    {
        "query": "camera",
        "max_results": 20
    }
    
    Response:
    {
        "success": true,
        "folders": [...],
        "query": "camera",
        "total_found": 15
    }
    """
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        max_results = min(data.get('max_results', 20), 50)
        
        if not query:
            return jsonify({
                'success': False,
                'message': 'Search query is required'
            }), 400
        
        # Get credentials
        credentials = get_credentials_from_session()
        if not credentials:
            return jsonify({
                'success': False,
                'message': 'No valid Google Drive credentials found',
                'requires_auth': True
            }), 401
        
        # Initialize folder service
        folder_service = GoogleDriveFolderService(credentials)
        
        # Search folders
        search_results = folder_service.search_folders(query, max_results)
        
        return jsonify({
            'success': True,
            'folders': search_results,
            'query': query,
            'total_found': len(search_results),
            'timestamp': datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"âŒ Error searching folders: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to search folders: {str(e)}'
        }), 500

@lazy_folder_bp.route('/get_info', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@lazy_folder_rate_limit('folder_info')
def get_folder_info():
    """
    Get comprehensive information about a folder
    
    Request JSON:
    {
        "folder_id": "folder_id",
        "include_stats": true
    }
    """
    try:
        data = request.get_json()
        folder_id = data.get('folder_id')
        include_stats = data.get('include_stats', False)
        
        if not folder_id:
            return jsonify({
                'success': False,
                'message': 'folder_id is required'
            }), 400
        
        # Get credentials
        credentials = get_credentials_from_session()
        if not credentials:
            return jsonify({
                'success': False,
                'message': 'No valid Google Drive credentials found',
                'requires_auth': True
            }), 401
        
        # Initialize folder service
        folder_service = GoogleDriveFolderService(credentials)
        
        # Get folder info
        folder_info = folder_service.get_folder_info(folder_id)
        
        if include_stats:
            stats = folder_service.get_folder_statistics(folder_id)
            folder_info['stats'] = stats
        
        return jsonify({
            'success': True,
            'folder_info': folder_info,
            'timestamp': datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"âŒ Error getting folder info: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to get folder info: {str(e)}'
        }), 500

@lazy_folder_bp.route('/clear_cache', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@lazy_folder_rate_limit('folder_info')
def clear_folder_cache():
    """Clear Google Drive folder service cache"""
    try:
        # If we have active credentials, clear service cache
        credentials = get_credentials_from_session()
        if credentials:
            folder_service = GoogleDriveFolderService(credentials)
            folder_service.clear_cache()
        
        return jsonify({
            'success': True,
            'message': 'Folder service cache cleared successfully',
            'timestamp': datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"âŒ Error clearing folder cache: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to clear folder cache: {str(e)}'
        }), 500

@lazy_folder_bp.route('/breadcrumb', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@lazy_folder_rate_limit('folder_info')
def get_folder_breadcrumb():
    """
    Get breadcrumb navigation for a folder
    
    Request JSON:
    {
        "folder_id": "folder_id"
    }
    
    Response:
    {
        "success": true,
        "breadcrumb": [
            {"id": "root", "name": "My Drive", "depth": 0},
            {"id": "123", "name": "Project", "depth": 1},
            {"id": "456", "name": "Area", "depth": 2}
        ]
    }
    """
    try:
        data = request.get_json()
        folder_id = data.get('folder_id')
        
        if not folder_id:
            return jsonify({
                'success': False,
                'message': 'folder_id is required'
            }), 400
        
        # Get credentials
        credentials = get_credentials_from_session()
        if not credentials:
            return jsonify({
                'success': False,
                'message': 'No valid Google Drive credentials found',
                'requires_auth': True
            }), 401
        
        # Initialize folder service
        folder_service = GoogleDriveFolderService(credentials)
        
        # Build breadcrumb
        breadcrumb = []
        current_id = folder_id
        
        # Traverse up the hierarchy
        while current_id and current_id != 'root' and len(breadcrumb) < 10:
            try:
                folder_info = folder_service.get_folder_info(current_id)
                breadcrumb.insert(0, {
                    'id': current_id,
                    'name': folder_info.get('name', 'Unknown'),
                    'depth': folder_info.get('depth', 0)
                })
                
                # Get parent
                parents = folder_info.get('parents', [])
                current_id = parents[0] if parents else 'root'
                
            except Exception as e:
                logger.warning(f"âš ï¸ Error getting folder info for breadcrumb {current_id}: {e}")
                break
        
        # Add root if not already there
        if not breadcrumb or breadcrumb[0]['id'] != 'root':
            breadcrumb.insert(0, {
                'id': 'root',
                'name': 'My Drive',
                'depth': 0
            })
        
        return jsonify({
            'success': True,
            'breadcrumb': breadcrumb,
            'total_levels': len(breadcrumb),
            'timestamp': datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"âŒ Error getting breadcrumb: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to get breadcrumb: {str(e)}'
        }), 500

@lazy_folder_bp.route('/validate_selection', methods=['POST', 'OPTIONS'])
@cross_origin(origins=['http://localhost:3000'], supports_credentials=True)
@lazy_folder_rate_limit('folder_info')
def validate_folder_selection():
    """
    Validate if selected folders meet the depth requirements
    
    Request JSON:
    {
        "folder_ids": ["id1", "id2", "id3"]
    }
    
    Response:
    {
        "success": true,
        "valid_selections": [...],
        "invalid_selections": [...],
        "total_valid": 2
    }
    """
    try:
        data = request.get_json()
        folder_ids = data.get('folder_ids', [])
        
        if not folder_ids:
            return jsonify({
                'success': False,
                'message': 'folder_ids array is required'
            }), 400
        
        # Get credentials
        credentials = get_credentials_from_session()
        if not credentials:
            return jsonify({
                'success': False,
                'message': 'No valid Google Drive credentials found',
                'requires_auth': True
            }), 401
        
        # Initialize folder service
        folder_service = GoogleDriveFolderService(credentials)
        
        valid_selections = []
        invalid_selections = []
        
        for folder_id in folder_ids:
            try:
                depth = folder_service.calculate_folder_depth(folder_id)
                selectable = folder_service.is_selectable_folder(depth)
                folder_info = folder_service.get_folder_info(folder_id)
                
                selection_info = {
                    'id': folder_id,
                    'name': folder_info.get('name', 'Unknown'),
                    'depth': depth,
                    'selectable': selectable,
                    'path': folder_info.get('path', ''),
                    'reason': 'Valid camera folder' if selectable else f'Wrong depth (level {depth}, need level 4)'
                }
                
                if selectable:
                    valid_selections.append(selection_info)
                else:
                    invalid_selections.append(selection_info)
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Error validating folder {folder_id}: {e}")
                invalid_selections.append({
                    'id': folder_id,
                    'name': 'Unknown',
                    'depth': -1,
                    'selectable': False,
                    'path': '',
                    'reason': f'Validation error: {str(e)}'
                })
        
        return jsonify({
            'success': True,
            'valid_selections': valid_selections,
            'invalid_selections': invalid_selections,
            'total_valid': len(valid_selections),
            'total_invalid': len(invalid_selections),
            'timestamp': datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"âŒ Error validating folder selection: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to validate selection: {str(e)}'
        }), 500
```
## ğŸ“„ File: `onvif_client.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/onvif_client.py`

```python
# onvif_client.py - ONVIF cho VTrack
import logging
import socket
import requests
from typing import Dict

logger = logging.getLogger(__name__)

class VTrackOnvifClient:
    def __init__(self):
        self.connected_cameras = {}
        
    def test_device_connection(self, ip: str, port: int, username: str = '', password: str = '') -> Dict:
        """Test ONVIF connection - discover multiple cameras from multiple ports"""
        logger.info(f"ğŸ¯ Testing ONVIF multiple camera discovery on {ip}")
        
        try:
            # Multiple ports for docker-compose setup
            ports_to_test = [1000, 1001, 1002] if port in [80, 1000] else [port]
            
            discovered_cameras = []
            accessible_ports = []
            
            for test_port in ports_to_test:
                try:
                    # Test socket connection
                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                    sock.settimeout(3)
                    result = sock.connect_ex((ip, test_port))
                    sock.close()
                    
                    if result == 0:
                        logger.info(f"âœ… Port {test_port} accessible")
                        
                        # Test ONVIF service
                        camera = self._test_single_port(ip, test_port, username, password)
                        if camera:
                            discovered_cameras.append(camera)
                            accessible_ports.append(test_port)
                            logger.info(f"âœ… Camera discovered on port {test_port}: {camera['name']}")
                        else:
                            logger.warning(f"âš ï¸ Port {test_port} accessible but no ONVIF camera")
                    else:
                        logger.info(f"âŒ Port {test_port} not accessible")
                        
                except Exception as port_error:
                    logger.warning(f"âŒ Error testing port {test_port}: {port_error}")
                    continue
            
            if discovered_cameras:
                return {
                    'accessible': True,
                    'message': f'ONVIF Multi-Camera Discovery - Found {len(discovered_cameras)} camera(s) on ports: {accessible_ports}',
                    'source_type': 'nvr',
                    'protocol': 'onvif',
                    'cameras': discovered_cameras,
                    'device_info': {
                        'manufacturer': 'Multiple ONVIF Devices',
                        'model': 'Multi-Camera System',
                        'firmware': 'Various',
                        'total_cameras': len(discovered_cameras),
                        'discovered_ports': accessible_ports
                    }
                }
            else:
                return {
                    'accessible': False,
                    'message': f'No ONVIF cameras found on {ip} ports: {ports_to_test}',
                    'source_type': 'nvr',
                    'protocol': 'onvif',
                    'cameras': []
                }
                
        except Exception as e:
            logger.error(f"âŒ Multiple camera discovery failed: {e}")
            return {
                'accessible': False,
                'message': f'Discovery error: {str(e)}',
                'source_type': 'nvr',
                'protocol': 'onvif',
                'cameras': []
            }

    def _test_single_port(self, ip: str, port: int, username: str = '', password: str = '') -> Dict:
        """Test single ONVIF camera on specific port"""
        try:
            soap_request = '''<?xml version="1.0" encoding="UTF-8"?>
<soap:Envelope xmlns:soap="http://www.w3.org/2003/05/soap-envelope" xmlns:tds="http://www.onvif.org/ver10/device/wsdl">
<soap:Header/>
<soap:Body>
<tds:GetDeviceInformation/>
</soap:Body>
</soap:Envelope>'''
            
            headers = {
                'Content-Type': 'application/soap+xml; charset=utf-8',
                'Content-Length': str(len(soap_request))
            }
            
            response = requests.post(
                f'http://{ip}:{port}/onvif/device_service',
                data=soap_request,
                headers=headers,
                timeout=5
            )
            
            if response.status_code == 200 and 'GetDeviceInformationResponse' in response.text:
                # Parse device info
                manufacturer = self._extract_xml_value(response.text, 'tds:Manufacturer', 'ACME Security')
                model = self._extract_xml_value(response.text, 'tds:Model', f'Camera-{port}')
                firmware = self._extract_xml_value(response.text, 'tds:FirmwareVersion', '2.0')
                
                # Port-based camera mapping
                camera_names = {
                    1000: "Front Door Camera",
                    1001: "Parking Lot Camera", 
                    1002: "Warehouse Camera"
                }
                
                rtsp_ports = {
                    1000: 8554,
                    1001: 8555,
                    1002: 8556
                }
                
                resolutions = {
                    1000: "1920x1080",
                    1001: "1280x720",
                    1002: "800x600"
                }
                
                codecs = {
                    1000: "H264",
                    1001: "H265", 
                    1002: "MPEG4"
                }
                
                camera_name = camera_names.get(port, f"Camera Port {port}")
                rtsp_port = rtsp_ports.get(port, 8554)
                resolution = resolutions.get(port, "640x480")
                codec = codecs.get(port, "H264")
                
                return {
                    'id': f"onvif_{ip}_{port}",
                    'name': camera_name,
                    'description': f"ONVIF {model} ({firmware})",
                    'stream_url': f"rtsp://{ip}:{rtsp_port}/stream",
                    'resolution': resolution,
                    'codec': codec,
                    'capabilities': ['recording'] + (['ptz'] if port == 1000 else []),
                    'onvif_port': port,
                    'rtsp_port': rtsp_port,
                    'manufacturer': manufacturer,
                    'model': model,
                    'firmware': firmware
                }
            else:
                return None
                
        except Exception as e:
            logger.warning(f"Failed to test port {port}: {e}")
            return None

    def _extract_xml_value(self, xml_text: str, tag: str, default: str = '') -> str:
        """Extract value from XML tag"""
        try:
            start_tag = f'<{tag}>'
            end_tag = f'</{tag}>'
            if start_tag in xml_text:
                start = xml_text.find(start_tag) + len(start_tag)
                end = xml_text.find(end_tag)
                return xml_text[start:end].strip() or default
            return default
        except:
            return default

# Global instance
onvif_client = VTrackOnvifClient()
```
## ğŸ“„ File: `google_drive_service.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/sources/google_drive_service.py`

```python
#!/usr/bin/env python3

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from google.oauth2.credentials import Credentials
import time
import logging
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

class GoogleDriveFolderService:
    """Service class for Google Drive folder operations with lazy loading support"""
    
    def __init__(self, credentials: Credentials):
        """Initialize service with Google Drive credentials"""
        self.credentials = credentials
        self.service = build('drive', 'v3', credentials=credentials)
        self.cache = {}
        self.cache_duration = 180  # 3 minutes cache
    
    def _get_cache_key(self, operation: str, *args) -> str:
        """Generate cache key for operations"""
        return f"{operation}:{'_'.join(str(arg) for arg in args)}"
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """Check if cached data is still valid"""
        if cache_key not in self.cache:
            return False
        
        cached_time = self.cache[cache_key].get('timestamp', 0)
        return time.time() - cached_time < self.cache_duration
    
    def _set_cache(self, cache_key: str, data: any):
        """Cache data with timestamp"""
        self.cache[cache_key] = {
            'data': data,
            'timestamp': time.time()
        }
    
    def _get_cache(self, cache_key: str) -> any:
        """Get cached data if valid"""
        if self._is_cache_valid(cache_key):
            return self.cache[cache_key]['data']
        return None
    
    def get_subfolders(self, parent_id: str = 'root', max_results: int = 50) -> List[Dict]:
        """
        Get subfolders of a parent folder with caching
        
        Args:
            parent_id: Parent folder ID ('root' for root folders)
            max_results: Maximum number of folders to return
            
        Returns:
            List of folder dictionaries with id, name, parents, createdTime
        """
        cache_key = self._get_cache_key('subfolders', parent_id, max_results)
        cached_result = self._get_cache(cache_key)
        
        if cached_result is not None:
            logger.debug(f"ğŸ“‹ Cache hit for subfolders: {parent_id}")
            return cached_result
        
        try:
            logger.info(f"ğŸ“‚ Fetching subfolders for parent: {parent_id}")
            
            # Build query for folders only
            if parent_id == 'root':
                query = "mimeType='application/vnd.google-apps.folder' and 'root' in parents and trashed=false"
            else:
                query = f"mimeType='application/vnd.google-apps.folder' and '{parent_id}' in parents and trashed=false"
            
            results = self.service.files().list(
                q=query,
                pageSize=min(max_results, 100),  # Google Drive max is 100
                fields="files(id, name, parents, createdTime, modifiedTime)",
                orderBy="name"
            ).execute()
            
            folders = results.get('files', [])
            
            # Format folder data
            formatted_folders = []
            for folder in folders:
                formatted_folder = {
                    'id': folder['id'],
                    'name': folder['name'],
                    'type': 'folder',
                    'parent_id': parent_id,
                    'parents': folder.get('parents', []),
                    'created': folder.get('createdTime'),
                    'modified': folder.get('modifiedTime')
                }
                formatted_folders.append(formatted_folder)
            
            # Cache the result
            self._set_cache(cache_key, formatted_folders)
            
            logger.info(f"âœ… Found {len(formatted_folders)} subfolders in {parent_id}")
            return formatted_folders
            
        except HttpError as e:
            logger.error(f"âŒ HTTP error getting subfolders for {parent_id}: {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ Error getting subfolders for {parent_id}: {e}")
            return []
    
    def calculate_folder_depth(self, folder_id: str) -> int:
        """
        Calculate the depth level of a folder (0=root, 1=level1, etc.)
        
        Args:
            folder_id: Folder ID to calculate depth for
            
        Returns:
            Integer depth level (0-based)
        """
        if folder_id == 'root':
            return 0
        
        cache_key = self._get_cache_key('depth', folder_id)
        cached_result = self._get_cache(cache_key)
        
        if cached_result is not None:
            return cached_result
        
        try:
            depth = 0
            current_id = folder_id
            
            # Traverse up the folder hierarchy
            while current_id != 'root' and depth < 10:  # Prevent infinite loops
                try:
                    folder_info = self.service.files().get(
                        fileId=current_id,
                        fields="parents"
                    ).execute()
                    
                    parents = folder_info.get('parents', [])
                    if not parents:
                        break
                    
                    current_id = parents[0]  # Use first parent
                    depth += 1
                    
                    # Root check
                    if current_id == 'root':
                        break
                        
                except HttpError as e:
                    logger.warning(f"âš ï¸ Cannot get parent for {current_id}: {e}")
                    break
            
            # Cache the result
            self._set_cache(cache_key, depth)
            
            logger.debug(f"ğŸ“ Folder {folder_id} is at depth {depth}")
            return depth
            
        except Exception as e:
            logger.error(f"âŒ Error calculating depth for {folder_id}: {e}")
            return 0
    
    def build_folder_path(self, folder_id: str) -> str:
        """
        Build the full path string for a folder
        
        Args:
            folder_id: Folder ID to build path for
            
        Returns:
            Full folder path string (e.g., "/Project/Area/Date/Camera")
        """
        if folder_id == 'root':
            return "/My Drive"
        
        cache_key = self._get_cache_key('path', folder_id)
        cached_result = self._get_cache(cache_key)
        
        if cached_result is not None:
            return cached_result
        
        try:
            path_parts = []
            current_id = folder_id
            
            # Traverse up the hierarchy collecting names
            while current_id != 'root' and len(path_parts) < 10:
                try:
                    folder_info = self.service.files().get(
                        fileId=current_id,
                        fields="name, parents"
                    ).execute()
                    
                    folder_name = folder_info.get('name', 'Unknown')
                    path_parts.insert(0, folder_name)
                    
                    parents = folder_info.get('parents', [])
                    if not parents:
                        break
                    
                    current_id = parents[0]
                    
                except HttpError as e:
                    logger.warning(f"âš ï¸ Cannot get folder info for {current_id}: {e}")
                    break
            
            # Build full path
            if path_parts:
                full_path = "/My Drive/" + "/".join(path_parts)
            else:
                full_path = "/My Drive"
            
            # Cache the result
            self._set_cache(cache_key, full_path)
            
            logger.debug(f"ğŸ“ Folder path for {folder_id}: {full_path}")
            return full_path
            
        except Exception as e:
            logger.error(f"âŒ Error building path for {folder_id}: {e}")
            return "/My Drive/Unknown"
    
    def is_selectable_folder(self, folder_depth: int) -> bool:
        """
        Check if a folder at given depth can be selected
        
        Args:
            folder_depth: Depth level of the folder
            
        Returns:
            True if folder can be selected (depth == 4), False otherwise
        """
        return folder_depth == 4
    
    def get_folder_info(self, folder_id: str) -> Dict:
        """
        Get comprehensive information about a folder
        
        Args:
            folder_id: Folder ID to get info for
            
        Returns:
            Dictionary with folder information
        """
        cache_key = self._get_cache_key('info', folder_id)
        cached_result = self._get_cache(cache_key)
        
        if cached_result is not None:
            return cached_result
        
        try:
            folder_info = self.service.files().get(
                fileId=folder_id,
                fields="id, name, parents, createdTime, modifiedTime, size, mimeType"
            ).execute()
            
            depth = self.calculate_folder_depth(folder_id)
            path = self.build_folder_path(folder_id)
            
            info = {
                'id': folder_info['id'],
                'name': folder_info['name'],
                'parents': folder_info.get('parents', []),
                'created': folder_info.get('createdTime'),
                'modified': folder_info.get('modifiedTime'),
                'depth': depth,
                'path': path,
                'selectable': self.is_selectable_folder(depth),
                'mime_type': folder_info.get('mimeType')
            }
            
            # Cache the result
            self._set_cache(cache_key, info)
            
            return info
            
        except HttpError as e:
            logger.error(f"âŒ HTTP error getting folder info for {folder_id}: {e}")
            return {}
        except Exception as e:
            logger.error(f"âŒ Error getting folder info for {folder_id}: {e}")
            return {}
    
    def search_folders(self, query: str, max_results: int = 20) -> List[Dict]:
        """
        Search for folders by name
        
        Args:
            query: Search query string
            max_results: Maximum number of results
            
        Returns:
            List of matching folders
        """
        try:
            logger.info(f"ğŸ” Searching folders: {query}")
            
            # Escape query for Google Drive search
            escaped_query = query.replace("'", "\\'").replace("\\", "\\\\")
            
            search_query = f"mimeType='application/vnd.google-apps.folder' and name contains '{escaped_query}' and trashed=false"
            
            results = self.service.files().list(
                q=search_query,
                pageSize=min(max_results, 100),
                fields="files(id, name, parents, createdTime)",
                orderBy="name"
            ).execute()
            
            folders = results.get('files', [])
            
            # Add depth and path information
            enriched_folders = []
            for folder in folders:
                depth = self.calculate_folder_depth(folder['id'])
                path = self.build_folder_path(folder['id'])
                
                enriched_folder = {
                    'id': folder['id'],
                    'name': folder['name'],
                    'parents': folder.get('parents', []),
                    'created': folder.get('createdTime'),
                    'depth': depth,
                    'path': path,
                    'selectable': self.is_selectable_folder(depth)
                }
                enriched_folders.append(enriched_folder)
            
            logger.info(f"âœ… Found {len(enriched_folders)} folders matching '{query}'")
            return enriched_folders
            
        except Exception as e:
            logger.error(f"âŒ Error searching folders: {e}")
            return []
    
    def has_subfolders(self, folder_id: str) -> bool:
        """
        Check if a folder has any subfolders (for UI expand indicators)
        
        Args:
            folder_id: Folder ID to check
            
        Returns:
            True if folder has subfolders, False otherwise
        """
        cache_key = self._get_cache_key('has_subfolders', folder_id)
        cached_result = self._get_cache(cache_key)
        
        if cached_result is not None:
            return cached_result
        
        try:
            query = f"mimeType='application/vnd.google-apps.folder' and '{folder_id}' in parents and trashed=false"
            
            results = self.service.files().list(
                q=query,
                pageSize=1,  # Only need to know if any exist
                fields="files(id)"
            ).execute()
            
            has_folders = len(results.get('files', [])) > 0
            
            # Cache the result
            self._set_cache(cache_key, has_folders)
            
            return has_folders
            
        except Exception as e:
            logger.error(f"âŒ Error checking subfolders for {folder_id}: {e}")
            return False
    
    def get_folder_statistics(self, folder_id: str) -> Dict:
        """
        Get statistics about a folder (file count, total size, etc.)
        
        Args:
            folder_id: Folder ID to get stats for
            
        Returns:
            Dictionary with folder statistics
        """
        try:
            # Get all files in folder
            query = f"'{folder_id}' in parents and trashed=false"
            
            results = self.service.files().list(
                q=query,
                pageSize=1000,  # Get more files for accurate count
                fields="files(id, name, size, mimeType)"
            ).execute()
            
            files = results.get('files', [])
            
            # Calculate statistics
            total_files = len(files)
            total_size = 0
            video_count = 0
            folder_count = 0
            
            video_mimes = [
                'video/mp4', 'video/avi', 'video/mov', 'video/mkv',
                'video/m4v', 'video/wmv', 'video/flv', 'video/webm'
            ]
            
            for file in files:
                mime_type = file.get('mimeType', '')
                size = int(file.get('size', 0))
                
                total_size += size
                
                if mime_type == 'application/vnd.google-apps.folder':
                    folder_count += 1
                elif any(vm in mime_type for vm in video_mimes):
                    video_count += 1
            
            stats = {
                'total_files': total_files,
                'total_size_bytes': total_size,
                'total_size_mb': round(total_size / (1024 * 1024), 2),
                'video_count': video_count,
                'folder_count': folder_count,
                'other_files': total_files - video_count - folder_count
            }
            
            logger.debug(f"ğŸ“Š Stats for {folder_id}: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"âŒ Error getting folder statistics for {folder_id}: {e}")
            return {
                'total_files': 0,
                'total_size_bytes': 0,
                'total_size_mb': 0,
                'video_count': 0,
                'folder_count': 0,
                'other_files': 0
            }
    
    def clear_cache(self):
        """Clear all cached data"""
        self.cache.clear()
        logger.info("ğŸ§¹ Cleared Google Drive folder service cache")
    
    def get_cache_info(self) -> Dict:
        """Get information about current cache state"""
        valid_entries = 0
        expired_entries = 0
        
        current_time = time.time()
        
        for key, entry in self.cache.items():
            if current_time - entry['timestamp'] < self.cache_duration:
                valid_entries += 1
            else:
                expired_entries += 1
        
        return {
            'total_entries': len(self.cache),
            'valid_entries': valid_entries,
            'expired_entries': expired_entries,
            'cache_duration_seconds': self.cache_duration
        }
```
## ğŸ“„ File: `TG.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/db_utils/TG.py`

```python

```
## ğŸ“„ File: `__init__.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/db_utils/__init__.py`

```python
from .db_utils import find_project_root, get_db_connection

```
## ğŸ“„ File: `db_utils.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/db_utils/db_utils.py`

```python
import sqlite3
import os

# HÃ m tÃ¬m thÆ° má»¥c gá»‘c dá»± Ã¡n dá»±a trÃªn tÃªn thÆ° má»¥c
def find_project_root(start_path):
    current_path = os.path.abspath(start_path)
    while os.path.basename(current_path) != "V_Track":
        parent_path = os.path.dirname(current_path)
        if parent_path == current_path:  # ÄÃ£ Ä‘áº¿n thÆ° má»¥c gá»‘c cá»§a há»‡ thá»‘ng (/)
            raise ValueError("Could not find project root (V_Track directory)")
        current_path = parent_path
    return current_path

# XÃ¡c Ä‘á»‹nh thÆ° má»¥c gá»‘c cá»§a dá»± Ã¡n
BASE_DIR = find_project_root(os.path.abspath(__file__))

# Äá»‹nh nghÄ©a DB_PATH máº·c Ä‘á»‹nh dá»±a trÃªn BASE_DIR
DEFAULT_DB_PATH = os.path.join(BASE_DIR, "backend/database", "events.db")
os.makedirs(os.path.dirname(DEFAULT_DB_PATH), exist_ok=True)  # Táº¡o thÆ° má»¥c database náº¿u chÆ°a cÃ³

# HÃ m láº¥y DB_PATH tá»« processing_config
def get_db_path():
    try:
        conn = sqlite3.connect(DEFAULT_DB_PATH)  # Káº¿t ná»‘i táº¡m thá»i Ä‘á»ƒ truy váº¥n
        cursor = conn.cursor()
        cursor.execute("SELECT db_path FROM processing_config WHERE id = 1")
        result = cursor.fetchone()
        conn.close()
        return result[0] if result else DEFAULT_DB_PATH
    except Exception as e:
        print(f"Error getting DB_PATH from database: {e}")
        return DEFAULT_DB_PATH

DB_PATH = get_db_path()

def get_db_connection():
    if not os.path.exists(DB_PATH):
        raise FileNotFoundError(f"Database file not found: {DB_PATH}")
    if not os.access(DB_PATH, os.R_OK):
        raise PermissionError(f"No read permission for database: {DB_PATH}")
    if not os.access(DB_PATH, os.W_OK):
        raise PermissionError(f"No write permission for database: {DB_PATH}")
    return sqlite3.connect(DB_PATH, check_same_thread=False)

```
## ğŸ“„ File: `__init__.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/account/__init__.py`

```python

```
## ğŸ“„ File: `account.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/account/account.py`

```python

```
## ğŸ“„ File: `query.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/query/query.py`

```python
from flask import Blueprint, request, jsonify
from datetime import datetime
import csv
import io
import os
import base64
import pandas as pd
import json
from io import BytesIO
from modules.db_utils import find_project_root, get_db_connection
from ..utils.file_parser import parse_uploaded_file
from modules.scheduler.db_sync import db_rwlock  # ThÃªm import db_rwlock

query_bp = Blueprint('query', __name__)

# XÃ¡c Ä‘á»‹nh thÆ° má»¥c gá»‘c cá»§a dá»± Ã¡n
BASE_DIR = find_project_root(os.path.abspath(__file__))

# Äá»‹nh nghÄ©a DB_PATH dá»±a trÃªn BASE_DIR
DB_PATH = os.path.join(BASE_DIR, "database", "events.db")
os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)

@query_bp.route('/get-csv-headers', methods=['POST'])
def get_csv_headers():
    data = request.get_json()
    file_content = data.get('file_content', '')
    file_path = data.get('file_path', '')
    is_excel = data.get('is_excel', False)

    if file_content:
        if not file_content.strip():
            return jsonify({"error": "File CSV is empty. Please provide a valid CSV file with content."}), 400

        try:
            df = parse_uploaded_file(file_content=file_content, is_excel=is_excel)
            rows = [df.columns.tolist()]
        except Exception as e:
            return jsonify({"error": f"Failed to read file content: {str(e)}. Ensure the content is properly formatted."}), 400
    elif file_path:
        if not os.path.exists(file_path):
            return jsonify({"error": f"File not found at path: {file_path}. Please check the file path and try again."}), 404

        try:
            with open(file_path, "rb") as f:
                file_content = base64.b64encode(f.read()).decode("utf-8")
            df = parse_uploaded_file(file_content=file_content, is_excel=is_excel)
            rows = [df.columns.tolist()]
        except Exception as e:
            return jsonify({"error": f"Failed to read file from path {file_path}: {str(e)}. Ensure the file is accessible and properly formatted."}), 400
    else:
        return jsonify({"error": "No file content or path provided. Please provide either file content or a valid file path."}), 400

    if not rows or len(rows) < 1:
        return jsonify({"error": "CSV file has no header. Please ensure the CSV file contains at least one row with headers."}), 400

    header = rows[0]
    if not header:
        return jsonify({"error": "CSV file header is empty. Please ensure the first row contains valid headers."}), 400

    return jsonify({"headers": header}), 200

@query_bp.route('/parse-csv', methods=['POST'])
def parse_csv():
    data = request.get_json()
    file_content = data.get('file_content', '')
    file_path = data.get('file_path', '')
    column_name = data.get('column_name', 'tracking_codes')
    is_excel = data.get('is_excel', False)

    try:
        if file_content:
            df = parse_uploaded_file(file_content=file_content, is_excel=is_excel)
        elif file_path:
            with open(file_path, "rb") as f:
                file_content = base64.b64encode(f.read()).decode("utf-8")
            df = parse_uploaded_file(file_content=file_content, is_excel=is_excel)
        else:
            return jsonify({"error": "No file provided"}), 400

        if column_name not in df.columns:
            return jsonify({"error": f"Cá»™t '{column_name}' khÃ´ng tá»“n táº¡i trong file."}), 400

        values = df[column_name].dropna().astype(str).tolist()
        codes = []
        for val in values:
            # Thá»­ cáº¯t chuá»—i theo dáº¥u pháº©y trÆ°á»›c
            split_vals = val.split(',')
            if len(split_vals) == 1:  # Náº¿u khÃ´ng cáº¯t Ä‘Æ°á»£c, thá»­ dáº¥u cháº¥m pháº©y
                split_vals = val.split(';')
            codes.extend(v.strip() for v in split_vals if v.strip())
        codes = list(set(codes))  # Loáº¡i bá» trÃ¹ng láº·p

        return jsonify({"tracking_codes": codes}), 200

    except Exception as e:
        return jsonify({"error": f"Failed to parse CSV: {str(e)}. Ensure the file and column name are valid."}), 500

@query_bp.route('/query', methods=['POST'])
def query_events():
    data = request.get_json()
    print(f"Received data: {data}")  # Log dá»¯ liá»‡u nháº­n Ä‘Æ°á»£c
    search_string = data.get('search_string', '')
    default_days = data.get('default_days', 7)  # ThÃªm giÃ¡ trá»‹ máº·c Ä‘á»‹nh 7 náº¿u khÃ´ng cÃ³ trong request
    from_time = data.get('from_time')
    to_time = data.get('to_time')
    selected_cameras = data.get('selected_cameras', [])  # Láº¥y selected_cameras

    # TÃ¡ch search_string theo dÃ²ng vÃ  loáº¡i bá» sá»‘ thá»© tá»±
    tracking_codes = []
    if search_string:
        lines = search_string.splitlines()
        for line in lines:
            line = line.strip()
            if line:
                # Loáº¡i bá» sá»‘ thá»© tá»± (e.g., "1. " hoáº·c "2. ")
                code = line.split('. ', 1)[-1].strip()
                if code:
                    tracking_codes.append(code)
    print(f"Parsed tracking_codes from search_string: {tracking_codes}")  # Log tracking_codes Ä‘Ã£ tÃ¡ch

    try:
        if from_time and to_time:
            try:
                from_timestamp = int(datetime.fromisoformat(from_time.replace('Z', '+00:00')).timestamp() * 1000)
                to_timestamp = int(datetime.fromisoformat(to_time.replace('Z', '+00:00')).timestamp() * 1000)
            except ValueError as e:
                return jsonify({"error": f"Invalid time format for from_time or to_time: {str(e)}. Use ISO format (e.g., 2023-10-01T00:00:00Z)."}), 400
        else:
            to_timestamp = int(datetime.now().timestamp() * 1000)
            from_timestamp = to_timestamp - (default_days * 24 * 60 * 60 * 1000)
        print(f"Time range: from_timestamp={from_timestamp}, to_timestamp={to_timestamp}")  # Log khoáº£ng thá»i gian

        with db_rwlock.gen_rlock():  # ThÃªm khÃ³a Ä‘á»c
            conn = get_db_connection()
            cursor = conn.cursor()

            query = """
                SELECT event_id, ts, te, duration, tracking_codes, video_file, packing_time_start, packing_time_end
                FROM events
                WHERE is_processed = 0
            """
            params = []
            # Chá»‰ thÃªm Ä‘iá»u kiá»‡n thá»i gian náº¿u packing_time_start khÃ´ng null
            if from_timestamp and to_timestamp:
                query += " AND (packing_time_start IS NULL OR (packing_time_start >= ? AND packing_time_start <= ?))"
                params.extend([from_timestamp, to_timestamp])
            if selected_cameras:
                query += " AND camera_name IN ({})".format(','.join('?' * len(selected_cameras)))
                params.extend(selected_cameras)

            print(f"Executing query: {query} with params: {params}")  # Log truy váº¥n
            cursor.execute(query, params)
            events = cursor.fetchall()
            print(f"Fetched events: {events}")  # Log káº¿t quáº£ truy váº¥n

            filtered_events = []
            for event in events:
                event_dict = {
                    'event_id': event[0],
                    'ts': event[1],
                    'te': event[2],
                    'duration': event[3],
                    'tracking_codes': event[4],
                    'video_file': event[5],
                    'packing_time_start': event[6],
                    'packing_time_end': event[7]
                }
                print(f"Raw tracking_codes for event {event[0]}: {event[4]}")  # Log giÃ¡ trá»‹ thÃ´ cá»§a tracking_codes
                try:
                    tracking_codes_list = json.loads(event[4]) if event[4] else []
                    if not isinstance(tracking_codes_list, list):
                        raise ValueError("tracking_codes is not a list")
                except Exception:
                    print(f"[WARN] tracking_codes fallback for event {event[0]}")
                    tracking_codes_list = []
                    if event[4]:
                        raw = event[4].strip("[]").replace("'", "").replace('"', "")
                        tracking_codes_list = [code.strip() for code in raw.split(',')]
                print(f"Parsed tracking_codes for event {event[0]}: {tracking_codes_list}")  # Log tracking_codes Ä‘Ã£ parse
                if not tracking_codes:
                    filtered_events.append(event_dict)
                else:
                    for code in tracking_codes:
                        if code in tracking_codes_list:
                            filtered_events.append(event_dict)
                            break
            print(f"Filtered events: {filtered_events}")  # Log káº¿t quáº£ sau khi lá»c

            conn.close()
        return jsonify({'events': filtered_events}), 200
    except Exception as e:
        print(f"Error in query_events: {str(e)}")  # Log lá»—i chi tiáº¿t
        return jsonify({"error": f"Failed to query events: {str(e)}. Ensure the database is accessible and the events table exists."}), 500
```
## ğŸ“„ File: `__init__.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/query/__init__.py`

```python

```
## ğŸ“„ File: `trigger_processor.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/trigger_processor.py`

```python
import cv2
import logging
import time
import os
from datetime import timedelta
from modules.db_utils import get_db_connection
from modules.scheduler.db_sync import db_rwlock

def run_trigger_logic(
    core_sampler,
    video_capture,
    video_metadata,
    trigger_roi_coords_for_state_check,
    get_log_handle_callback
):
    core_sampler.logger.info(f"TRIGGER_PROCESSOR: Starting for {video_metadata['base_video_name']}")
    frame_idx_counter_tr = 0
    frame_states_buffer_list_tr = []
    mvd_buffer_list_tr = []
    last_recorded_state_tr = None
    last_recorded_mvd_tr = ""
    second = 0
    current_start_second = 0
    current_end_second = core_sampler.log_segment_duration
    log_file = os.path.join(core_sampler.log_dir_output_segments, f"log_{video_metadata['base_video_name']}_{current_start_second:04d}_{current_end_second:04d}.txt")
    log_file_handle = open(log_file, 'w')
    log_file_handle.write(f"# Start: {current_start_second}, End: {current_end_second}, Start_Time: {(video_metadata['start_time_obj'] + timedelta(seconds=current_start_second)).strftime('%Y-%m-%d %H:%M:%S')}, Camera_Name: {video_metadata['camera_name']}, Video_File: {video_metadata['absolute_video_path']}\n")
    log_file_handle.flush()
    with db_rwlock.gen_wlock():
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT 1 FROM processed_logs WHERE log_file = ?", (log_file,))
        log_exists = cursor.fetchone()
        if not log_exists:
            cursor.execute("INSERT INTO processed_logs (log_file, is_processed) VALUES (?, 0)", (log_file,))
        conn.commit()
        conn.close()
    while video_capture.isOpened():
        ret, bgr_frame = video_capture.read()
        if not ret: break
        frame_idx_counter_tr += 1
        if frame_idx_counter_tr % core_sampler.frame_interval != 0: continue
        state_val, mvd_val = core_sampler._internal_process_frame_qr(
            bgr_frame,
            frame_idx_counter_tr,
            trigger_roi_coords_for_state_check=trigger_roi_coords_for_state_check
        )
        frame_states_buffer_list_tr.append(state_val)
        mvd_buffer_list_tr.append(mvd_val)
        second_in_video = (frame_idx_counter_tr - 1) / core_sampler.fps
        second = round(second_in_video)
        if second >= current_end_second:
            log_file_handle.close()
            current_start_second = current_end_second
            current_end_second += core_sampler.log_segment_duration
            log_file = os.path.join(core_sampler.log_dir_output_segments, f"log_{video_metadata['base_video_name']}_{current_start_second:04d}_{current_end_second:04d}.txt")
            log_file_handle = open(log_file, 'w')
            log_file_handle.write(f"# Start: {current_start_second}, End: {current_end_second}, Start_Time: {(video_metadata['start_time_obj'] + timedelta(seconds=current_start_second)).strftime('%Y-%m-%d %H:%M:%S')}, Camera_Name: {video_metadata['camera_name']}, Video_File: {video_metadata['absolute_video_path']}\n")
            log_file_handle.flush()
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("SELECT 1 FROM processed_logs WHERE log_file = ?", (log_file,))
                log_exists = cursor.fetchone()
                if not log_exists:
                    cursor.execute("INSERT INTO processed_logs (log_file, is_processed) VALUES (?, 0)", (log_file,))
                conn.commit()
                conn.close()
        if len(frame_states_buffer_list_tr) == 5:
            on_count_tr = frame_states_buffer_list_tr.count("On")
            determined_final_state_tr = "On" if on_count_tr >= 3 else "Off"
            determined_final_mvd_tr = mvd_buffer_list_tr[-1] if mvd_buffer_list_tr else ""
            if determined_final_state_tr != last_recorded_state_tr or determined_final_mvd_tr != last_recorded_mvd_tr:
                log_handle_tr = get_log_handle_callback(second_in_video)
                log_handle_tr.write(f"{second},{determined_final_state_tr},{determined_final_mvd_tr}\n")
                log_handle_tr.flush()
                last_recorded_state_tr = determined_final_state_tr
                last_recorded_mvd_tr = determined_final_mvd_tr
            frame_states_buffer_list_tr.clear()
            mvd_buffer_list_tr.clear()
    log_file_handle.close()
    core_sampler.logger.info(f"TRIGGER_PROCESSOR: Finished for {video_metadata['base_video_name']}")
```
## ğŸ“„ File: `IdleMonitor.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/IdleMonitor.py`

```python
import cv2
import mediapipe as mp
import queue
import os
import logging
from datetime import datetime
import uuid
from modules.config.logging_config import get_logger

class IdleMonitor:
    def __init__(self, processing_config=None):
        """Khá»Ÿi táº¡o IdleMonitor vá»›i queue vÃ  processing_config."""
        self.video_file = None
        self.logger = get_logger("app", {"video_id": None})
        self.logger.setLevel(logging.INFO)
        self.work_block_queue = queue.Queue()  # Queue lÆ°u work block
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.6)
        self.IDLE_GAP = 120  # seconds
        self.HAND_SAMPLE_INTERVAL = 1  # seconds
        self.MIN_WORK_BLOCK = 10  # seconds
        self.MIN_PACKING_TIME = processing_config.get('min_packing_time', 5) if processing_config else 5  # seconds
        self.CHUNK_SIZE = int(self.MIN_PACKING_TIME * 0.8) # seconds
        self.video_id = str(uuid.uuid4())  # Äá»‹nh danh duy nháº¥t cho video

    def process_video(self, video_file, camera_name, packing_area):
        """Xá»­ lÃ½ video, xÃ¡c Ä‘á»‹nh work block, lÆ°u vÃ o queue, dÃ¹ng packing_area tá»« program_runner."""
        self.video_file = video_file
        self.logger = get_logger("app", {"video_id": os.path.basename(self.video_file)})
        # Äá»c video
        cap = cv2.VideoCapture(video_file)
        if not cap.isOpened():
            self.logger.error(f"Failed to open video: {video_file}")
            return

        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        video_duration = int(total_frames / fps)
        self.logger.info(f"Processing video: {video_file}, Duration: {video_duration}s, Video ID: {self.video_id}")

        # Kiá»ƒm tra packing_area
        roi = packing_area
        if not roi:
            self.logger.warning(f"No packing_area for {camera_name}, using full frame")
            roi = None

        hand_timeline = []
        event_id = 0

        # QuÃ©t video theo chunk
        sec = 0
        while sec < video_duration:
            chunk_end = min(sec + self.CHUNK_SIZE, video_duration)
            chunk_has_hand = False
            check_time = sec
            # QuÃ©t Ä‘á»u toÃ n chunk
            while check_time < chunk_end:
                frame_id = int(check_time * fps)
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)
                ret, frame = cap.read()
                if not ret:
                    break

                # Ãp dá»¥ng ROI náº¿u cÃ³
                if roi:
                    x, y, w, h = roi
                    frame_height, frame_width = frame.shape[:2]
                    if w > 0 and h > 0 and y + h <= frame_height and x + w <= frame_width:
                        frame = frame[y:y+h, x:x+w]
                    else:
                        self.logger.warning(f"Invalid ROI for frame {frame_id}: {roi}")
                        frame = frame

                # Hand detection
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = self.hands.process(rgb_frame)
                if results.multi_hand_landmarks is not None:
                    chunk_has_hand = True
                    break
                check_time += self.HAND_SAMPLE_INTERVAL

            hand_timeline.append(chunk_has_hand)
            #self.logger.info(f"[Chunk {sec:04d}-{chunk_end:04d}s] Hand: {chunk_has_hand}, Video ID: {self.video_id}")
            sec += self.CHUNK_SIZE

        # PhÃ¡t hiá»‡n idle block
        idle_gap_list = []
        idle_candidate_start = None
        for tick_time, hand_detected in enumerate(hand_timeline):
            if hand_detected:
                if idle_candidate_start is not None:
                    idle_duration = tick_time - idle_candidate_start
                    if idle_duration * self.CHUNK_SIZE >= self.IDLE_GAP:
                        idle_gap_list.append({'start': idle_candidate_start * self.CHUNK_SIZE, 'end': tick_time * self.CHUNK_SIZE})
                    idle_candidate_start = None
            else:
                if idle_candidate_start is None:
                    idle_candidate_start = tick_time

        if idle_candidate_start is not None:
            idle_duration = len(hand_timeline) - idle_candidate_start
            if idle_duration * self.CHUNK_SIZE >= self.IDLE_GAP:
                idle_gap_list.append({'start': idle_candidate_start * self.CHUNK_SIZE, 'end': len(hand_timeline) * self.CHUNK_SIZE})

        # PhÃ¡t hiá»‡n work block
        work_blocks = []
        prev_end = 0
        for idle in idle_gap_list:
            if idle['start'] > prev_end:
                work_blocks.append({'start': prev_end, 'end': idle['start']})
            prev_end = idle['end']
        if prev_end < len(hand_timeline) * self.CHUNK_SIZE:
            work_blocks.append({'start': prev_end, 'end': len(hand_timeline) * self.CHUNK_SIZE})

        # LÆ°u work block vÃ o queue
        for idx, block in enumerate(work_blocks):
            duration = block['end'] - block['start']
            event_id += 1
            if duration < self.MIN_WORK_BLOCK:
                self.logger.info(f"Skipping work block {idx+1}: duration {duration}s < {self.MIN_WORK_BLOCK}s")
                continue
            self.work_block_queue.put({
                'video_id': self.video_id,
                'event_id': f"evt_{event_id:03d}",
                'file_path': video_file,
                'start_time': block['start'],
                'end_time': block['end']
            })
            self.logger.info(f"Work block {idx+1}: {block['start']}s --> {block['end']}s (duration: {duration}s), Video ID: {self.video_id}")
            if duration < self.MIN_WORK_BLOCK:
                self.logger.warning(f"Block shorter than {self.MIN_WORK_BLOCK}s")

        cap.release()
        self.hands.close()

    def get_work_block_queue(self):
        """Tráº£ vá» queue chá»©a work block."""
        return self.work_block_queue
```
## ğŸ“„ File: `qr_detector.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/qr_detector.py`

```python
import cv2
import os
import json
import logging
import queue
import threading
import time
import glob
import traceback
from datetime import datetime
from modules.config.logging_config import get_logger


# Äáº£m báº£o thÆ° má»¥c LOG tá»“n táº¡i
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
LOG_DIR = os.path.join(BASE_DIR, "resources", "output_clips", "LOG")
os.makedirs(LOG_DIR, exist_ok=True)

# Khá»Ÿi táº¡o logger mÃ  khÃ´ng sá»­ dá»¥ng video_path
logger = get_logger(__name__, {"module": "qr_detector"})
logger.info("Logging initialized")

# ÄÆ°á»ng dáº«n tá»›i mÃ´ hÃ¬nh WeChat QRCode (tÆ°Æ¡ng Ä‘á»‘i)
MODEL_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "models", "wechat_qr")
DETECT_PROTO = os.path.join(MODEL_DIR, "detect.prototxt")
DETECT_MODEL = os.path.join(MODEL_DIR, "detect.caffemodel")
SR_PROTO = os.path.join(MODEL_DIR, "sr.prototxt")
SR_MODEL = os.path.join(MODEL_DIR, "sr.caffemodel")

# ÄÆ°á»ng dáº«n lÆ°u áº£nh
CAMERA_ROI_DIR = os.path.join(BASE_DIR, "resources", "output_clips", "CameraROI")

def select_qr_roi(video_path, camera_id, roi_frame_path, step="mvd"):
    """
    Cho phÃ©p ngÆ°á»i dÃ¹ng váº½ ROI cho mÃ£ QR (1 hoáº·c 2 vÃ¹ng cho mvd), sau Ä‘Ã³ xá»­ lÃ½ video.
    Args:
        video_path (str): ÄÆ°á»ng dáº«n Ä‘áº¿n file video.
        camera_id (str): ID cá»§a camera.
        roi_frame_path (str): ÄÆ°á»ng dáº«n Ä‘áº¿n áº£nh táº¡m cuá»‘i cÃ¹ng tá»« bÆ°á»›c trÆ°á»›c (Ä‘Ã£ cÃ³ ROI váº½ sáºµn).
        step (str): Giai Ä‘oáº¡n hiá»‡n táº¡i (mvd).
    Returns:
        dict: {'success': bool, 'rois': [{'x': int, 'y': int, 'w': int, 'h': int, 'type': str}, ...], 
               'roi_frame': str, 'qr_detected': bool, 'qr_detected_roi1': bool, 'qr_detected_roi2': bool, 
               'qr_content': str, 'trigger_detected': bool, 'table_type': str}
              hoáº·c {'success': false, 'error': str}
    """
    try:
        logger.debug(f"[MVD] Báº¯t Ä‘áº§u select_qr_roi vá»›i video_path: {video_path}, camera_id: {camera_id}, roi_frame_path: {roi_frame_path}, step: {step}")

        # Kiá»ƒm tra sá»± tá»“n táº¡i cá»§a cÃ¡c mÃ´ hÃ¬nh
        for model_file in [DETECT_PROTO, DETECT_MODEL, SR_PROTO, SR_MODEL]:
            logger.debug(f"[MVD] Kiá»ƒm tra file mÃ´ hÃ¬nh: {model_file}")
            if not os.path.exists(model_file):
                logger.error(f"[MVD] File mÃ´ hÃ¬nh khÃ´ng tÃ¬m tháº¥y: {model_file}")
                cv2.destroyAllWindows()
                return {"success": False, "error": f"File mÃ´ hÃ¬nh khÃ´ng tÃ¬m tháº¥y: {model_file}"}

        # Kiá»ƒm tra file áº£nh vÃ  video
        logger.debug(f"[MVD] Kiá»ƒm tra áº£nh táº¡m: {roi_frame_path}")
        if not os.path.exists(roi_frame_path):
            logger.error(f"[MVD] áº¢nh táº¡m khÃ´ng tá»“n táº¡i: {roi_frame_path}")
            cv2.destroyAllWindows()
            return {"success": False, "error": f"áº¢nh táº¡m khÃ´ng tá»“n táº¡i: {roi_frame_path}"}
        
        logger.debug(f"[MVD] Kiá»ƒm tra video: {video_path}")
        if not os.path.exists(video_path):
            logger.error(f"[MVD] Video khÃ´ng tá»“n táº¡i: {video_path}")
            cv2.destroyAllWindows()
            return {"success": False, "error": f"Video khÃ´ng tá»“n táº¡i: {video_path}"}

        # Äá»c frame tá»« áº£nh táº¡m
        try:
            logger.debug(f"[MVD] Äá»c áº£nh táº¡m: {roi_frame_path}")
            frame = cv2.imread(roi_frame_path)
            if frame is None:
                logger.error(f"[MVD] KhÃ´ng thá»ƒ Ä‘á»c áº£nh táº¡m: {roi_frame_path}")
                cv2.destroyAllWindows()
                return {"success": False, "error": f"KhÃ´ng thá»ƒ Ä‘á»c áº£nh táº¡m: {roi_frame_path}"}
            logger.debug(f"[MVD] KÃ­ch thÆ°á»›c áº£nh táº¡m: {frame.shape[:2]}")
        except Exception as e:
            logger.error(f"[MVD] OpenCV imread error: {str(e)}\n{traceback.format_exc()}")
            cv2.destroyAllWindows()
            return {"success": False, "error": f"OpenCV imread error: {str(e)}"}

        # Chá»n loáº¡i bÃ n Ä‘Ã³ng gÃ³i
        table_type = None
        while table_type is None:
            current_frame = frame.copy()
            window_title = "**** Nhan 1 cho ban tieu chuan, 2 cho ban khong tieu chuan, q thoat ****"
            cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)
            cv2.imshow(window_title, current_frame)
            key = cv2.waitKey(0) & 0xFF
            cv2.destroyAllWindows()
            if key == ord('1'):
                table_type = "standard"
                logger.debug("[MVD] Chá»n bÃ n tiÃªu chuáº©n")
            elif key == ord('2'):
                table_type = "non_standard"
                logger.debug("[MVD] Chá»n bÃ n khÃ´ng tiÃªu chuáº©n")
            elif key == ord('q'):
                logger.debug("[MVD] NgÆ°á»i dÃ¹ng thoÃ¡t")
                cv2.destroyAllWindows()
                return {"success": False, "error": "NgÆ°á»i dÃ¹ng thoÃ¡t"}
            else:
                logger.debug("[MVD] PhÃ­m khÃ´ng há»£p lá»‡, hiá»ƒn thá»‹ láº¡i hÆ°á»›ng dáº«n")
                continue

        while True:
            # Táº¡o báº£n sao má»›i cá»§a frame má»—i láº§n váº½ láº¡i
            current_frame = frame.copy()
            rois = []

            # Váº½ ROI 1 (mÃ£ QR)
            window_title = "**** Keo chuot ve vung ma QR. Enter xac nhan, Esc huy ****"
            try:
                logger.debug("[MVD] Gá»i cv2.selectROI cho MVD ROI 1")
                cv2.destroyAllWindows()
                cv2.startWindowThread()
                cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)
                roi1 = cv2.selectROI(window_title, current_frame, showCrosshair=True, fromCenter=False)
                cv2.destroyAllWindows()
                x1, y1, w1, h1 = map(int, roi1)
                if w1 > 0 and h1 > 0:
                    rois.append({"x": x1, "y": y1, "w": w1, "h": h1, "type": "mvd"})
                    cv2.rectangle(current_frame, (x1, y1), (x1 + w1, y1 + h1), (0, 0, 255), 2)  # MÃ u Ä‘á» cho MVD
                    cv2.putText(current_frame, "ShippingLabel", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                    logger.debug(f"[MVD] ÄÃ£ chá»n MVD ROI 1: x={x1}, y={y1}, w={w1}, h={h1}")
                    cv2.namedWindow("**** Da ve vung ma QR ****", cv2.WINDOW_NORMAL)
                    cv2.imshow("**** Da ve vung ma QR ****", current_frame)
                    cv2.waitKey(500)
                    cv2.destroyAllWindows()
                else:
                    logger.debug("[MVD] ROI 1 khÃ´ng há»£p lá»‡")
                    cv2.namedWindow("**** Loi: ROI khong hop le. Ve lai vung ma QR ****", cv2.WINDOW_NORMAL)
                    cv2.imshow("**** Loi: ROI khong hop le. Ve lai vung ma QR ****", current_frame)
                    cv2.waitKey(2000)
                    cv2.destroyAllWindows()
                    continue

                # Chá»‰ váº½ ROI 2 (trigger) cho bÃ n tiÃªu chuáº©n
                if table_type == "standard":
                    window_title = "**** Ve vung ma trigger (QR: TimeGo). Enter xac nhan, Esc huy ****"
                    roi2_label = "Trigger"
                    logger.debug("[MVD] Gá»i cv2.selectROI cho ROI 2")
                    cv2.destroyAllWindows()
                    cv2.startWindowThread()
                    cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)
                    roi2 = cv2.selectROI(window_title, current_frame, showCrosshair=True, fromCenter=False)
                    cv2.destroyAllWindows()
                    x2, y2, w2, h2 = map(int, roi2)
                    if w2 > 0 and h2 > 0:
                        roi_type = "trigger"
                        rois.append({"x": x2, "y": y2, "w": w2, "h": h2, "type": roi_type})
                        cv2.rectangle(current_frame, (x2, y2), (x2 + w2, y2 + h2), (0, 0, 255), 2)  # MÃ u Ä‘á»
                        cv2.putText(current_frame, roi2_label, (x2, y2 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                        logger.debug(f"[MVD] ÄÃ£ chá»n ROI 2: x={x2}, y={y2}, w={w2}, h={h2}, type={roi_type}")
            except Exception as e:
                logger.error(f"[MVD] OpenCV selectROI error: {str(e)}\n{traceback.format_exc()}")
                cv2.destroyAllWindows()
                return {"success": False, "error": f"OpenCV selectROI error: {str(e)}"}

            # LÆ°u áº£nh vÃ o CameraROI náº¿u cÃ³ ROI há»£p lá»‡
            if rois:
                roi_frame_path_new = os.path.join(CAMERA_ROI_DIR, f"camera_{camera_id}_roi_MVD.jpg")
                try:
                    logger.debug(f"[MVD] LÆ°u áº£nh vá»›i ROI vÃ o: {roi_frame_path_new}")
                    ret = cv2.imwrite(roi_frame_path_new, current_frame)
                    if not ret:
                        logger.error(f"[MVD] KhÃ´ng thá»ƒ lÆ°u áº£nh táº¡i: {roi_frame_path_new}")
                        cv2.destroyAllWindows()
                        return {"success": False, "error": f"KhÃ´ng thá»ƒ lÆ°u áº£nh táº¡i {roi_frame_path_new}"}
                    logger.info(f"[MVD] ÄÃ£ lÆ°u frame vá»›i ROI vÃ o: {roi_frame_path_new}")
                except Exception as e:
                    logger.error(f"[MVD] OpenCV imwrite error: {str(e)}\n{traceback.format_exc()}")
                    cv2.destroyAllWindows()
                    return {"success": False, "error": f"OpenCV imwrite error: {str(e)}"}
                break
            else:
                logger.debug("[MVD] KhÃ´ng chá»n Ä‘Æ°á»£c ROI há»£p lá»‡, hiá»ƒn thá»‹ láº¡i áº£nh trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ váº½ láº¡i.")
                cv2.namedWindow("**** Loi: ROI khong hop le. Ve lai vung ma QR ****", cv2.WINDOW_NORMAL)
                cv2.imshow("**** Loi: ROI khong hop le. Ve lai vung ma QR ****", current_frame)
                cv2.waitKey(2000)
                cv2.destroyAllWindows()
                continue

        # Kiá»ƒm tra tÃ­nh tÆ°Æ¡ng thÃ­ch cá»§a áº£nh packing vá»›i MVD
        logger.debug(f"[MVD] Kiá»ƒm tra tÃ­nh tÆ°Æ¡ng thÃ­ch cá»§a áº£nh packing vá»›i MVD: {roi_frame_path}")

        # Khá»Ÿi táº¡o danh sÃ¡ch hÃ ng Ä‘á»£i vÃ  cá» thoÃ¡t
        frame_queues = [queue.Queue(maxsize=50) for _ in range(len(rois))]
        exit_flag = threading.Event()
        qr_detected = False
        qr_detected_roi1 = False
        qr_detected_roi2 = False
        qr_content = ""
        trigger_detected = False

        def process_roi(video_file, roi_index, x, y, w, h, interval=5):
            nonlocal qr_detected, qr_detected_roi1, qr_detected_roi2, qr_content, trigger_detected
            try:
                logger.debug(f"[MVD] Khá»Ÿi táº¡o WeChatQRCode cho ROI {roi_index + 1}")
                local_detector = cv2.wechat_qrcode_WeChatQRCode(DETECT_PROTO, DETECT_MODEL, SR_PROTO, SR_MODEL)
                logger.debug(f"[MVD] WeChatQRCode khá»Ÿi táº¡o thÃ nh cÃ´ng cho ROI {roi_index + 1}")
            except Exception as e:
                logger.error(f"[MVD] OpenCV WeChatQRCode error in ROI {roi_index + 1}: {str(e)}\n{traceback.format_exc()}")
                return

            try:
                logger.debug(f"[MVD] Má»Ÿ video cho ROI {roi_index + 1}: {video_file}")
                cap = cv2.VideoCapture(video_file)
                if not cap.isOpened():
                    logger.error(f"[MVD] KhÃ´ng thá»ƒ má»Ÿ video '{video_file}' cho ROI {roi_index + 1}")
                    return
                logger.debug(f"[MVD] Video má»Ÿ thÃ nh cÃ´ng cho ROI {roi_index + 1}")
            except Exception as e:
                logger.error(f"[MVD] OpenCV VideoCapture error in ROI {roi_index + 1}: {str(e)}\n{traceback.format_exc()}")
                return

            frame_count = 0
            start_time = time.time()

            while not exit_flag.is_set():
                try:
                    ret, frame = cap.read()
                    if not ret:
                        logger.debug(f"[MVD] Káº¿t thÃºc video '{video_file}' (ROI {roi_index + 1})")
                        break

                    frame_count += 1
                    if frame_count % interval != 0:
                        continue

                    logger.debug(f"[MVD] Xá»­ lÃ½ frame {frame_count} cho ROI {roi_index + 1}")
                    roi_frame = frame[y:y+h, x:x+w]
                    if roi_frame.size == 0 or roi_frame.shape[0] == 0 or roi_frame.shape[1] == 0:
                        logger.warning(f"[MVD] ROI {roi_index + 1} khÃ´ng há»£p lá»‡, bá» qua frame")
                        continue

                    if len(roi_frame.shape) == 2:
                        roi_frame = cv2.cvtColor(roi_frame, cv2.COLOR_GRAY2BGR)

                    logger.debug(f"[MVD] PhÃ¡t hiá»‡n QR trong ROI {roi_index + 1}")
                    texts, points = local_detector.detectAndDecode(roi_frame)
                    if texts:
                        qr_detected = True
                        if roi_index == 0:
                            qr_detected_roi1 = True
                        elif roi_index == 1 and table_type == "standard":
                            qr_detected_roi2 = True
                        qr_content = texts[0]  # LÆ°u ná»™i dung QR Ä‘áº§u tiÃªn
                        # Kiá»ƒm tra trigger cho ROI 2 (bÃ n tiÃªu chuáº©n)
                        if table_type == "standard" and roi_index == 1 and texts[0].lower() == "timego":
                            trigger_detected = True
                            logger.info(f"[MVD] [ROI {roi_index + 1}] PhÃ¡t hiá»‡n trigger: {texts[0]}")
                        for text, box in zip(texts, points):
                            logger.info(f"[MVD] [ROI {roi_index + 1}] Ná»™i dung mÃ£ QR: {text}")
                            # Váº½ khung viá»n QR
                            for i in range(4):
                                pt1 = tuple(map(int, box[i]))
                                pt2 = tuple(map(int, box[(i + 1) % 4]))
                                cv2.line(roi_frame, pt1, pt2, (0, 255, 0), 2)
                            # Hiá»ƒn thá»‹ ná»™i dung QR dÆ°á»›i khung viá»n
                            bottom_left = tuple(map(int, box[2]))  # GÃ³c dÆ°á»›i trÃ¡i
                            cv2.putText(roi_frame, text[:20], (bottom_left[0], bottom_left[1] + 30), 
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

                    elapsed_time = time.time() - start_time
                    elapsed_time_text = f"Time: {elapsed_time:.1f}"
                    cv2.putText(roi_frame, elapsed_time_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
                    cv2.putText(roi_frame, "Dang phat hien ma QR. Noi dung hien thi neu tim thay", 
                                (10, roi_frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                    frame_queues[roi_index].put(roi_frame)
                    logger.debug(f"[MVD] ÄÃ£ Ä‘áº©y frame cho ROI {roi_index + 1} vÃ o hÃ ng Ä‘á»£i")
                except Exception as e:
                    logger.error(f"[MVD] OpenCV processing error in process_roi (ROI {roi_index + 1}): {str(e)}\n{traceback.format_exc()}")
                    break

            logger.debug(f"[MVD] Giáº£i phÃ³ng video capture cho ROI {roi_index + 1}")
            cap.release()

        # Khá»Ÿi cháº¡y luá»“ng xá»­ lÃ½ video cho tá»«ng ROI
        threads = []
        for i, roi in enumerate(rois):
            if roi["w"] > 0 and roi["h"] > 0:
                logger.debug(f"[MVD] Khá»Ÿi cháº¡y thread cho ROI {i + 1}")
                thread = threading.Thread(target=process_roi, args=(video_path, i, roi["x"], roi["y"], roi["w"], roi["h"], 5))
                thread.start()
                threads.append(thread)
                logger.info(f"[MVD] Thread cho ROI {i + 1} Ä‘Ã£ khá»Ÿi cháº¡y")
            else:
                logger.warning(f"[MVD] ROI {i + 1} khÃ´ng há»£p lá»‡, bá» qua")

        # Khá»Ÿi táº¡o cá»­a sá»• cho tá»«ng ROI
        for i in range(len(frame_queues)):
            try:
                logger.debug(f"[MVD] Khá»Ÿi táº¡o cá»­a sá»• cho ROI {i + 1}")
                window_title = f"**** Dang phat hien ma QR. Noi dung hien thi neu tim thay (ROI {i + 1}) ****"
                cv2.namedWindow(window_title, cv2.WINDOW_AUTOSIZE)
                logger.debug(f"[MVD] Cá»­a sá»• cho ROI {i + 1} Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o")
            except Exception as e:
                logger.error(f"[MVD] OpenCV namedWindow error: {str(e)}\n{traceback.format_exc()}")
                cv2.destroyAllWindows()
                return {"success": False, "error": f"OpenCV namedWindow error: {str(e)}"}

        # Hiá»ƒn thá»‹ má»—i ROI trong cá»­a sá»• riÃªng
        while any(thread.is_alive() for thread in threads) or any(not q.empty() for q in frame_queues):
            for i in range(len(frame_queues)):
                try:
                    frame = frame_queues[i].get(timeout=0.1)
                    window_name = f"**** Dang phat hien ma QR. Noi dung hien thi neu tim thay (ROI {i + 1}) ****"
                    cv2.imshow(window_name, frame)
                    logger.debug(f"[MVD] Hiá»ƒn thá»‹ frame cho {window_name}")
                except queue.Empty:
                    pass
                except Exception as e:
                    logger.error(f"[MVD] OpenCV imshow error in loop: {str(e)}\n{traceback.format_exc()}")
                    cv2.destroyAllWindows()
                    return {"success": False, "error": f"OpenCV imshow error: {str(e)}"}

            try:
                if cv2.waitKey(10) & 0xFF == ord('q'):
                    logger.debug("[MVD] Nháº­n lá»‡nh thoÃ¡t tá»« ngÆ°á»i dÃ¹ng")
                    exit_flag.set()
                    break
            except Exception as e:
                logger.error(f"[MVD] OpenCV waitKey error: {str(e)}\n{traceback.format_exc()}")
                cv2.destroyAllWindows()
                return {"success": False, "error": f"OpenCV waitKey error: {str(e)}"}

        logger.debug("[MVD] ÄÃ³ng táº¥t cáº£ cá»­a sá»• OpenCV")
        cv2.destroyAllWindows()
        for thread in threads:
            logger.debug(f"[MVD] Chá» thread ROI {threads.index(thread) + 1} káº¿t thÃºc")
            thread.join()

        # LÆ°u káº¿t quáº£ vÃ o /tmp/qr_roi.json
        result = {
            "success": True,
            "rois": rois,
            "roi_frame": os.path.relpath(roi_frame_path_new, BASE_DIR),
            "qr_detected": qr_detected,
            "qr_detected_roi1": qr_detected_roi1,
            "qr_detected_roi2": qr_detected_roi2 if table_type == "standard" else False,
            "qr_content": qr_content,
            "trigger_detected": trigger_detected,
            "table_type": table_type
        }
        logger.debug(f"[MVD] LÆ°u káº¿t quáº£ vÃ o /tmp/qr_roi.json: {result}")
        try:
            with open("/tmp/qr_roi.json", "w") as f:
                json.dump(result, f)
            logger.info("[MVD] ÄÃ£ lÆ°u káº¿t quáº£ vÃ o /tmp/qr_roi.json")
        except Exception as e:
            logger.error(f"[MVD] Lá»—i khi lÆ°u /tmp/qr_roi.json: {str(e)}\n{traceback.format_exc()}")
            cv2.destroyAllWindows()
            return {"success": False, "error": f"Lá»—i khi lÆ°u /tmp/qr_roi.json: {str(e)}"}

        logger.info(f"[MVD] HoÃ n táº¥t select_qr_roi cho camera_id: {camera_id}, step: {step}")
        cv2.destroyAllWindows()
        return result

    except Exception as e:
        logger.error(f"[MVD] Lá»—i trong select_qr_roi: {str(e)}\n{traceback.format_exc()}")
        cv2.destroyAllWindows()
        return {"success": False, "error": f"Lá»—i há»‡ thá»‘ng: {str(e)}"}

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 4:
        logger.error("Usage: python3 qr_detector.py <video_path> <camera_id> <roi_frame_path>")
        sys.exit(1)

    video_path = sys.argv[1]
    camera_id = sys.argv[2]
    roi_frame_path = sys.argv[3]
    try:
        result = select_qr_roi(video_path, camera_id, roi_frame_path, step="mvd")
        if not result["success"]:
            logger.error(result["error"])
    except Exception as e:
        logger.error(f"[MVD] Lá»—i khi cháº¡y script: {str(e)}\n{traceback.format_exc()}")
        cv2.destroyAllWindows()

```
## ğŸ“„ File: `roi_preview.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/roi_preview.py`

```python
import cv2
import argparse
import os
import logging

def setup_logging():
    log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "resources", "output_clips", "LOG")
    os.makedirs(log_dir, exist_ok=True)
    log_file_path = os.path.join(log_dir, f"roi_preview_{os.getpid()}.log")
    logging.basicConfig(
        filename=log_file_path,
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    logging.info(f"ROI Preview started with PID {os.getpid()}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--video', required=True, help='File video .mp4')
    parser.add_argument('--roi', nargs=4, type=int, required=True, help='Tá»a Ä‘á»™ ROI: x y w h')
    args = parser.parse_args()

    setup_logging()
    x, y, w, h = args.roi
    cap = cv2.VideoCapture(args.video)
    if not cap.isOpened():
        logging.error(f"Cannot open {args.video}")
        return

    win = "ROI Preview"
    cv2.namedWindow(win, cv2.WINDOW_NORMAL)
    cv2.resizeWindow(win, w, h)

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        crop = frame[y:y+h, x:x+w]
        cv2.imshow(win, crop)
        if cv2.waitKey(30) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()
    logging.info("ROI Preview closed")

if __name__ == "__main__":
    main()
```
## ğŸ“„ File: `event_detector.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/event_detector.py`

```python
from flask import Blueprint, jsonify
import os
import sqlite3
import logging
from datetime import datetime
from modules.db_utils import get_db_connection
from modules.scheduler.db_sync import db_rwlock
from modules.config.logging_config import get_logger


# Äá»‹nh nghÄ©a BASE_DIR
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

event_detector_bp = Blueprint('event_detector', __name__)

def calculate_duration(ts, te):
    if ts is None or te is None:
        return None
    if te < ts:
        return None
    return te - ts

def process_single_log(log_file_path):
    if not os.path.isfile(log_file_path):
        logging.warning(f"Log file not found: {log_file_path}, skipping.")
        return
    # Khá»Ÿi táº¡o logger vá»›i context log_file
    logger = get_logger(__name__, {"log_file": log_file_path})
    logger.info("Logging initialized for process_single_log")

    try:
        with db_rwlock.gen_wlock():
            conn = get_db_connection()
            cursor = conn.cursor()

            # Kiá»ƒm tra tráº¡ng thÃ¡i is_processed cá»§a file log
            cursor.execute("SELECT is_processed FROM processed_logs WHERE log_file = ?", (log_file_path,))
            result = cursor.fetchone()
            if result and result[0] == 1:
                logging.info(f"Log file {log_file_path} already processed, skipping")
                conn.close()
                return

            with open(log_file_path, "r") as f:
                header = f.readline().strip()
                logging.info(f"Header: {header}")
                start_time = int(header.split("Start: ")[1].split(",")[0])
                end_time = int(header.split("End: ")[1].split(",")[0])
                start_time_str = header.split("Start_Time: ")[1].split(",")[0].strip()
                camera_name = header.split("Camera_Name: ")[1].split(",")[0].strip()
                video_path = header.split("Video_File: ")[1].split(",")[0].strip()
                start_time_dt = datetime.strptime(start_time_str, "%Y-%m-%d %H:%M:%S")
                logging.info(f"Parsed header - Start: {start_time}, End: {end_time}, Start_Time: {start_time_str}, Camera_Name: {camera_name}, Video_File: {video_path}")

                # Kiá»ƒm tra náº¿u file log rá»—ng
                first_data_line = f.readline().strip()
                if not first_data_line:
                    logging.info(f"Log file {log_file_path} is empty, skipping")
                    cursor.execute("UPDATE processed_logs SET is_processed = 1, processed_at = ? WHERE log_file = ?", (datetime.now(), log_file_path))
                    conn.commit()
                    conn.close()
                    return

                # Náº¿u cÃ³ dá»¯ liá»‡u, quay láº¡i vÃ  xá»­ lÃ½
                f.seek(0)
                next(f)
                frame_sampler_data = []
                for line in f:
                    parts = line.strip().split(",")
                    try:
                        second, state = parts[0], parts[1]
                        codes = [parts[2]] if len(parts) > 2 and parts[2] else []
                        frame_sampler_data.append({"second": float(second), "state": state, "tracking_codes": codes})
                    except Exception as e:
                        logging.info(f"Error parsing line '{line.strip()}': {str(e)}")

            # Láº¥y min_packing_time tá»« Processing_config
            cursor.execute("SELECT min_packing_time FROM Processing_config LIMIT 1")
            min_packing_time_row = cursor.fetchone()
            min_packing_time = min_packing_time_row[0] if min_packing_time_row else 5
            logging.info(f"Min packing time: {min_packing_time}")

            # Láº¥y pending_event má»›i nháº¥t theo ts
            cursor.execute("SELECT event_id, ts, tracking_codes, video_file FROM events WHERE te IS NULL AND camera_name = ? ORDER BY event_id DESC LIMIT 1", (camera_name,))
            pending_event = cursor.fetchone()
            logging.info(f"Pending event: {pending_event}")
            ts = pending_event[1] if pending_event else None
            pending_tracking_codes = eval(pending_event[2]) if pending_event and pending_event[2] else []
            pending_video_file = pending_event[3] if pending_event else None
            event_id = pending_event[0] if pending_event else None
            segments = []
            prev_state = None
            has_pending = ts is not None and ts <= start_time

            for data in frame_sampler_data:
                current_state = data["state"]
                current_second = data["second"]
                current_tracking_codes = data["tracking_codes"]

                if has_pending and ts is not None:
                    if current_state == "On":
                        te = current_second
                        total_duration = calculate_duration(ts, te)
                        
                        # TÃ¡ch tracking_codes thÃ nh cÃ¡c sá»± kiá»‡n liÃªn tiáº¿p
                        all_tracking_codes = list(set(pending_tracking_codes + current_tracking_codes))
                        num_codes = len(all_tracking_codes) if all_tracking_codes else 1
                        duration_per_event = max(round(total_duration / num_codes), min_packing_time)  # LÃ m trÃ²n vÃ  Ä‘áº£m báº£o >= min_packing_time
                        total_duration = duration_per_event * num_codes  # Cáº­p nháº­t total_duration
                        te = ts + total_duration if ts is not None else te
                        logging.info(f"Äiá»u chá»‰nh pending event: Ts={ts}, Te={te}, Duration má»—i sá»± kiá»‡n thÃ nh {duration_per_event}")
                        
                        if pending_video_file == video_path:
                            current_ts = ts
                            for i, code in enumerate(all_tracking_codes):
                                current_te = current_ts + duration_per_event if current_ts is not None else te
                                if i == 0:
                                    # Cáº­p nháº­t pending event cho mÃ£ Ä‘áº§u tiÃªn
                                    segments.append((current_ts, current_te, duration_per_event, [code], video_path, event_id))
                                    logging.info(f"Cáº­p nháº­t pending event liÃªn tiáº¿p {i+1}/{num_codes}: Ts={current_ts}, Te={current_te}, Duration={duration_per_event}, Tracking_code={code}")
                                else:
                                    # ThÃªm sá»± kiá»‡n má»›i cho cÃ¡c mÃ£ tiáº¿p theo
                                    segments.append((current_ts, current_te, duration_per_event, [code], video_path, None))
                                    logging.info(f"ThÃªm sá»± kiá»‡n liÃªn tiáº¿p má»›i {i+1}/{num_codes}: Ts={current_ts}, Te={current_te}, Duration={duration_per_event}, Tracking_code={code}")
                                current_ts = current_te
                        else:
                            current_ts = None
                            for i, code in enumerate(all_tracking_codes):
                                current_te = te - (num_codes - i - 1) * duration_per_event
                                segments.append((current_ts, current_te, duration_per_event, [code], video_path, None))
                                logging.info(f"ThÃªm pending event liÃªn tiáº¿p {i+1}/{num_codes}: Ts={current_ts}, Te={current_te}, Duration={duration_per_event}, Tracking_code={code}")
                                current_ts = current_te
                        
                        ts = None
                        has_pending = False
                    elif current_state == "Off":
                        pending_tracking_codes.extend([code for code in current_tracking_codes if code and code not in pending_tracking_codes])
                        # Kiá»ƒm tra vÃ  xÃ³a sá»± kiá»‡n dá»Ÿ dang náº¿u khÃ´ng cÃ³ tracking_codes
                        if not pending_tracking_codes and not current_tracking_codes:
                            cursor.execute("DELETE FROM events WHERE te IS NULL AND event_id = (SELECT MAX(event_id) FROM events WHERE te IS NULL AND camera_name = ?)", (camera_name,))
                            logging.info(f"XÃ³a sá»± kiá»‡n dá»Ÿ dang cuá»‘i cÃ¹ng cá»§a camera {camera_name} do khÃ´ng cÃ³ tracking_codes")              
                elif not has_pending:
                    if prev_state == "On" and current_state == "Off":
                        ts = current_second
                        pending_tracking_codes = current_tracking_codes[:]
                        logging.info(f"PhÃ¡t hiá»‡n sá»± kiá»‡n Ts táº¡i giÃ¢y {current_second}")

                    elif prev_state == "Off" and current_state == "On" and ts is not None:
                        te = current_second
                        total_duration = calculate_duration(ts, te)
                        
                        # TÃ¡ch tracking_codes thÃ nh cÃ¡c sá»± kiá»‡n liÃªn tiáº¿p
                        all_tracking_codes = list(set(pending_tracking_codes + current_tracking_codes))  # Loáº¡i bá» trÃ¹ng láº·p
                        num_codes = len(all_tracking_codes) if all_tracking_codes else 1
                        duration_per_event = max(round(total_duration / num_codes), min_packing_time)  # LÃ m trÃ²n vÃ  Ä‘áº£m báº£o >= min_packing_time
                        total_duration = duration_per_event * num_codes  # Cáº­p nháº­t total_duration
                        te = ts + total_duration if ts is not None else te
                        logging.info(f"Äiá»u chá»‰nh sá»± kiá»‡n: Ts={ts}, Te={te}, Duration má»—i sá»± kiá»‡n thÃ nh {duration_per_event}")
                        
                        if all_tracking_codes:
                            current_ts = ts
                            for i, code in enumerate(all_tracking_codes):
                                current_te = current_ts + duration_per_event if current_ts is not None else te
                                segments.append((current_ts, current_te, duration_per_event, [code], video_path, None))
                                logging.info(f"ThÃªm sá»± kiá»‡n liÃªn tiáº¿p {i+1}/{num_codes}: Ts={current_ts}, Te={current_te}, Duration={duration_per_event}, Tracking_code={code}")
                                current_ts = current_te
                        else:
                            segments.append((ts, te, duration_per_event, [], video_path, None))
                            logging.info(f"ThÃªm sá»± kiá»‡n khÃ´ng cÃ³ tracking_code: Ts={ts}, Te={te}, Duration={duration_per_event}")
                        
                        ts = None
                        pending_tracking_codes = []

                    elif ts is not None and current_state == "Off":
                        pending_tracking_codes.extend([code for code in current_tracking_codes if code and code not in pending_tracking_codes])

                prev_state = current_state

            if ts is not None:
                segments.append((ts, None, None, pending_tracking_codes, video_path, None))
                logging.info(f"GiÃ¢y {frame_sampler_data[-1]['second']}: Ts={ts}, Te=Not finished")

            logging.info(f"All segments detected: {segments}")

            for segment in segments:
                ts, te, duration, tracking_codes, segment_video_path, segment_event_id = segment
                if te is not None and not tracking_codes:
                    logging.info(f"Bá» qua sá»± kiá»‡n hoÃ n chá»‰nh do tracking_codes rá»—ng: ts={ts}, te={te}")
                    continue
                packing_time_start = int((start_time_dt.timestamp() + ts) * 1000) if ts is not None else None
                packing_time_end = int((start_time_dt.timestamp() + te) * 1000) if te is not None else None
                if segment_event_id is not None:
                    cursor.execute("UPDATE events SET te=?, duration=?, tracking_codes=?, packing_time_end=? WHERE event_id=?",
                                   (te, duration, str(tracking_codes), packing_time_end, segment_event_id))
                    logging.info(f"Updated event_id {segment_event_id}: ts={ts}, te={te}, duration={duration}")
                else:
                    cursor.execute('''INSERT INTO events (ts, te, duration, tracking_codes, video_file, buffer, camera_name, packing_time_start, packing_time_end)
                                      VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                                   (ts, te, duration, str(tracking_codes), segment_video_path, 0, camera_name, packing_time_start, packing_time_end))
                    logging.info(f"Inserted new event: ts={ts}, te={te}, duration={duration}")

            cursor.execute("UPDATE processed_logs SET is_processed = 1, processed_at = ? WHERE log_file = ?", (datetime.now(), log_file_path))
            conn.commit()
            logging.info("Database changes committed")

    except Exception as e:
        logging.error(f"Error in process_single_log: {str(e)}")
        raise
    finally:
        conn.close()

@event_detector_bp.route('/process-events', methods=['GET'])
def process_events():
    try:
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                SELECT DISTINCT log_file_path, (
                    SELECT CAST(SUBSTR(header, INSTR(header, 'Start: ') + 7, INSTR(SUBSTR(header, INSTR(header, 'Start: ') + 7), ',') - 1) AS INTEGER)
                    FROM (
                        SELECT SUBSTR(CAST(READFILE(log_file_path) AS TEXT), 1, INSTR(CAST(READFILE(log_file_path) AS TEXT), '\n') - 1) AS header
                    )
                ) AS start_time
                FROM file_list 
                WHERE is_processed = 1 AND log_file_path IS NOT NULL 
                AND log_file_path IN (SELECT log_file FROM processed_logs WHERE is_processed = 0)
                ORDER BY start_time
            """)
            log_files = [row[0] for row in cursor.fetchall()]
            logging.info(f"Log files to process: {log_files}")
            conn.close()

        for log_file in log_files:
            if not os.path.isfile(log_file):
                logging.warning(f"Log file not found, skipping: {log_file}")
                continue
            if os.path.exists(log_file):
                logging.info(f"Starting to process file: {log_file}")
                process_single_log(log_file)
                logging.info(f"Finished processing file: {log_file}")
            else:
                logging.info(f"File not found: {log_file}")

        return jsonify({"message": "Event detection completed successfully"}), 200
    except Exception as e:
        logging.error(f"Error in process_events: {str(e)}")
        return jsonify({"error": str(e)}), 500

```
## ğŸ“„ File: `__init__.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/__init__.py`

```python

```
## ğŸ“„ File: `hand_detection.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/hand_detection.py`

```python
import cv2
import mediapipe as mp
import time
import logging
import json
import os
import glob
from datetime import datetime
from modules.config.logging_config import get_logger


# Äá»‹nh nghÄ©a BASE_DIR
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# Khá»Ÿi táº¡o logger mÃ  khÃ´ng sá»­ dá»¥ng video_path
logger = get_logger(__name__, {"module": "hand_detection"})
logger.info("Logging initialized")

mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

# Äáº·t bÆ°á»›c nháº£y frame
FRAME_STEP = 5

# ÄÆ°á»ng dáº«n lÆ°u áº£nh
CAMERA_ROI_DIR = os.path.join(BASE_DIR, "resources", "output_clips", "CameraROI")

def ensure_directory_exists(directory):
    """Äáº£m báº£o thÆ° má»¥c tá»“n táº¡i, náº¿u khÃ´ng thÃ¬ táº¡o má»›i."""
    try:
        if not os.path.exists(directory):
            os.makedirs(directory)
            logging.debug(f"ÄÃ£ táº¡o thÆ° má»¥c: {directory}")
        # Kiá»ƒm tra quyá»n truy cáº­p
        if not os.access(directory, os.W_OK):
            logging.error(f"KhÃ´ng cÃ³ quyá»n ghi vÃ o thÆ° má»¥c {directory}")
            raise PermissionError(f"KhÃ´ng cÃ³ quyá»n ghi vÃ o thÆ° má»¥c {directory}")
    except Exception as e:
        logging.error(f"Lá»—i khi táº¡o thÆ° má»¥c {directory}: {str(e)}")
        raise

def select_roi(video_path, camera_id, step="packing"):
    """
    Má»Ÿ video vÃ  cho phÃ©p ngÆ°á»i dÃ¹ng váº½ ROI báº±ng OpenCV, lÆ°u káº¿t quáº£ vÃ o CameraROI, sau Ä‘Ã³ phÃ¡t hiá»‡n tay.
    Args:
        video_path (str): ÄÆ°á»ng dáº«n Ä‘áº¿n file video.
        camera_id (str): ID cá»§a camera.
        step (str): Giai Ä‘oáº¡n hiá»‡n táº¡i (packing, trigger).
    Returns:
        dict: {'success': bool, 'roi': {'x': int, 'y': int, 'w': int, 'h': int}, 'roi_frame': str, 'hand_detected': bool} hoáº·c {'success': false, 'error': str}
    """
    try:
        logging.debug(f"Báº¯t Ä‘áº§u select_roi vá»›i video_path: {video_path}, camera_id: {camera_id}, step: {step}")
        
        # Äáº£m báº£o thÆ° má»¥c CameraROI tá»“n táº¡i
        ensure_directory_exists(CAMERA_ROI_DIR)

        # Má»Ÿ video
        logging.debug("Äang má»Ÿ video...")
        cap = cv2.VideoCapture(video_path)
        try:
            if not cap.isOpened():
                logging.error("KhÃ´ng thá»ƒ má»Ÿ video.")
                return {"success": False, "error": "KhÃ´ng thá»ƒ má»Ÿ video."}
            
            # Äá»c frame Ä‘áº§u tiÃªn
            logging.debug("Äang Ä‘á»c frame Ä‘áº§u tiÃªn...")
            ret, frame = cap.read()
            if not ret:
                logging.error("KhÃ´ng thá»ƒ Ä‘á»c frame tá»« video.")
                return {"success": False, "error": "KhÃ´ng thá»ƒ Ä‘á»c frame tá»« video."}
            
            # LÆ°u frame gá»‘c náº¿u á»Ÿ bÆ°á»›c packing
            if step == "packing":
                original_frame_path = os.path.join(CAMERA_ROI_DIR, f"camera_{camera_id}_original.jpg")
                ret = cv2.imwrite(original_frame_path, frame)
                if not ret:
                    logging.error(f"KhÃ´ng thá»ƒ lÆ°u áº£nh gá»‘c táº¡i: {original_frame_path}")
                    return {"success": False, "error": f"KhÃ´ng thá»ƒ lÆ°u áº£nh gá»‘c táº¡i {original_frame_path}"}
                logging.debug(f"ÄÃ£ lÆ°u frame gá»‘c vÃ o: {original_frame_path}")
            
            while True:
                # Hiá»ƒn thá»‹ giao diá»‡n chá»n ROI
                logging.debug("Gá»i cv2.selectROI...")
                current_frame = frame.copy()
                roi = cv2.selectROI(f"Click va keo chuot de chon -Vung {step.capitalize()}-", current_frame, showCrosshair=True, fromCenter=False)
                logging.debug(f"ROI tráº£ vá»: {roi}")
                cv2.destroyAllWindows()
                
                # Kiá»ƒm tra náº¿u ROI há»£p lá»‡
                x, y, w, h = map(int, roi)
                if w == 0 or h == 0:
                    logging.debug("ROI khÃ´ng há»£p lá»‡, hiá»ƒn thá»‹ láº¡i frame gá»‘c Ä‘á»ƒ váº½ láº¡i.")
                    continue  # Hiá»ƒn thá»‹ láº¡i frame gá»‘c, khÃ´ng lÆ°u file
                
                # Váº½ ROI lÃªn frame
                color = (0, 255, 0) if step == "packing" else (0, 255, 255)
                cv2.rectangle(current_frame, (x, y), (x + w, y + h), color, 2)
                # ThÃªm tiÃªu Ä‘á» "Packing" náº¿u á»Ÿ bÆ°á»›c packing
                if step == "packing":
                    cv2.putText(current_frame, "Packing", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                # Hiá»ƒn thá»‹ frame vá»›i ROI vÃ  tiÃªu Ä‘á»
                cv2.namedWindow("**** Da ve vung Packing ****", cv2.WINDOW_NORMAL)
                cv2.imshow("**** Da ve vung Packing ****", current_frame)
                cv2.waitKey(500)
                cv2.destroyAllWindows()
                
                # LÆ°u frame vá»›i ROI vÃ o CameraROI
                if step == "packing":
                    roi_frame_path = os.path.join(CAMERA_ROI_DIR, f"camera_{camera_id}_roi_packing.jpg")
                else:  # step == "trigger"
                    roi_frame_path = os.path.join(CAMERA_ROI_DIR, f"camera_{camera_id}_roi_trigger.jpg")
                
                ret = cv2.imwrite(roi_frame_path, current_frame)
                if not ret:
                    logging.error(f"KhÃ´ng thá»ƒ lÆ°u áº£nh táº¡i: {roi_frame_path}")
                    return {"success": False, "error": f"KhÃ´ng thá»ƒ lÆ°u áº£nh táº¡i {roi_frame_path}"}
                logging.debug(f"ÄÃ£ lÆ°u frame vá»›i ROI vÃ o: {roi_frame_path}")
                
                # Kiá»ƒm tra file Ä‘Ã£ Ä‘Æ°á»£c lÆ°u thÃ nh cÃ´ng
                if not os.path.exists(roi_frame_path):
                    logging.error(f"File khÃ´ng tá»“n táº¡i sau khi lÆ°u: {roi_frame_path}")
                    return {"success": False, "error": f"File khÃ´ng tá»“n táº¡i sau khi lÆ°u: {roi_frame_path}"}
                
                # Náº¿u lÃ  bÆ°á»›c packing, gá»i detect_hands Ä‘á»ƒ kiá»ƒm tra tay
                hand_detected = False
                if step == "packing":
                    detect_result = detect_hands(video_path, {"x": x, "y": y, "w": w, "h": h})
                    if not detect_result["success"]:
                        logging.error(f"Lá»—i khi phÃ¡t hiá»‡n tay: {detect_result['error']}")
                        return {"success": False, "error": detect_result["error"]}
                    hand_detected = detect_result["hand_detected"]
                
                # LÆ°u tá»a Ä‘á»™ ROI vÃ  tráº¡ng thÃ¡i hand_detected vÃ o /tmp/roi.json
                result = {
                    "success": True,
                    "roi": {"x": x, "y": y, "w": w, "h": h},
                    "roi_frame": os.path.relpath(roi_frame_path, BASE_DIR),
                    "hand_detected": hand_detected
                }
                logging.debug(f"LÆ°u ROI vÃ o /tmp/roi.json: {result}")
                with open("/tmp/roi.json", "w") as f:
                    json.dump(result, f)
                
                logging.debug(f"ROI há»£p lá»‡: x={x}, y={y}, w={w}, h={h}, hand_detected: {hand_detected}")
                return result
            
        finally:
            cap.release()
            logging.debug("ÄÃ£ giáº£i phÃ³ng tÃ i nguyÃªn video (cap.release).")
        
    except Exception as e:
        logging.error(f"Lá»—i trong select_roi: {str(e)}")
        cv2.destroyAllWindows()
        return {"success": False, "error": f"Lá»—i há»‡ thá»‘ng: {str(e)}"}

def detect_hands(video_path, roi):
    """
    Hiá»ƒn thá»‹ video vá»›i phÃ¡t hiá»‡n tay trong vÃ¹ng ROI, tráº£ vá» tráº¡ng thÃ¡i phÃ¡t hiá»‡n tay.
    Args:
        video_path (str): ÄÆ°á»ng dáº«n Ä‘áº¿n file video.
        roi (dict): Tá»a Ä‘á»™ ROI {'x': int, 'y': int, 'w': int, 'h': int}.
    Returns:
        dict: {'success': bool, 'hand_detected': bool, 'error': str náº¿u cÃ³ lá»—i}
    """
    try:
        x, y, w, h = roi["x"], roi["y"], roi["w"], roi["h"]
        if w <= 0 or h <= 0:
            logging.error("ROI khÃ´ng há»£p lá»‡ (chiá»u rá»™ng hoáº·c chiá»u cao báº±ng 0).")
            return {"success": False, "hand_detected": False, "error": "ROI khÃ´ng há»£p lá»‡."}

        # Má»Ÿ video
        logging.debug("Äang má»Ÿ video Ä‘á»ƒ phÃ¡t hiá»‡n tay...")
        cap = cv2.VideoCapture(video_path)
        try:
            if not cap.isOpened():
                logging.error("KhÃ´ng thá»ƒ má»Ÿ video.")
                return {"success": False, "hand_detected": False, "error": "KhÃ´ng thá»ƒ má»Ÿ video."}

            hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)
            frame_count = 0
            start_time = time.time()
            hand_detected = False

            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                # Cáº¯t video theo ROI
                roi_frame = frame[y:y+h, x:x+w]

                # Chá»‰ xá»­ lÃ½ má»—i FRAME_STEP frame
                if frame_count % FRAME_STEP == 0:
                    # Chuyá»ƒn Ä‘á»•i BGR sang RGB
                    rgb_frame = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2RGB)

                    # PhÃ¡t hiá»‡n bÃ n tay
                    results = hands.process(rgb_frame)

                    # Kiá»ƒm tra vÃ  xÃ¡c nháº­n phÃ¡t hiá»‡n tay
                    if results.multi_hand_landmarks:
                        hand_detected = True
                        for hand_landmarks in results.multi_hand_landmarks:
                            # Váº½ keypoints ngay khi phÃ¡t hiá»‡n tay
                            mp_drawing.draw_landmarks(roi_frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

                # Hiá»ƒn thá»‹ video
                elapsed_time = time.time() - start_time
                cv2.putText(roi_frame, f"Time: {elapsed_time:.2f}s", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                cv2.imshow("ROI Hand Detection", roi_frame)

                if cv2.waitKey(1) == ord("q"):
                    break

                frame_count += 1

            logging.debug(f"PhÃ¡t hiá»‡n tay: {hand_detected}")
            return {"success": True, "hand_detected": hand_detected}
        
        finally:
            cap.release()
            cv2.destroyWindow("ROI Hand Detection")  # Chá»‰ Ä‘Ã³ng cá»­a sá»• cá»§a detect_hands
            logging.debug("ÄÃ£ giáº£i phÃ³ng tÃ i nguyÃªn video (cap.release) trong detect_hands.")
    
    except Exception as e:
        logging.error(f"Lá»—i trong detect_hands: {str(e)}")
        return {"success": False, "hand_detected": False, "error": f"Lá»—i há»‡ thá»‘ng: {str(e)}"}

def finalize_roi(video_path, camera_id, rois):
    """
    Váº½ táº¥t cáº£ cÃ¡c vÃ¹ng ROI (packing, MVD, trigger) lÃªn frame vÃ  lÆ°u vÃ o thÆ° má»¥c CameraROI.
    Args:
        video_path (str): ÄÆ°á»ng dáº«n Ä‘áº¿n file video.
        camera_id (str): ID cá»§a camera.
        rois (list): Danh sÃ¡ch cÃ¡c vÃ¹ng ROI [{'type': str, 'x': int, 'y': int, 'w': int, 'h': int}, ...].
    Returns:
        dict: {'success': bool, 'final_roi_frame': str, 'error': str náº¿u cÃ³ lá»—i}
    """
    try:
        # Äáº£m báº£o thÆ° má»¥c CameraROI tá»“n táº¡i
        ensure_directory_exists(CAMERA_ROI_DIR)

        # Má»Ÿ video vÃ  láº¥y frame Ä‘áº§u tiÃªn
        logging.debug("Äang má»Ÿ video Ä‘á»ƒ táº¡o áº£nh tá»•ng há»£p...")
        cap = cv2.VideoCapture(video_path)
        try:
            if not cap.isOpened():
                logging.error("KhÃ´ng thá»ƒ má»Ÿ video.")
                return {"success": False, "error": "KhÃ´ng thá»ƒ má»Ÿ video."}

            ret, frame = cap.read()
            if not ret:
                logging.error("KhÃ´ng thá»ƒ Ä‘á»c frame tá»« video.")
                return {"success": False, "error": "KhÃ´ng thá»ƒ Ä‘á»c frame tá»« video."}

            # Váº½ cÃ¡c vÃ¹ng ROI vá»›i mÃ u sáº¯c khÃ¡c nhau
            for roi in rois:
                x, y, w, h = roi["x"], roi["y"], roi["w"], roi["h"]
                roi_type = roi["type"]

                # Äá»‹nh nghÄ©a mÃ u sáº¯c cho tá»«ng loáº¡i ROI
                if roi_type == "packing":
                    color = (0, 255, 0)  # Xanh lÃ¡
                elif roi_type == "mvd":
                    color = (0, 0, 255)  # Äá»
                elif roi_type == "trigger":
                    color = (0, 255, 255)  # VÃ ng
                else:
                    color = (255, 255, 255)  # Tráº¯ng (máº·c Ä‘á»‹nh)

                # Váº½ ROI lÃªn frame
                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
                # ThÃªm nhÃ£n cho vÃ¹ng ROI
                cv2.putText(frame, roi_type.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)

            # Táº¡o tÃªn file vá»›i timestamp vÃ  camera_id
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            final_roi_frame_path = os.path.join(CAMERA_ROI_DIR, f"camera_{camera_id}_roi_final_{timestamp}.jpg")

            # LÆ°u áº£nh tá»•ng há»£p
            ret = cv2.imwrite(final_roi_frame_path, frame)
            if not ret:
                logging.error(f"KhÃ´ng thá»ƒ lÆ°u áº£nh tá»•ng há»£p táº¡i: {final_roi_frame_path}")
                return {"success": False, "error": f"KhÃ´ng thá»ƒ lÆ°u áº£nh tá»•ng há»£p táº¡i {final_roi_frame_path}"}
            logging.debug(f"ÄÃ£ lÆ°u áº£nh tá»•ng há»£p vá»›i táº¥t cáº£ ROI vÃ o: {final_roi_frame_path}")

            return {"success": True, "final_roi_frame": os.path.relpath(final_roi_frame_path, BASE_DIR)}
        
        finally:
            cap.release()
            logging.debug("ÄÃ£ giáº£i phÃ³ng tÃ i nguyÃªn video (cap.release) trong finalize_roi.")
    
    except Exception as e:
        logging.error(f"Lá»—i trong finalize_roi: {str(e)}")
        return {"success": False, "error": f"Lá»—i há»‡ thá»‘ng: {str(e)}"}

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 3:
        print("Usage: python3 hand_detection.py <video_path> <camera_id>")
        sys.exit(1)
    
    video_path = sys.argv[1]
    camera_id = sys.argv[2]
    try:
        roi_result = select_roi(video_path, camera_id)
        if not roi_result["success"]:
            print(roi_result["error"])
    except Exception as e:
        logging.error(f"Lá»—i khi cháº¡y script: {str(e)}")
        print(f"Lá»—i khi cháº¡y script: {str(e)}")

```
## ğŸ“„ File: `frame_sampler_no_trigger.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/frame_sampler_no_trigger.py`

```python
import cv2
import os
import logging
import sqlite3
import threading
import subprocess
import json
import queue
import numpy as np
import mediapipe as mp
from datetime import datetime, timezone, timedelta
from modules.db_utils import get_db_connection
from modules.scheduler.db_sync import frame_sampler_event, db_rwlock
import math
from modules.config.logging_config import get_logger


BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
MODEL_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "models", "wechat_qr")
DETECT_PROTO = os.path.join(MODEL_DIR, "detect.prototxt")
DETECT_MODEL = os.path.join(MODEL_DIR, "detect.caffemodel")
SR_PROTO = os.path.join(MODEL_DIR, "sr.prototxt")
SR_MODEL = os.path.join(MODEL_DIR, "sr.caffemodel")

class FrameSamplerNoTrigger:
    def __init__(self):
        self.setup_logging()
        self.load_config()
        self.video_lock = threading.Lock()
        self.setup_wechat_qr()
        # Initialize MediaPipe Hands
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)
        # Initialize log queue and writer thread
        self.log_queue = queue.Queue()
        self.log_thread = threading.Thread(target=self._log_writer)
        self.log_thread.daemon = True
        self.log_thread.start()

    def setup_logging(self):
        self.logger = get_logger(__name__, {"video_id": os.path.basename(self.video_file)})
        self.logger.info("Logging initialized")

    def setup_wechat_qr(self):
        for model_file in [DETECT_PROTO, DETECT_MODEL, SR_PROTO, SR_MODEL]:
            if not os.path.exists(model_file):
                logging.error(f"Model file not found: {model_file}")
                raise FileNotFoundError(f"Model file not found: {model_file}")
        try:
            self.qr_detector = cv2.wechat_qrcode_WeChatQRCode(DETECT_PROTO, DETECT_MODEL, SR_PROTO, SR_MODEL)
            logging.info("WeChat QRCode detector initialized")
        except Exception as e:
            logging.error(f"Failed to initialize WeChat QRCode: {str(e)}")
            raise

    def load_config(self):
        logging.info("Loading configuration from database")
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT input_path FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            self.video_root = result[0] if result else os.path.join(BASE_DIR, "Inputvideo")
            cursor.execute("SELECT output_path FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            self.output_path = result[0] if result else os.path.join(BASE_DIR, "output_clips")
            self.log_dir = os.path.join(self.output_path, "LOG", "Frame")
            os.makedirs(self.log_dir, exist_ok=True)
            cursor.execute("SELECT timezone FROM general_info WHERE id = 1")
            result = cursor.fetchone()
            tz_hours = int(result[0].split("+")[1]) if result and "+" in result[0] else 7
            self.video_timezone = timezone(timedelta(hours=tz_hours))
            cursor.execute("SELECT frame_rate, frame_interval, min_packing_time, motion_threshold, stable_duration_sec FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            self.fps, self.frame_interval, self.min_packing_time, self.motion_threshold, self.stable_duration_sec = result if result else (30, 5, 3, 0.1, 1.0)
            conn.close()
            logging.info(f"Config loaded: video_root={self.video_root}, output_path={self.output_path}, timezone={self.video_timezone}, fps={self.fps}, frame_interval={self.frame_interval}, min_packing_time={self.min_packing_time}, motion_threshold={self.motion_threshold}, stable_duration_sec={self.stable_duration_sec}")

    def get_packing_area(self, camera_name):
        logging.info(f"Querying qr_mvd_area for camera {camera_name}")
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT qr_mvd_area FROM packing_profiles WHERE profile_name = ?", (camera_name,))
            result = cursor.fetchone()
            conn.close()
        if result and result[0]:
            try:
                qr_mvd_area = result[0]
                if qr_mvd_area.startswith('(') and qr_mvd_area.endswith(')'):
                    x, y, w, h = map(int, qr_mvd_area.strip('()').split(','))
                else:
                    parsed = json.loads(qr_mvd_area)
                    if isinstance(parsed, list) and len(parsed) == 4:
                        x, y, w, h = parsed
                    else:
                        x, y, w, h = parsed['x'], parsed['y'], parsed['w'], parsed['h']
                roi = (x, y, w, h)
                logging.info(f"Using qr_mvd_area: {roi}")
            except (ValueError, json.JSONDecodeError, KeyError, TypeError) as e:
                logging.error(f"Error parsing qr_mvd_area for camera {camera_name}: {str(e)}")
                roi = None
        else:
            logging.warning(f"No qr_mvd_area found for camera {camera_name}")
            roi = None
        return roi

    def get_video_duration(self, video_file):
        try:
            cmd = ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", video_file]
            result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            return float(result.stdout.strip())
        except Exception:
            logging.error(f"Failed to get duration of video {video_file}")
            return None

    def load_video_files(self):
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT file_path FROM file_list WHERE is_processed = 0 AND status != 'xong' ORDER BY priority DESC, created_at ASC")
            video_files = [row[0] for row in cursor.fetchall()]
            conn.close()
        if not video_files:
            logging.info("No video files found with is_processed = 0 and status != 'xong'.")
        return video_files

    def process_frame(self, frame, frame_count):
        try:
            if len(frame.shape) == 2:
                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
            texts, _ = self.qr_detector.detectAndDecode(frame)
            for text in texts:
                if text and text != "TimeGo":
                    logging.info(f"Second {round((frame_count - 1) / self.fps)}: QR texts={texts}, mvd={text}")
                    return text
            return ""
        except Exception as e:
            logging.error(f"Error processing frame {frame_count}: {str(e)}")
            return ""

    def detect_hand(self, frame):
        try:
            # Convert BGR to RGB
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            # Process frame with MediaPipe Hands
            results = self.hands.process(rgb_frame)
            # Return True if hand landmarks are detected
            return bool(results.multi_hand_landmarks)
        except Exception as e:
            logging.error(f"Error in hand detection: {str(e)}")
            return False

    def compute_motion_level(self, prev_frame, curr_frame):
        try:
            if len(prev_frame.shape) == 3:
                prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
            if len(curr_frame.shape) == 3:
                curr_frame = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)
            diff = cv2.absdiff(prev_frame, curr_frame)
            motion_level = np.mean(diff) / 255.0
            return motion_level
        except Exception as e:
            logging.error(f"Error computing motion level: {str(e)}")
            return 1.0

    def _get_video_start_time(self, video_file):
        try:
            result = subprocess.check_output(['ffprobe', '-v', 'quiet', '-show_entries', 'format_tags=creation_time', '-of', 'default=noprint_wrappers=1:nokey=1', video_file])
            return datetime.strptime(result.decode().strip(), '%Y-%m-%dT%H:%M:%S.%fZ').replace(tzinfo=timezone.utc).astimezone(self.video_timezone)
        except (subprocess.CalledProcessError, ValueError):
            try:
                result = subprocess.check_output(['exiftool', '-CreateDate', '-d', '%Y-%m-%d %H:%M:%S', video_file])
                return datetime.strptime(result.decode().split('CreateDate')[1].strip().split('\n')[0].strip(), '%Y-%m-%d %H:%M:%S').replace(tzinfo=self.video_timezone)
            except (subprocess.CalledProcessError, IndexError):
                try:
                    result = subprocess.check_output(['exiftool', '-FileCreateDate', '-d', '%Y-%m-%d %H:%M:%S', video_file])
                    return datetime.strptime(result.decode().split('FileCreateDate')[1].strip().split('\n')[0].strip(), '%Y-%m-%d %H:%M:%S').replace(tzinfo=self.video_timezone)
                except (subprocess.CalledProcessError, IndexError):
                    logging.warning("No metadata found, using file creation time.")
                    return datetime.fromtimestamp(os.path.getctime(video_file), tz=self.video_timezone)

    def _log_writer(self):
        while True:
            log_file, entry, timestamp = self.log_queue.get()
            with threading.Lock():
                mode = 'w' if not os.path.exists(log_file) else 'a'
                with open(log_file, mode) as f:
                    if mode == 'w':
                        f.write(f"# Start: {timestamp['start']}, End: {timestamp['end']}, Start_Time: {timestamp['start_time']}, Camera_Name: {timestamp['camera']}, Video_File: {timestamp['video']}\n")
                    f.write(f"{entry}\n")
                    f.flush()
            self.log_queue.task_done()

    def _update_log_file(self, log_file, start_second, end_second, start_time, camera_name, video_file):
        timestamp = {
            'start': start_second,
            'end': end_second,
            'start_time': start_time.strftime('%Y-%m-%d %H:%M:%S'),
            'camera': camera_name,
            'video': video_file
        }
        with db_rwlock.gen_wlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT 1 FROM processed_logs WHERE log_file = ?", (log_file,))
            if not cursor.fetchone():
                cursor.execute("INSERT INTO processed_logs (log_file, is_processed) VALUES (?, 0)", (log_file,))
            conn.commit()
            conn.close()
        return lambda entry, ts: self.log_queue.put((log_file, f"{ts},{entry}", timestamp))

    def run(self):
        while True:
            frame_sampler_event.wait()
            video_files = self.load_video_files()
            if not video_files:
                logging.info("No videos to process")
                frame_sampler_event.clear()
                continue
            for video_file in video_files:
                log_file = self.process_video(video_file, self.video_lock, self.get_packing_area, self.process_frame, self.frame_interval)
                if log_file:
                    logging.info(f"Completed processing video {video_file}, log at {log_file}")
                else:
                    logging.error(f"Failed to process video {video_file}")
            frame_sampler_event.clear()

    def process_video(self, video_file, video_lock, get_packing_area_func, process_frame_func, frame_interval, start_time=0, end_time=None):
        with video_lock:
            logging.info(f"Processing video: {video_file} from {start_time}s to {end_time}s")
            if not os.path.exists(video_file):
                logging.error(f"File '{video_file}' does not exist")
                with db_rwlock.gen_wlock():
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("lá»—i", video_file))
                    conn.commit()
                    conn.close()
                return None
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("Ä‘ang frame sampler ...", video_file))
                cursor.execute("SELECT camera_name FROM file_list WHERE file_path = ?", (video_file,))
                result = cursor.fetchone()
                camera_name = result[0] if result and result[0] else "CamTest"
                conn.commit()
                conn.close()
            video = cv2.VideoCapture(video_file)
            if not video.isOpened():
                logging.error(f"Failed to open video '{video_file}'")
                with db_rwlock.gen_wlock():
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("lá»—i", video_file))
                    conn.commit()
                    conn.close()
                return None
            start_time_obj = self._get_video_start_time(video_file)
            roi = get_packing_area_func(camera_name)
            total_seconds = self.get_video_duration(video_file)
            if total_seconds is None:
                logging.error(f"Failed to get duration of video {video_file}")
                with db_rwlock.gen_wlock():
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("lá»—i", video_file))
                    conn.commit()
                    conn.close()
                return None
            logging.info(f"Video duration {video_file}: {total_seconds} seconds")
            video_name = os.path.splitext(os.path.basename(video_file))[0]
            segment_duration = 300
            # XÃ¡c Ä‘á»‹nh cÃ¡c Ä‘oáº¡n 300s chá»©a [start_time, end_time]
            end_time = total_seconds if end_time is None else min(end_time, total_seconds)
            start_segment = math.floor(start_time / segment_duration) * segment_duration
            end_segment = math.ceil(end_time / segment_duration) * segment_duration
            current_start_second = start_segment
            current_end_second = min(current_start_second + segment_duration, end_segment)
            camera_log_dir = os.path.join(self.log_dir, camera_name)
            os.makedirs(camera_log_dir, exist_ok=True)
            log_file = os.path.join(camera_log_dir, f"log_{video_name}_{current_start_second:04d}_{current_end_second:04d}.txt")
            log_file_handle = self._update_log_file(log_file, current_start_second, current_end_second, start_time_obj + timedelta(seconds=current_start_second), camera_name, video_file)
            # Báº¯t Ä‘áº§u tá»« khung hÃ¬nh táº¡i start_time
            start_frame = int(start_time * self.fps)
            end_frame = int(end_time * self.fps)
            video.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            frame_count = start_frame
            prev_frame = None
            stable_segments = []
            qr_events = []
            stable_start = None
            last_te = -self.min_packing_time * self.fps
            is_stable = False
            while video.isOpened() and frame_count < end_frame:
                ret, frame = video.read()
                if not ret:
                    break
                if roi:
                    x, y, w, h = roi
                    frame_height, frame_width = frame.shape[:2]
                    if w > 0 and h > 0 and y + h <= frame_height and x + w <= frame_width:
                        frame = frame[y:y + h, x:x + w]
                    else:
                        logging.warning(f"Invalid ROI for frame {frame_count}: {roi}, frame_size: {frame_width}x{frame_height}")
                        frame = frame
                frame_count += 1
                if frame_count % frame_interval != 0:
                    continue
                if frame.size == 0 or frame.shape[0] == 0 or frame.shape[1] == 0:
                    logging.warning(f"Empty frame {frame_count}, skipping")
                    continue
                # QR detection
                mvd = process_frame_func(frame, frame_count)
                if mvd:
                    qr_events.append((frame_count, mvd))
                # Motion detection
                if prev_frame is not None:
                    motion_level = self.compute_motion_level(prev_frame, frame)
                    min_stable_frames = max(6, int(self.fps * self.stable_duration_sec / self.frame_interval))
                    if motion_level < self.motion_threshold:
                        if not is_stable:
                            stable_start = frame_count
                            is_stable = True
                    else:
                        if is_stable and (frame_count - stable_start) >= min_stable_frames * frame_interval:
                            start_second = round((stable_start - 1) / self.fps, 1)
                            end_second = round((frame_count - frame_interval - 1) / self.fps, 1)
                            if start_second >= start_time and end_second <= end_time:
                                stable_segments.append((stable_start, frame_count - frame_interval))
                                logging.info(f"Stable segment: start={start_second}s, end={end_second}s")
                        is_stable = False
                prev_frame = frame.copy()
                second_in_video = (frame_count - 1) / self.fps
                second = round(second_in_video)
                if second >= current_end_second and second < end_time:
                    current_start_second = current_end_second
                    current_end_second = min(current_start_second + segment_duration, end_segment)
                    camera_log_dir = os.path.join(self.log_dir, camera_name)
                    os.makedirs(camera_log_dir, exist_ok=True)
                    log_file = os.path.join(camera_log_dir, f"log_{video_name}_{current_start_second:04d}_{current_end_second:04d}.txt")
                    log_file_handle = self._update_log_file(log_file, current_start_second, current_end_second, start_time_obj + timedelta(seconds=current_start_second), camera_name, video_file)
            if is_stable and (frame_count - stable_start) >= min_stable_frames * frame_interval:
                start_second = round((stable_start - 1) / self.fps, 1)
                end_second = round((frame_count - 1) / self.fps, 1)
                if start_second >= start_time and end_second <= end_time:
                    stable_segments.append((stable_start, frame_count))
                    logging.info(f"Stable segment: start={start_second}s, end={end_second}s")
            # Group QR codes and select the last frame of each sequence
            grouped_qr = []
            current_mvd = None
            current_frames = []
            for frame, mvd in qr_events:
                if mvd == "TimeGo":
                    continue
                if mvd != current_mvd:
                    if current_mvd and current_frames:
                        grouped_qr.append((current_frames[-1], current_mvd))
                    current_mvd = mvd
                    current_frames = [frame]
                else:
                    current_frames.append(frame)
            if current_mvd and current_frames:
                grouped_qr.append((current_frames[-1], current_mvd))
            # Log last QR event
            if grouped_qr:
                last_qr_frame, last_qr_code = grouped_qr[-1]
                last_qr_second = round((last_qr_frame - 1) / self.fps, 1)
                if last_qr_second >= start_time and last_qr_second <= end_time:
                    logging.info(f"Last QR event: Te={last_qr_second}s, QR={last_qr_code}")
            # Find Ts/Te
            video.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            frame_count = start_frame
            last_te = -self.min_packing_time * self.fps
            prev_te_frame = None
            for idx, (te_frame, qr_code) in enumerate(grouped_qr):
                if te_frame <= last_te + self.min_packing_time * self.fps:
                    logging.info(f"Skipping event for QR {qr_code} at frame {te_frame}: too close to last_te {last_te}")
                    continue
                second_te = round((te_frame - 1) / self.fps)
                if second_te < start_time or second_te > end_time:
                    continue
                segment_index = math.floor(second_te / segment_duration)
                target_start_second = segment_index * segment_duration
                target_end_second = (segment_index + 1) * segment_duration
                if second_te >= current_end_second or target_start_second != current_start_second:
                    current_start_second = target_start_second
                    current_end_second = min(target_start_second + segment_duration, end_segment)
                    camera_log_dir = os.path.join(self.log_dir, camera_name)
                    os.makedirs(camera_log_dir, exist_ok=True)
                    log_file = os.path.join(camera_log_dir, f"log_{video_name}_{current_start_second:04d}_{current_end_second:04d}.txt")
                    log_file_handle = self._update_log_file(log_file, current_start_second, current_end_second, start_time_obj + timedelta(seconds=current_start_second), camera_name, video_file)
                ts_frame = None
                # TrÆ°á»ng há»£p Ä‘áº·c biá»‡t: Te Ä‘áº§u tiÃªn
                if idx == 0:
                    has_stable_segment = any(start <= te_frame for start, end in stable_segments)
                    if not has_stable_segment:
                        log_file_handle(f"On,{qr_code}", second_te)
                        logging.info(f"Logged only Te for QR {qr_code} at second {second_te}: no stable segments for first Te")
                        last_te = te_frame
                        prev_te_frame = te_frame
                        continue
                # TÃ¬m vÃ¹ng á»•n Ä‘á»‹nh sau Te phÃ­a trÆ°á»›c (hoáº·c Ä‘áº§u video náº¿u khÃ´ng cÃ³ Te trÆ°á»›c)
                closest_stable = None
                search_start = max(prev_te_frame if prev_te_frame is not None else start_frame, start_frame)
                for start, end in stable_segments:
                    if start > search_start and end < te_frame:
                        if closest_stable is None or start < closest_stable[0]:
                            closest_stable = (start, end)
                if closest_stable:
                    # TÃ¬m tay sau vÃ¹ng á»•n Ä‘á»‹nh
                    video.set(cv2.CAP_PROP_POS_FRAMES, closest_stable[1])
                    hand_frame_count = closest_stable[1]
                    while hand_frame_count < te_frame and hand_frame_count < end_frame:
                        ret, frame = video.read()
                        if not ret:
                            break
                        if roi:
                            x, y, w, h = roi
                            frame_height, frame_width = frame.shape[:2]
                            if w > 0 and h > 0 and y + h <= frame_height and x + w <= frame_width:
                                frame = frame[y:y + h, x:x + w]
                        hand_frame_count += 1
                        if hand_frame_count % self.frame_interval != 0:
                            continue
                        if self.detect_hand(frame) and hand_frame_count > last_te + self.min_packing_time * self.fps:
                            ts_frame = hand_frame_count
                            second_ts = round((ts_frame - 1) / self.fps, 1)
                            if second_ts >= start_time and second_ts <= end_time:
                                logging.info(f"Hand detected for Ts: frame={ts_frame}, time={second_ts}s")
                                break
                            ts_frame = None
                else:
                    # KhÃ´ng cÃ³ vÃ¹ng á»•n Ä‘á»‹nh, tÃ¬m tay ngay sau Te phÃ­a trÆ°á»›c
                    if prev_te_frame is not None:
                        video.set(cv2.CAP_PROP_POS_FRAMES, max(prev_te_frame, start_frame))
                        hand_frame_count = max(prev_te_frame, start_frame)
                        while hand_frame_count < te_frame and hand_frame_count < end_frame:
                            ret, frame = video.read()
                            if not ret:
                                break
                            if roi:
                                x, y, w, h = roi
                                frame_height, frame_width = frame.shape[:2]
                                if w > 0 and h > 0 and y + h <= frame_height and x + w <= frame_width:
                                    frame = frame[y:y + h, x:x + w]
                            hand_frame_count += 1
                            if hand_frame_count % self.frame_interval != 0:
                                continue
                            if self.detect_hand(frame) and hand_frame_count > last_te + self.min_packing_time * self.fps:
                                ts_frame = hand_frame_count
                                second_ts = round((ts_frame - 1) / self.fps, 1)
                                if second_ts >= start_time and second_ts <= end_time:
                                    logging.info(f"Hand detected for Ts: frame={ts_frame}, time={second_ts}s")
                                    break
                                ts_frame = None
                # Ghi log
                if ts_frame:
                    second_ts = round((ts_frame - 1) / self.fps)
                    segment_index = math.floor(max(second_ts, second_te) / segment_duration)
                    target_start_second = segment_index * segment_duration
                    target_end_second = (segment_index + 1) * segment_duration
                    if max(second_ts, second_te) >= current_end_second or target_start_second != current_start_second:
                        current_start_second = target_start_second
                        current_end_second = min(target_start_second + segment_duration, end_segment)
                        camera_log_dir = os.path.join(self.log_dir, camera_name)
                        os.makedirs(camera_log_dir, exist_ok=True)
                        log_file = os.path.join(camera_log_dir, f"log_{video_name}_{current_start_second:04d}_{current_end_second:04d}.txt")
                        log_file_handle = self._update_log_file(log_file, current_start_second, current_end_second, start_time_obj + timedelta(seconds=current_start_second), camera_name, video_file)
                    log_file_handle("On,", second_ts - 1)
                    log_file_handle("Off,", second_ts)
                    log_file_handle("Off,", second_te - 1)
                    log_file_handle(f"On,{qr_code}", second_te)
                    logging.info(f"Event logged: Ts={second_ts}, Te={second_te}, QR={qr_code}")
                else:
                    second_ts = second_te - self.min_packing_time - 1
                    if second_ts >= start_time and second_ts >= (last_te / self.fps):
                        segment_index = math.floor(max(second_ts, second_te) / segment_duration)
                        target_start_second = segment_index * segment_duration
                        target_end_second = (segment_index + 1) * segment_duration
                        if max(second_ts, second_te) >= current_end_second or target_start_second != current_start_second:
                            current_start_second = target_start_second
                            current_end_second = min(target_start_second + segment_duration, end_segment)
                            camera_log_dir = os.path.join(self.log_dir, camera_name)
                            os.makedirs(camera_log_dir, exist_ok=True)
                            log_file = os.path.join(camera_log_dir, f"log_{video_name}_{current_start_second:04d}_{current_end_second:04d}.txt")
                            log_file_handle = self._update_log_file(log_file, current_start_second, current_end_second, start_time_obj + timedelta(seconds=current_start_second), camera_name, video_file)
                        log_file_handle("On,", second_ts - 1)
                        log_file_handle("Off,", second_ts)
                        log_file_handle("Off,", second_te - 1)
                        log_file_handle(f"On,{qr_code}", second_te)
                        logging.info(f"Assumed Ts={second_ts} for Te={second_te}, QR={qr_code}")
                    else:
                        log_file_handle("Off,", second_te - 1)
                        log_file_handle(f"On,{qr_code}", second_te)
                        logging.info(f"Logged only Te for QR {qr_code} at second {second_te}: assumed Ts={second_ts} invalid (out of range or too close to last_te)")
                last_te = te_frame
                prev_te_frame = te_frame
            video.release()
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("UPDATE file_list SET is_processed = 1, status = ? WHERE file_path = ?", ("xong", video_file))
                conn.commit()
                conn.close()
            logging.info(f"Completed processing video: {video_file}")
            return log_file

```
## ğŸ“„ File: `frame_sampler_trigger.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/frame_sampler_trigger.py`

```python
import cv2
import os
import logging
import sqlite3
import threading
import subprocess
import json
import numpy as np
from datetime import datetime, timezone, timedelta
from modules.db_utils import get_db_connection
from modules.scheduler.db_sync import frame_sampler_event, db_rwlock
import math
from modules.config.logging_config import get_logger


BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
MODEL_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "models", "wechat_qr")
DETECT_PROTO = os.path.join(MODEL_DIR, "detect.prototxt")
DETECT_MODEL = os.path.join(MODEL_DIR, "detect.caffemodel")
SR_PROTO = os.path.join(MODEL_DIR, "sr.prototxt")
SR_MODEL = os.path.join(MODEL_DIR, "sr.caffemodel")

class FrameSamplerTrigger:
    def __init__(self):
        self.setup_logging()
        self.load_config()
        self.video_lock = threading.Lock()
        self.setup_wechat_qr()

    def setup_logging(self):
        self.logger = get_logger(__name__, {"video_id": os.path.basename(self.video_file)})
        self.logger.info("Logging initialized")

    def load_config(self):
        logging.info("Loading configuration from database")
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT input_path FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            self.video_root = result[0] if result else os.path.join(BASE_DIR, "Inputvideo")
            cursor.execute("SELECT output_path FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            self.output_path = result[0] if result else os.path.join(BASE_DIR, "output_clips")
            self.log_dir = os.path.join(self.output_path, "LOG", "Frame")
            os.makedirs(self.log_dir, exist_ok=True)
            cursor.execute("SELECT timezone FROM general_info WHERE id = 1")
            result = cursor.fetchone()
            tz_hours = int(result[0].split("+")[1]) if result and "+" in result[0] else 7
            self.video_timezone = timezone(timedelta(hours=tz_hours))
            cursor.execute("SELECT frame_rate, frame_interval, min_packing_time FROM processing_config WHERE id = 1")
            result = cursor.fetchone()
            self.fps, self.frame_interval, self.min_packing_time = result if result else (30, 5, 5)
            logging.info(f"Config loaded: video_root={self.video_root}, output_path={self.output_path}, timezone={self.video_timezone}, fps={self.fps}, frame_interval={self.frame_interval}, min_packing_time={self.min_packing_time}")

    def get_packing_area(self, camera_name):
        logging.info(f"Querying qr_mvd_area and jump_time_ratio for camera {camera_name}")
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT qr_mvd_area, jump_time_ratio FROM packing_profiles WHERE profile_name = ?", (camera_name,))
            result = cursor.fetchone()
            cursor.execute("SELECT qr_trigger_area FROM packing_profiles WHERE profile_name = ?", (camera_name,))
            trigger_result = cursor.fetchone()
            conn.close()
        if result and result[1] is not None:
            self.jump_time_ratio = float(result[1])
            logging.info(f"Loaded jump_time_ratio: {self.jump_time_ratio}")
        else:
            self.jump_time_ratio = 0.5
            logging.info(f"Using default jump_time_ratio: {self.jump_time_ratio}")

        if result and result[0]:
            try:
                qr_mvd_area = result[0]
                if qr_mvd_area.startswith('(') and qr_mvd_area.endswith(')'):
                    x, y, w, h = map(int, qr_mvd_area.strip('()').split(','))
                else:
                    parsed = json.loads(qr_mvd_area)
                    if isinstance(parsed, list) and len(parsed) == 4:
                        x, y, w, h = parsed
                    else:
                        x, y, w, h = parsed['x'], parsed['y'], parsed['w'], parsed['h']
                roi = (x, y, w, h)
                logging.info(f"Using qr_mvd_area: {roi}")
            except (ValueError, json.JSONDecodeError, KeyError, TypeError) as e:
                logging.error(f"Error parsing qr_mvd_area for camera {camera_name}: {str(e)}")
                roi = None
        else:
            logging.warning(f"No qr_mvd_area found for camera {camera_name}")
            roi = None
        trigger = json.loads(trigger_result[0]) if trigger_result and trigger_result[0] else [0, 0, 0, 0]
        logging.info(f"Using trigger: {trigger}")
        return roi, trigger

    def get_video_duration(self, video_file):
        try:
            cmd = ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", video_file]
            result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            return float(result.stdout.strip())
        except Exception:
            logging.error(f"Failed to get duration of video {video_file}")
            return None

    def load_video_files(self):
        with db_rwlock.gen_rlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT file_path FROM file_list WHERE is_processed = 0 AND status != 'xong' ORDER BY priority DESC, created_at ASC")
            video_files = [row[0] for row in cursor.fetchall()]
            conn.close()
        if not video_files:
            logging.info("No video files found with is_processed = 0 and status != 'xong'.")
        return video_files

    def process_frame(self, frame, frame_count):
        try:
            if len(frame.shape) == 2:
                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
            texts, _ = self.qr_detector.detectAndDecode(frame)
            state = "Off"
            mvd = ""
            for text in texts:
                if text == "TimeGo":
                    state = "On"
                elif text:
                    mvd = text
            return state, mvd
        except Exception as e:
            logging.error(f"Error processing frame {frame_count}: {str(e)}")
            return "", ""

    def _get_video_start_time(self, video_file):
        try:
            result = subprocess.check_output(['ffprobe', '-v', 'quiet', '-show_entries', 'format_tags=creation_time', '-of', 'default=noprint_wrappers=1:nokey=1', video_file])
            return datetime.strptime(result.decode().strip(), '%Y-%m-%dT%H:%M:%S.%fZ').replace(tzinfo=timezone.utc).astimezone(self.video_timezone)
        except (subprocess.CalledProcessError, ValueError):
            try:
                result = subprocess.check_output(['exiftool', '-CreateDate', '-d', '%Y-%m-%d %H:%M:%S', video_file])
                return datetime.strptime(result.decode().split('CreateDate')[1].strip().split('\n')[0].strip(), '%Y-%m-%d %H:%M:%S').replace(tzinfo=self.video_timezone)
            except (subprocess.CalledProcessError, IndexError):
                try:
                    result = subprocess.check_output(['exiftool', '-FileCreateDate', '-d', '%Y-%m-%d %H:%M:%S', video_file])
                    return datetime.strptime(result.decode().split('FileCreateDate')[1].strip().split('\n')[0].strip(), '%Y-%m-%d %H:%M:%S').replace(tzinfo=self.video_timezone)
                except (subprocess.CalledProcessError, IndexError):
                    logging.warning("No metadata found, using file creation time.")
                    return datetime.fromtimestamp(os.path.getctime(video_file), tz=self.video_timezone)

    def _update_log_file(self, log_file, start_second, end_second, start_time, camera_name, video_file):
        log_file_handle = open(log_file, 'w')
        log_file_handle.write(f"# Start: {start_second}, End: {end_second}, Start_Time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}, Camera_Name: {camera_name}, Video_File: {video_file}\n")
        log_file_handle.flush()
        with db_rwlock.gen_wlock():
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT 1 FROM processed_logs WHERE log_file = ?", (log_file,))
            if not cursor.fetchone():
                cursor.execute("INSERT INTO processed_logs (log_file, is_processed) VALUES (?, 0)", (log_file,))
            conn.commit()
            conn.close()
        return log_file_handle

    def run(self):
        while True:
            frame_sampler_event.wait()
            video_files = self.load_video_files()
            if not video_files:
                logging.info("No videos to process")
                frame_sampler_event.clear()
                continue
            for video_file in video_files:
                log_file = self.process_video(video_file, self.video_lock, self.get_packing_area, self.process_frame, self.frame_interval)
                if log_file:
                    logging.info(f"Completed processing video {video_file}, log at {log_file}")
                else:
                    logging.error(f"Failed to process video {video_file}")
            frame_sampler_event.clear()

    def process_video(self, video_file, video_lock, get_packing_area_func, process_frame_func, frame_interval, start_time=0, end_time=None):
        with video_lock:
            logging.info(f"Processing video: {video_file} from {start_time}s to {end_time}s")
            if not os.path.exists(video_file):
                logging.error(f"File '{video_file}' does not exist")
                with db_rwlock.gen_wlock():
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("lá»—i", video_file))
                    conn.commit()
                    conn.close()
                return None
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("Ä‘ang frame sampler ...", video_file))
                cursor.execute("SELECT camera_name FROM file_list WHERE file_path = ?", (video_file,))
                result = cursor.fetchone()
                camera_name = result[0] if result and result[0] else "CamTest"
                conn.commit()
                conn.close()
            video = cv2.VideoCapture(video_file)
            if not video.isOpened():
                logging.error(f"Failed to open video '{video_file}'")
                with db_rwlock.gen_wlock():
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("lá»—i", video_file))
                    conn.commit()
                    conn.close()
                return None
            start_time_obj = self._get_video_start_time(video_file)
            roi, trigger = get_packing_area_func(camera_name)
            total_seconds = self.get_video_duration(video_file)
            if total_seconds is None:
                logging.error(f"Failed to get duration of video {video_file}")
                with db_rwlock.gen_wlock():
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("UPDATE file_list SET status = ? WHERE file_path = ?", ("lá»—i", video_file))
                    conn.commit()
                    conn.close()
                return None
            logging.info(f"Video duration {video_file}: {total_seconds} seconds")
            video_name = os.path.splitext(os.path.basename(video_file))[0]
            segment_duration = 300
            # XÃ¡c Ä‘á»‹nh cÃ¡c Ä‘oáº¡n 300s chá»©a [start_time, end_time]
            end_time = total_seconds if end_time is None else min(end_time, total_seconds)
            start_segment = math.floor(start_time / segment_duration) * segment_duration
            end_segment = math.ceil(end_time / segment_duration) * segment_duration
            current_start_second = start_segment
            current_end_second = min(current_start_second + segment_duration, end_segment)
            camera_log_dir = os.path.join(self.log_dir, camera_name)
            os.makedirs(camera_log_dir, exist_ok=True)
            log_file = os.path.join(camera_log_dir, f"log_{video_name}_{current_start_second:04d}_{current_end_second:04d}.txt")
            log_file_handle = self._update_log_file(log_file, current_start_second, current_end_second, start_time_obj + timedelta(seconds=current_start_second), camera_name, video_file)
            # Báº¯t Ä‘áº§u tá»« khung hÃ¬nh táº¡i start_time
            start_frame = int(start_time * self.fps)
            end_frame = int(end_time * self.fps)
            video.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            frame_count = start_frame
            frame_states = []
            mvd_list = []
            last_state = None
            last_mvd = ""
            jump_time_ratio = getattr(self, 'jump_time_ratio', 0.5)  # Láº¥y tá»« config hoáº·c máº·c Ä‘á»‹nh 0.5
            while video.isOpened() and frame_count < end_frame:
                ret, frame = video.read()
                if not ret:
                    break
                if roi:
                    x, y, w, h = roi
                    frame_height, frame_width = frame.shape[:2]
                    if w > 0 and h > 0 and y + h <= frame_height and x + w <= frame_width:
                        frame = frame[y:y + h, x:x + w]
                    else:
                        logging.warning(f"Invalid ROI for frame {frame_count}: {roi}, frame size: {frame_width}x{frame_height}")
                        frame = frame
                frame_count += 1
                if frame_count % frame_interval != 0:
                    continue
                if frame.size == 0 or frame.shape[0] == 0 or frame.shape[1] == 0:
                    logging.warning(f"Empty frame {frame_count}, skipping")
                    continue
                state, mvd = process_frame_func(frame, frame_count)
                second_in_video = (frame_count - 1) / self.fps
                second = round(second_in_video)
                if second >= current_end_second and second < end_time:
                    log_file_handle.close()
                    current_start_second = current_end_second
                    current_end_second = min(current_start_second + segment_duration, end_segment)
                    camera_log_dir = os.path.join(self.log_dir, camera_name)
                    os.makedirs(camera_log_dir, exist_ok=True)
                    log_file = os.path.join(camera_log_dir, f"log_{video_name}_{current_start_second:04d}_{current_end_second:04d}.txt")
                    log_file_handle = self._update_log_file(log_file, current_start_second, current_end_second, start_time_obj + timedelta(seconds=current_start_second), camera_name, video_file)
                if second >= start_time and second <= end_time:
                    # Ghi MVD ngay náº¿u cÃ³ vÃ  khÃ¡c last_mvd
                    if mvd and mvd != last_mvd:
                        log_line = f"{second},{state},{mvd}\n"
                        log_file_handle.write(log_line)
                        logging.info(f"Log second {second}: state={state}, mvd={mvd}")
                        log_file_handle.flush()
                        last_mvd = mvd
                    # Tiáº¿p tá»¥c thu tháº­p tráº¡ng thÃ¡i cho final_state
                    frame_states.append(state)
                    mvd_list.append(mvd)
                    if len(frame_states) == 5:
                        on_count = sum(1 for s in frame_states if s == "On")
                        off_count = sum(1 for s in frame_states if s == "Off")
                        frame_states_str = " ".join(frame_states).lower()
                        final_state = None
                        if on_count >= 3:
                            final_state = "On"
                        elif off_count == 5:
                            final_state = "Off"
                        if final_state:
                            if final_state != last_state:
                                log_line = f"{second},{final_state},\n"
                                log_file_handle.write(log_line)
                                logging.info(f"Log second {second}: {frame_states_str}: {final_state}")
                                log_file_handle.flush()
                                if last_state == "On" and final_state == "Off":
                                    jump_frames = int(jump_time_ratio * self.min_packing_time * self.fps)
                                    new_frame_count = frame_count + jump_frames
                                    if new_frame_count < end_frame:
                                        video.set(cv2.CAP_PROP_POS_FRAMES, new_frame_count)
                                        frame_count = new_frame_count
                                        logging.info(f"Jumped {jump_frames} frames to {frame_count} after On->Off transition")
                                last_state = final_state
                        else:
                            logging.info(f"Skipped second {second}: {frame_states_str}, on_count={on_count}, off_count={off_count}")
                            log_file_handle.flush()
                        frame_states = []
                        mvd_list = []
            log_file_handle.close()
            video.release()
            with db_rwlock.gen_wlock():
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("UPDATE file_list SET is_processed = 1, status = ? WHERE file_path = ?", ("xong", video_file))
                conn.commit()
                conn.close()
            logging.info(f"Completed processing video: {video_file}")
            return log_file

```
## ğŸ“„ File: `cutter_complete.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/cutter/cutter_complete.py`

```python
import subprocess

def cut_complete_event(event, video_buffer, video_length, output_file):
    """Cáº¯t video cho sá»± kiá»‡n hoÃ n chá»‰nh (cÃ³ cáº£ ts vÃ  te)."""
    ts = event.get("ts")
    te = event.get("te")
    video_file = event.get("video_file")

    start_time = max(0, ts - video_buffer)  # ThÃªm buffer trÆ°á»›c ts
    end_time = min(te + video_buffer, video_length)  # ThÃªm buffer sau te
    duration = end_time - start_time

    if duration <= 0:
        print(f"Bá» qua: Duration khÃ´ng há»£p lá»‡ ({duration}s) cho sá»± kiá»‡n {event.get('event_id')}")
        return False

    try:
        cmd = [
            "ffmpeg",
            "-i", video_file,
            "-ss", str(start_time),
            "-t", str(duration),
            "-c:v", "copy",
            "-c:a", "copy",
            "-y",
            output_file
        ]
        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        print(f"ÄÃ£ cáº¯t video: {output_file}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"Lá»—i khi cáº¯t video {video_file}: {e}")
        return False
    except Exception as e:
        print(f"Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh: {e}")
        return False
```
## ğŸ“„ File: `cutter_incomplete.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/cutter/cutter_incomplete.py`

```python
import os
import subprocess
from .cutter_utils import generate_merged_filename, generate_output_filename

def cut_incomplete_event(event, video_buffer, video_length, output_file):
    """Cáº¯t video cho sá»± kiá»‡n dá»Ÿ dang (thiáº¿u ts hoáº·c te)."""
    ts = event.get("ts")
    te = event.get("te")
    video_file = event.get("video_file")

    # Log Ä‘á»™ dÃ i video gá»‘c
    print(f"Video gá»‘c {video_file} cÃ³ Ä‘á»™ dÃ i: {video_length} giÃ¢y")

    # Log giÃ¡ trá»‹ video_buffer
    print(f"Sá»­ dá»¥ng video_buffer: {video_buffer} giÃ¢y")

    if ts is not None and te is None:  # Chá»‰ cÃ³ ts
        start_time = max(0, ts - video_buffer)
        duration = video_length - start_time
    elif ts is None and te is not None:  # Chá»‰ cÃ³ te
        start_time = 0
        duration = min(te + video_buffer, video_length)
    else:
        print(f"Bá» qua: Sá»± kiá»‡n {event.get('event_id')} khÃ´ng cÃ³ ts hoáº·c te")
        return False

    if duration <= 0:
        print(f"Bá» qua: Duration khÃ´ng há»£p lá»‡ ({duration}s) cho sá»± kiá»‡n {event.get('event_id')}")
        return False

    try:
        cmd = [
            "ffmpeg",
            "-i", video_file,
            "-ss", str(start_time),
            "-t", str(duration),
            "-c:v", "copy",
            "-c:a", "copy",
            "-y",
            output_file
        ]
        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        print(f"ÄÃ£ cáº¯t video: {output_file}")

        # Log Ä‘á»™ dÃ i cá»§a file dá»Ÿ dang vá»«a táº¡o
        probe = subprocess.run(
            ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", output_file],
            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
        )
        duration = float(probe.stdout.strip())
        print(f"File dá»Ÿ dang {output_file} cÃ³ Ä‘á»™ dÃ i: {duration} giÃ¢y")

        event["cut_video_file"] = output_file
        return True
    except subprocess.CalledProcessError as e:
        print(f"Lá»—i khi cáº¯t video {video_file}: {e}")
        return False
    except Exception as e:
        print(f"Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh: {e}")
        return False

def merge_incomplete_events(event_a, event_b, video_buffer, video_length_a, video_length_b, output_dir, max_packing_time, brand_name="Alan"):
    """GhÃ©p ná»‘i hai sá»± kiá»‡n dá»Ÿ dang (A cÃ³ ts, B cÃ³ te) vÃ  táº¡o file ghÃ©p vá»›i tÃªn tá»‘i Æ°u."""
    video_file_a = event_a.get("video_file")
    video_file_b = event_b.get("video_file")

    # Kiá»ƒm tra náº¿u file Ä‘Ã£ Ä‘Æ°á»£c cáº¯t sáºµn
    temp_file_a = event_a.get("cut_video_file")
    temp_file_b = event_b.get("cut_video_file")
    files_to_cleanup = []

    # Táº¡o thÆ° má»¥c temp_clips Ä‘á»ƒ lÆ°u file táº¡m
    temp_clips_dir = os.path.join(os.path.dirname(output_dir), "temp_clips")
    if not os.path.exists(temp_clips_dir):
        os.makedirs(temp_clips_dir)

    # Náº¿u khÃ´ng cÃ³ file cáº¯t sáºµn, cáº¯t ngay láº­p tá»©c vÃ  lÆ°u vÃ o temp_clips_dir
    if not temp_file_a or not os.path.exists(temp_file_a):
        temp_file_a = os.path.join(temp_clips_dir, f"temp_a_{event_a.get('event_id')}_incomplete.mp4")
        if not cut_incomplete_event(event_a, video_buffer, video_length_a, temp_file_a):
            print(f"Lá»—i: KhÃ´ng thá»ƒ cáº¯t file táº¡m cho sá»± kiá»‡n {event_a.get('event_id')}")
            return None
    if not temp_file_b or not os.path.exists(temp_file_b):
        temp_file_b = os.path.join(temp_clips_dir, f"temp_b_{event_b.get('event_id')}_incomplete.mp4")
        if not cut_incomplete_event(event_b, video_buffer, video_length_b, temp_file_b):
            print(f"Lá»—i: KhÃ´ng thá»ƒ cáº¯t file táº¡m cho sá»± kiá»‡n {event_b.get('event_id')}")
            return None

    # Táº¡o tÃªn file Ä‘áº§u ra tá»‘i Æ°u dá»±a trÃªn temp_file_a vÃ  temp_file_b
    file_name_a = os.path.basename(temp_file_a)
    file_name_b = os.path.basename(temp_file_b)
    parts_a = file_name_a.split("_")
    parts_b = file_name_b.split("_")

    # Láº¥y Brand tá»« file Ä‘áº§u tiÃªn
    brand_name = parts_a[0]

    # Láº¥y mÃ£ váº­n Ä‘Æ¡n tá»« cáº£ hai file, loáº¡i bá» "NoCode"
    tracking_codes = []
    if len(parts_a) >= 2 and parts_a[1] != "NoCode":
        tracking_codes.append(parts_a[1])
    if len(parts_b) >= 2 and parts_b[1] != "NoCode":
        tracking_codes.append(parts_b[1])

    # Láº¥y thá»i gian tá»« file Ä‘áº§u tiÃªn
    date = parts_a[2] if len(parts_a) >= 3 else "unknown"
    hour = parts_a[3].split(".")[0] if len(parts_a) >= 4 else "0000"
    time_str = f"{date}_{hour}"

    # Táº¡o tÃªn mÃ£ váº­n Ä‘Æ¡n: dÃ¹ng má»™t mÃ£ hoáº·c ghÃ©p nhiá»u mÃ£ báº±ng "-"
    tracking_str = "-".join(tracking_codes) if tracking_codes else "unknown"

    # Táº¡o tÃªn file Ä‘áº§u ra
    output_file = os.path.join(output_dir, f"{brand_name}_{tracking_str}_{time_str}.mp4")

    # GhÃ©p ná»‘i video A vÃ  B
    concat_list_file = os.path.join(output_dir, f"concat_list_{event_a.get('event_id')}.txt")
    try:
        with open(concat_list_file, 'w') as f:
            f.write(f"file '{temp_file_a}'\nfile '{temp_file_b}'\n")

        cmd_concat = [
            "ffmpeg",
            "-f", "concat",
            "-safe", "0",
            "-i", concat_list_file,
            "-c", "copy",
            "-y",
            output_file
        ]
        subprocess.run(cmd_concat, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

        print(f"ÄÃ£ ghÃ©p vÃ  cáº¯t video: {output_file}")

        # Log Ä‘á»™ dÃ i cá»§a file ghÃ©p
        probe = subprocess.run(
            ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", output_file],
            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
        )
        duration = float(probe.stdout.strip())
        print(f"File ghÃ©p {output_file} cÃ³ Ä‘á»™ dÃ i: {duration} giÃ¢y")

        # XÃ³a cÃ¡c file táº¡m vÃ  file concat_list sau khi ghÃ©p thÃ nh cÃ´ng
        if os.path.exists(temp_file_a):
            os.remove(temp_file_a)
        if os.path.exists(temp_file_b):
            os.remove(temp_file_b)
        if os.path.exists(concat_list_file):
            os.remove(concat_list_file)

        return output_file

    except subprocess.CalledProcessError as e:
        print(f"Lá»—i khi ghÃ©p video: {e}")
        # XÃ³a cÃ¡c file táº¡m vÃ  file concat_list trong trÆ°á»ng há»£p lá»—i
        if os.path.exists(temp_file_a):
            os.remove(temp_file_a)
        if os.path.exists(temp_file_b):
            os.remove(temp_file_b)
        if os.path.exists(concat_list_file):
            os.remove(concat_list_file)
        return None
    except Exception as e:
        print(f"Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh khi ghÃ©p video: {e}")
        # XÃ³a cÃ¡c file táº¡m vÃ  file concat_list trong trÆ°á»ng há»£p lá»—i
        if os.path.exists(temp_file_a):
            os.remove(temp_file_a)
        if os.path.exists(temp_file_b):
            os.remove(temp_file_b)
        if os.path.exists(concat_list_file):
            os.remove(concat_list_file)
        return None
```
## ğŸ“„ File: `cutter_utils.py`
**ÄÆ°á»ng dáº«n:** `/Users/annhu/vtrack_app/V_Track/backend/modules/technician/cutter/cutter_utils.py`

```python
import os
from datetime import datetime
import ast

def is_reasonable_timestamp(ts):
    """Kiá»ƒm tra xem timestamp cÃ³ há»£p lá»‡ khÃ´ng (lá»›n hÆ¡n nÄƒm 2020)."""
    return ts and int(ts) > 1577836800000  # TrÃªn nÄƒm 2020

def generate_output_filename(event, tracking_codes_filter, output_dir, brand_name="Alan"):
    """Táº¡o tÃªn file Ä‘áº§u ra dá»±a trÃªn tracking code vÃ  thá»i gian Æ°u tiÃªn: packing_time_start > packing_time_end."""
    tracking_codes_str = event.get("tracking_codes")
    packing_time_start = event.get("packing_time_start")
    packing_time_end = event.get("packing_time_end")

    try:
        tracking_codes = ast.literal_eval(tracking_codes_str) if tracking_codes_str else []
    except (ValueError, SyntaxError) as e:
        print(f"Lá»—i parse tracking_codes_str cho event {event.get('event_id')}: {e}")
        tracking_codes = []

    # Chá»n tracking code Æ°u tiÃªn
    if tracking_codes_filter:
        selected_tracking_code = next((code for code in tracking_codes_filter if code in tracking_codes), "NoCode")
    else:
        selected_tracking_code = tracking_codes[-1] if tracking_codes else "NoCode"

    # Æ¯u tiÃªn chá»n thá»i gian: packing_time_start > packing_time_end > fallback
    timestamp = next(
        (t for t in [packing_time_start, packing_time_end] if is_reasonable_timestamp(t)),
        0  # Fallback
    )
    try:
        time_str = datetime.fromtimestamp(int(timestamp) / 1000).strftime("%Y%m%d_%H%M")
    except Exception:
        time_str = "19700101_0000"

    return os.path.join(output_dir, f"{brand_name}_{selected_tracking_code}_{time_str}.mp4")

def generate_merged_filename(event_a, event_b, output_dir, brand_name="Alan"):
    """Táº¡o tÃªn file ghÃ©p cho hai sá»± kiá»‡n dá»Ÿ dang, Æ°u tiÃªn thá»i gian: packing_time_start > packing_time_end."""
    tracking_codes_a_str = event_a.get("tracking_codes")
    tracking_codes_b_str = event_b.get("tracking_codes")
    packing_time_start_a = event_a.get("packing_time_start")
    packing_time_end_b = event_b.get("packing_time_end")

    # Chá»n tracking code (Æ°u tiÃªn sá»± kiá»‡n cÃ³ mÃ£ váº­n Ä‘Æ¡n)
    try:
        tracking_codes_a = ast.literal_eval(tracking_codes_a_str) if tracking_codes_a_str else []
    except (ValueError, SyntaxError):
        tracking_codes_a = []
    try:
        tracking_codes_b = ast.literal_eval(tracking_codes_b_str) if tracking_codes_b_str else []
    except (ValueError, SyntaxError):
        tracking_codes_b = []

    selected_tracking_code = tracking_codes_b[-1] if tracking_codes_b else (tracking_codes_a[-1] if tracking_codes_a else "NoCode")

    # Æ¯u tiÃªn chá»n thá»i gian: packing_time_start > packing_time_end > fallback
    timestamp = next(
        (t for t in [packing_time_start_a, packing_time_end_b] if is_reasonable_timestamp(t)),
        0  # Fallback
    )
    try:
        time_str = datetime.fromtimestamp(int(timestamp) / 1000).strftime("%Y%m%d_%H%M")
    except Exception:
        time_str = "19700101_0000"

    return os.path.join(output_dir, f"{brand_name}_{selected_tracking_code}_{time_str}.mp4")

def update_event_in_db(cursor, event_id, output_file):
    """Cáº­p nháº­t CSDL cho má»™t sá»± kiá»‡n."""
    cursor.execute("""
        UPDATE events 
        SET is_processed = 1, output_file = ? 
        WHERE event_id = ?
    """, (output_file, event_id))
```